# 5种归一化方法

[45、五种归一化的原理与PyTorch逐行手写实现讲解(BatchNorm/LayerNorm/InsNorm/GroupNorm/WeightNorm)](https://www.bilibili.com/video/BV1Pq4y1a7pH?spm_id_from=333.788.videopod.sections&vd_source=ddd7d236ab3e9b123c4086c415f4939e)



![image-20241125213344927](images/image-20241125213344927.png)

## BatchNorm

> 批归一化、通道级别的归一化

### 官网api，BatchNorm1D & 2D

![image-20241125213959648](images/image-20241125213959648.png)

BatchNorm1D的输入：NCL，用于NLP

![image-20241125214107222](images/image-20241125214107222.png)

BatchNorm2D的输入是四维的，用于图像

![image-20241125214207707](images/image-20241125214207707.png)

一个是三维tensor作为输入

一个是四维tensor作为输入

### BatchNorm1D

 ![image-20241125215123844](images/image-20241125215123844.png)

- 首先，位于torch.nn模块下，是一个class，所以要用的话，需要实例化
- 接下来，看实例化需要接收的参数：
  - num features：输入张量的特征维度，或者通道的数目，或者embedding的大小
  - eps：5种归一化都需要的eps，分母数值稳定性，让分母加上一个微小的量，使得除法能够正常进行，默认1e-05
  - momentum：动量
    - 批归一化在计算均值和方差的时候，momentum通常需要跟track_running_sate联合起来理解，也就是说我们的统计量 通常是通过滑动平均计算出来了，而不是单一时刻的mini batch，是一个累计的过程，为了提高估计的准确度
  - affine：
    - 也就是 gamma & beta，也就是再做完归一化以后，也可以加一个映射，将其映射到一个新的分布上，做一个rescale和recenter

官网定义：

![image-20241125220039574](images/image-20241125220039574.png)

（解释官网定义）均值和标准差是经过整个mini_batch

> 一句话说明 BatchNorm：per channel across mini-batch
>
> 贯穿整个mini batch计算统计量，每个通道单独去算的
>
> gamma 和 beta 是可学习的向量，维度都是C，默认的情况下 $\gamma = 1、\beta=0 $
>
> 标准差用的是有偏估计，也就是计算的标准差是 $\frac{1}{n}$，强调这句话的目的是 ，在计算方差的时候，要用 $\mathrm{unbiased=False}$，这里用得是有偏估计
>
> 在默认情况下，在训练中，会不断的记录历史的均值和方差，并且使用0.1的动量，来做移动的估计，当训练结束以后，用最后一个时刻的估计量来做 inference
>
> 也可以设置 track running states等于false，就是不要记录历史的移动的值

以上是api的介绍

接下来 自己写一个BatchNorm 更好的理解

- [x] NLP的标准数据格式：inputx = torch.randn(batch_size,times_steps,embedding_dim) # $N*L*C$

- [x] 实例化，接收的输入（特征维度，是否进行仿射变换）：batch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)

- [x] batchnorm的forward函数 接收的数据集格式是 BDN  b表示batch size；D表示model dim；N表示序列长度（符号表示方法的不同

  `bn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)`

  

```python
import torch

batch_size = 2
times_steps = 3
embedding_dim = 4

inputx = torch.randn(batch_size,times_steps,embedding_dim) # N*L*C

# 1. 实现batch_norm并验证API

## 调用 batch_norm API
batch_norm_op = torch.nn.BatchNorm1d(embedding_dim,affine=False)
bn_y = batch_norm_op(inputx.transpose(-1,-2)).transpose(-1,-2)

## 手写batch_norm
bn_mean = inputx.mean(dim=(0,1),keepdim=True)
bn_std = inputx.std(dim=(0,1),unbiased=False,keepdim=True)
verify_bn_y = (inputx - bn_mean)/(bn_std+1e-5)
print(bn_y)
print(verify_bn_y)
print(torch.allclose(bn_y,verify_bn_y))
```

输出：

```python
tensor([[[-0.3771,  1.7863, -1.0572,  0.2856],
         [-0.7956, -0.0363, -0.7429, -0.1670],
         [ 2.0838,  0.7039,  1.1345,  0.7286]],

        [[-0.5775, -0.3680, -1.1160, -1.3169],
         [ 0.3298, -1.0699,  1.2153, -1.0909],
         [-0.6634, -1.0160,  0.5663,  1.5606]]])
tensor([[[-0.3771,  1.7863, -1.0572,  0.2856],
         [-0.7956, -0.0363, -0.7429, -0.1670],
         [ 2.0838,  0.7039,  1.1345,  0.7286]],

        [[-0.5775, -0.3680, -1.1160, -1.3169],
         [ 0.3298, -1.0699,  1.2153, -1.0909],
         [-0.6634, -1.0160,  0.5663,  1.5606]]])
True
```

解释：去看图解BN&LN

