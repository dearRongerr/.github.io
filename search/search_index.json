{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Memo","text":"<p>\u6bcf\u5929\u6478\u6478\u5934\uff0c\u4e07\u4e8b\u90fd\u4e0d\u6101</p> <p>241125 \u5e72\u6d3b</p> <ul> <li> swinTransformer\u7b14\u8bb0\u30014\u79cd\u4f4d\u7f6e\u7f16\u7801</li> <li> \u53ef\u4ee5\u5f00\u59cb\u770bGAN\u3001Diffusion\u3001CLIP\u4e86</li> <li> \ud83e\udd5f</li> <li> \u7edf\u4e00\u63d0\u4ea4</li> </ul> <p>\u89c9\u5f97\u70e6\u5c31\u8dd1\uff0c\u8981\u5b66\u4f1a\u653e\u8fc7\u81ea\u5df1\ud83d\udc85\ud83c\udffb</p>"},{"location":"Error/github/","title":"github","text":"<p>Git: fatal: unable to access 'https://github.com/dearRongerr/Rongerr.github.io.git/': Failure when receiving data from the peer</p> <p>\u7ebf\u4e0a\u548c\u672c\u5730\u4e0d\u540c\u6b65\u95ee\u9898</p> <p>\u5148\u5c06\u8fdc\u7a0b\u5206\u652f\u7684\u66f4\u6539\u5408\u5e76\u5230\u672c\u5730\u5206\u652f\uff0c\u7136\u540e\u518d\u63a8\u9001\u3002\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c\uff1a</p> <ul> <li>\u62c9\u53d6\u8fdc\u7a0b\u5206\u652f\u7684\u66f4\u6539\u5e76\u5408\u5e76\u5230\u672c\u5730\u5206\u652f\uff1a</li> </ul> <pre><code>  git pull origin main --rebase\n</code></pre> <ul> <li>\u89e3\u51b3\u4efb\u4f55\u53ef\u80fd\u7684\u51b2\u7a81\u3002\u5982\u679c\u6709\u51b2\u7a81\uff0cGit \u4f1a\u63d0\u793a\u4f60\u89e3\u51b3\u51b2\u7a81\u3002\u89e3\u51b3\u51b2\u7a81\u540e\uff0c\u7ee7\u7eed\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</li> </ul> <pre><code>  git rebase --continue\n</code></pre> <ul> <li>\u6700\u540e\uff0c\u63a8\u9001\u672c\u5730\u5206\u652f\u5230\u8fdc\u7a0b\u4ed3\u5e93\uff1a</li> </ul> <pre><code>git push -u origin main\n</code></pre> <p>Git: fatal: unable to access 'https://github.com/dearRongerr/Rongerr.github.io.git/': Failed to connect to github.com port 443 after 75002 ms: Couldn't connect to server</p> <p>\u7f51\u7edc\u95ee\u9898\uff0c\u5173\u4ee3\u7406</p>"},{"location":"bagu/","title":"Index","text":"<pre><code>(base) ... bagu % tree\n.\n\u251c\u2500\u2500 deeplearning\n\u2502   \u251c\u2500\u2500 former\n\u2502   \u2502   \u251c\u2500\u2500 pe.md\n\u2502   \u2502   \u2514\u2500\u2500 transformer.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 pytorch\u5e38\u7528\u7684\u7ef4\u5ea6\u53d8\u6362\u51fd\u6570.md\n\u251c\u2500\u2500 leetcode\n\u2502   \u251c\u2500\u2500 1.md\n\u2502   \u251c\u2500\u2500 2.md\n\u2502   \u2514\u2500\u2500 index.md\n\u2514\u2500\u2500 machinelearning\n\n5 directories, 7 files\n</code></pre>"},{"location":"bagu/deeplearning/","title":"Index","text":"<ul> <li> Batchnorm &amp; layernorm</li> </ul>"},{"location":"bagu/deeplearning/pytorch_shape_function/","title":"pytorch\u7684\u7ef4\u5ea6\u53d8\u6362\u516c\u5f0f","text":"<ol> <li><code>torch.unsqueeze(input, dim)</code>\uff1a\u5728\u6307\u5b9a\u7ef4\u5ea6 <code>dim</code> \u4e0a\u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6\u3002\u5982\u679c <code>dim</code> \u5df2\u7ecf\u5b58\u5728\uff0c\u5219\u5728\u5176\u524d\u9762\u6dfb\u52a0\u65b0\u7684\u7ef4\u5ea6\u3002</li> <li><code>torch.squeeze(input, dim=None)</code>\uff1a\u79fb\u9664\u6240\u6709\u957f\u5ea6\u4e3a1\u7684\u7ef4\u5ea6\u3002\u5982\u679c\u6307\u5b9a\u4e86 <code>dim</code>\uff0c\u5219\u53ea\u79fb\u9664\u8be5\u7ef4\u5ea6\u3002</li> <li><code>torch.flatten(input, start_dim=0, end_dim=-1)</code>\uff1a\u5c06\u8f93\u5165\u5f20\u91cf\u4ece <code>start_dim</code> \u5230 <code>end_dim</code> \u7684\u6240\u6709\u7ef4\u5ea6\u5c55\u5e73\u3002</li> <li><code>torch.view(input, size)</code> \u6216 <code>input.view(size)</code>\uff1a\u91cd\u65b0\u8c03\u6574\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4e0d\u6539\u53d8\u6570\u636e\u3002</li> <li><code>torch.reshape(input, shape)</code>\uff1a\u4e0e <code>view</code> \u7c7b\u4f3c\uff0c\u7528\u4e8e\u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4f46 <code>reshape</code> \u53ef\u4ee5\u5904\u7406\u66f4\u590d\u6742\u7684\u7ef4\u5ea6\u53d8\u6362\uff0c\u5982\u589e\u52a0\u6216\u51cf\u5c11\u7ef4\u5ea6\u3002</li> <li><code>torch.permute(input, dims)</code>\uff1a\u91cd\u65b0\u6392\u5217\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6\uff0c<code>dims</code> \u662f\u4e00\u4e2a\u7ef4\u5ea6\u7d22\u5f15\u7684\u5143\u7ec4\u3002</li> <li><code>torch.transpose(input, dim0, dim1)</code>\uff1a\u4ea4\u6362\u8f93\u5165\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\u3002</li> <li><code>torch.expand(input, size)</code>\uff1a\u5c06\u8f93\u5165\u5f20\u91cf\u6cbf\u6307\u5b9a\u7684\u7ef4\u5ea6\u590d\u5236\u6269\u5c55\u3002</li> <li><code>torch.cat(tensors, dim)</code>\uff1a\u6cbf\u6307\u5b9a\u7ef4\u5ea6 <code>dim</code> \u8fde\u63a5\u591a\u4e2a\u5f20\u91cf\u3002</li> <li><code>torch.stack(tensors, dim)</code>\uff1a\u6cbf\u65b0\u7684\u7ef4\u5ea6 <code>dim</code> \u5806\u53e0\u591a\u4e2a\u5f20\u91cf\uff0c\u4e0e <code>cat</code> \u4e0d\u540c\u7684\u662f\uff0c<code>stack</code> \u4f1a\u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6\u3002</li> <li><code>torch.reapeat</code></li> </ol>"},{"location":"bagu/deeplearning/former/multihead/","title":"\u624b\u6495\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,model_dim,num_heads,dropout=0.1):\n        super().__init__()\n        self.model_dim = model_dim\n        self.num_heads = num_heads\n        self.head_dim = model_dim // num_heads\n\n        self.q_proj = nn.Linear(model_dim,model_dim)\n        self.k_proj = nn.Linear(model_dim,model_dim)\n        self.v_proj = nn.Linear(model_dim,model_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.o_proj = nn.Linear(model_dim,model_dim)\n\n    def forward(self,q,k,v,mask=None):\n\n        batch_size,sequence_length,model_dim = q.shape\n\n        q = self.q_proj(q).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)\n        k = self.k_proj(k).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)\n        v = self.v_proj(v).view(batch_size,sequence_length,self.num_heads,self.head_dim).transpose(1,2)\n\n        scores = torch.matmul(q,k.transpose(-1,-2))//math.sqrt(self.head_dim)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask==0,-1e09)\n\n        prob = F.softmax(scores,dim=-1)\n\n        prob = self.dropout(prob)\n\n        attn_weights = torch.matmul(prob,v).transpose(1,2).contiguous().view(batch_size,sequence_length,model_dim)\n\n        output = self.o_proj(attn_weights)\n        return output\n\nmodel_dim = 512 \nnum_heads = 8\nmha = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads, dropout=0.1)\nbatch_size = 10\nsequence_length = 60\nq = torch.randn(batch_size, sequence_length, model_dim)\nk = torch.randn(batch_size, sequence_length, model_dim)\nv = torch.randn(batch_size, sequence_length, model_dim)\n\nmask = None\noutput = mha(q, k, v, mask)\nprint(output.shape)  # \u8f93\u51fa\u7684\u5f62\u72b6\u5e94\u8be5\u662f(batch_size, sequence_length, model_dim)\n</code></pre> <p>note\uff1a</p> <ul> <li> <p>\u5047\u8bbe\u8f93\u5165\u6570\u636eq, k, v\u7684\u5f62\u72b6\u662f(batch_size, sequence_length, model_dim)</p> </li> <li> <p>\u4f8b\u5982\uff0c\u4e00\u4e2a\u6279\u6b21\u5927\u5c0f\u4e3a10\uff0c\u5e8f\u5217\u957f\u5ea6\u4e3a60\uff0c\u6a21\u578b\u7ef4\u5ea6\u4e3a512\u7684\u8f93\u5165 </p> </li> <li> <p>\u8fd9\u662f\u56e0\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u7684\u76ee\u7684\u662f\u4e3a\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u8868\u793a\uff0c   \u800c\u4e0d\u662f\u751f\u6210\u4e00\u4e2a\u5e8f\u5217\u957f\u5ea6\u4e3a sequence_length \u7684\u5e8f\u5217\u3002</p> </li> </ul> <p>\u200b   \u6ce8\u610f\u529b\u673a\u5236\u4e3a\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u8868\u793a\uff0c\u8fd9\u4e2a\u8868\u793a\u7efc\u5408\u4e86\u5e8f\u5217\u4e2d\u6240\u6709\u5143\u7d20\u7684\u4fe1\u606f\uff0c</p> <p>\u200b   \u4f46\u8f93\u51fa\u7684\u5f62\u72b6\u4ecd\u7136\u662f (batch_size, sequence_length, model_dim)\uff0c</p> <p>\u200b   \u800c\u4e0d\u662f (batch_size, sequence_length, sequence_length)\u3002</p> <p>\u200b   \u8fd9\u662f\u56e0\u4e3a\u8f93\u51fa\u7684\u6bcf\u4e2a\u5143\u7d20\u662f\u5e8f\u5217\u4e2d\u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u5728\u6240\u6709\u5934\u4e2d\u7684\u52a0\u6743\u8868\u793a\uff0c \u200b   \u800c\u4e0d\u662f\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5bf9\u5176\u4ed6\u5143\u7d20\u7684\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635\u3002</p>"},{"location":"bagu/deeplearning/former/pe/","title":"\u4f4d\u7f6e\u7f16\u7801","text":""},{"location":"bagu/deeplearning/former/pe/#_2","title":"\u4f4d\u7f6e\u7f16\u7801\u4e3a\u4ec0\u4e48\u662f\u4e09\u89d2\u51fd\u6570\u5f62\u5f0f\u7684\uff1f","text":"<ol> <li>\u6700\u76f4\u89c2\u7684\u7f16\u7801\u65b9\u5f0f\u662f\u4ece0\u5230sequence length\uff0c\u4f46\u662f\u65e0\u754c</li> <li>\u7528 \\(\\frac{1}{sequence\\_length}\\) \u6539\u53d8\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e</li> <li>\u4e8c\u8fdb\u5236\u7f16\u7801\uff0cd model\u901a\u5e38\u8bbe\u7f6e\u4e3a512,2\u7684512\u6b21\u65b9\u80fd\u7f16\u7801\u5b8c max sequence length\u4e2a\u4f4d\u7f6e\uff0c\u4f46\u662f\u662f\u79bb\u6563\u7684</li> <li>\u8fde\u7eed\uff0c\u5e26\u6709\u5468\u671f\u6027\u7684\u4e09\u89d2\u51fd\u6570\u4f4d\u7f6e\u7f16\u7801\uff0c\u7c7b\u4f3c\u4e8c\u8fdb\u5236\uff0c\u4f4e\u4f4d\u53d8\u5316\u5feb\uff0c\u9ad8\u4f4d\u53d8\u5316\u6162</li> </ol> <p>\u301046\u3001\u56db\u79cdPosition Embedding\u7684\u539f\u7406\u4e0ePyTorch\u624b\u5199\u9010\u884c\u5b9e\u73b0\uff08Transformer/ViT/Swin-T/MAE\uff09-\u54d4\u54e9\u54d4\u54e9\u3011</p> <p></p>"},{"location":"bagu/deeplearning/former/pe/#transformer","title":"\u539f\u59cbTransformer\u7684\u4f4d\u7f6e\u7f16\u7801 \uff1a\u4e00\u7ef4\u7edd\u5bf9\u3001\u5e38\u6570\u4f4d\u7f6e\u7f16\u7801","text":"<p>pos\uff1a\u53e5\u5b50\u4e2d\u8bcd\u7684\u4f4d\u7f6e\uff080-max sequence length\uff09</p> <p>i\uff1a\u8bcd\u5d4c\u5165\u4f4d\u7f6e\uff080\u2014255\uff09</p> <p>1D\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e38\u6570\u4e0d\u9700\u8981\u8bad\u7ec3</p> <p>\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <p>(\u7c7b\u5199\u6cd5)</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SinCosPositionEmbedding(nn.Module):\n    def __init__(self, max_sequence_length,model_dim):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.model_dim = model_dim\n    def forward(self):\n        pe = torch.zeros(self.max_sequence_length,self.model_dim)\n        pos_mat = torch.arange(self.max_sequence_length).reshape(-1,1)\n        i_mat = torch.pow(10000,\n                          torch.arange(0,self.model_dim,2).reshape(1,-1)/self.model_dim\n                          )\n\n        pe[:,0::2] = torch.sin(pos_mat/i_mat)\n        pe[:,1::2] = torch.cos(pos_mat/i_mat)\n\n        return pe\nprint(SinCosPositionEmbedding(max_sequence_length=8,model_dim=4).forward())\n</code></pre> <p>\uff08\u51fd\u6570\u5199\u6cd5\uff09</p> <pre><code>def position_sincos_embedding(max_sequence_length,model_dim):\n    assert model_dim%2 == 0,\"wrong dimension\"\n    pe_table = torch.zeros(max_sequence_length,model_dim)\n    pos_mat = torch.arange(max_sequence_length).reshape(-1,1)\n    i_mat = torch.pow(\n        10000,\n        torch.arange(0,model_dim,2)/model_dim\n    )\n    pe_table[:,0::2]=torch.sin(pos_mat/i_mat)\n    pe_table[:,1::2]=torch.cos(pos_mat/i_mat)\n    return pe_table\n\n# Transformer\u8bba\u6587 \u4e00\u7ef4\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\nif __name__==\"__main__\":\n    max_sequence_length = 8\n    model_dim = 4\n    pe_table = position_sincos_embedding(max_sequence_length,model_dim)\n    print(pe_table)\n</code></pre>"},{"location":"bagu/deeplearning/former/pe/#vit-1","title":"ViT  1\u7ef4\u7edd\u5bf9\u7684\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u6807\u51c6\u7684\u3001\u53ef\u5b66\u4e60\u7684\u4e00\u7ef4\u4f4d\u7f6e\u7f16\u7801\uff1b\u4e8c\u7ef4\u7684\u4f4d\u7f6e\u7f16\u7801\u5e76\u6ca1\u6709\u5e26\u6765\u66f4\u597d\u7684\u6548\u679c</p> <pre><code>def create_1d_absolute_trainable_embeddings(max_sequence_length,model_dim):\n    pe = nn.Embedding(max_sequence_length,model_dim)\n    nn.init.constant_(pe.weight,0.)\n\n    return pe\n</code></pre>"},{"location":"bagu/deeplearning/former/pe/#swintransformer-2","title":"SwinTransformer 2\u7ef4\u7684\u3001\u76f8\u5bf9\u7684\u3001\u57fa\u4e8e\u4f4d\u7f6e\u504f\u5dee\u53ef\u8bad\u7ec3\u7684\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u3001\u53ef\u5b66\u4e60\u7684\u3001\u76f8\u5bf9\u4f4d\u7f6e\u504f\u5dee\u52a0\u5230\u6bcf\u4e00\u4e2a\u5934\u4e0a</p> <p>\\(QK^T\\) \u7684\u7ef4\u5ea6\u662f \\(\u5e8f\u5217\u957f\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\\)\uff0c\u6240\u4ee5B\u7684\u5f62\u72b6\u4e5f\u662f  \\(\u5e8f\u5217\u957f\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\\)</p>"},{"location":"bagu/deeplearning/former/selfattention/","title":"\u624b\u6495\u81ea\u6ce8\u610f\u673a\u5236","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self,model_dim,dropout=0.1):\n\n        super().__init__()\n        self.model_dim = model_dim\n\n        self.q_proj = nn.Linear(model_dim,model_dim)\n        self.k_proj = nn.Linear(model_dim,model_dim)\n        self.v_proj = nn.Linear(model_dim,model_dim)\n\n        self.o_proj = nn.Linear(model_dim,model_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self,x,mask=None):\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        scores = torch.matmul(q,k.transpose(-1,-2))//math.sqrt(self.model_dim)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask==0,-1e09)\n\n        prob = F.softmax(scores,dim=-1)\n\n        prob = self.dropout(prob)\n\n        attn_weights = torch.matmul(prob,v)\n\n        output = self.o_proj(attn_weights)\n\n        return output\n\nmodel_dim = 512\nseq_len = 8\nbatch_size = 2\n# mask shape = seq_len * model_dim\nx = torch.randn(batch_size,seq_len,model_dim)\n\nsa = SelfAttention(model_dim)\n\nattn_weights = sa(x)\nprint(attn_weights.shape)  # \u8f93\u51fa\u7684\u5f62\u72b6\u5e94\u8be5\u662f(batch_size, seq_len, model_dim)\n</code></pre>"},{"location":"bagu/deeplearning/former/transformer/","title":"Transformer","text":"<p>transformer\u9762\u7ecf</p>"},{"location":"bagu/leetcode/","title":"Index","text":"<ul> <li> 1</li> <li> 2</li> </ul>"},{"location":"bagu/leetcode/1/","title":"1","text":""},{"location":"bagu/leetcode/1/#_1","title":"\u4e24\u6570\u4e4b\u548c","text":"<p>\u529b\u62631 \u4e24\u6570\u4e4b\u548c </p> <pre><code>class Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n</code></pre> <p>\u793a\u4f8b 1\uff1a</p> <pre><code>\u8f93\u5165\uff1anums = [2,7,11,15], target = 9\n\u8f93\u51fa\uff1a[0,1]\n\u89e3\u91ca\uff1a\u56e0\u4e3a nums[0] + nums[1] == 9 \uff0c\u8fd4\u56de [0, 1] \u3002\n</code></pre> <pre><code>class Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        dict = {}\n\n        for i in range(len(nums)):\n            if target - nums[i] not in dict:\n                dict[nums[i]] = i\n            else:\n                return [dict[target-nums[i]],i]\n</code></pre> <p>\u9010\u5b57\u8be6\u89e3\uff1a\u9996\u5148\u8f93\u5165\u7684\u6570\u7ec4\u662fnums\uff0c\u5047\u8bbenums = [2,7,11,15]\uff0ci=0,1,2,3 \u5f53 i=0\u65f6\uff0c\u6b64\u65f6target=9\uff0ctarget-nums[0]=9-2=7,7\u4e0d\u5728\u5b57\u5178\u4e2d\uff0c\u56e0\u4e3a\u5b57\u5178\u662f\u7a7a\u7684\uff0c\u6240\u4ee5\u6267\u884cdict[num[i]] = i \u5373 dict[nums[0]]=0 \uff0cdict[2]=0\uff0ci=1,target-nums[i]=9-nums[1]=9-7=2,2\u5728\u5b57\u5178\u4e2d\uff0c\u6240\u4ee5\u6267\u884celse return[dict[2],1]=[0,1]\u8fd4\u56de\uff0c\u5f97\u5230\u7ed3\u679c</p> <p>\u518d\u6765\uff1a\u9996\u5148\u5b9a\u4e49\u7a7a\u5b57\u5178\uff0c\u7528\u6765\u5b58\u50a8\u5df2\u7ecf\u904d\u5386\u8fc7\u7684\u6570\u5b57\u53ca\u5176\u5bf9\u5e94\u7684\u7d22\u5f15\uff1b\u63a5\u4e0b\u6765 for loop\u904d\u5386\u6570\u7ec4nums,\u5176\u4e2di\u662f\u5f53\u524d\u904d\u5386\u5230\u7684\u6570\u5b57\u7684\u7d22\u5f15\uff1b\u5982\u679c\u4e0d\u5728\u5b57\u5178\u4e2d\uff0c\u8bf4\u660e\u4e4b\u524d\u6ca1\u6709\u9047\u5230\u8fc7\u4e0e\u5f53\u524d\u6570\u5b57\u76f8\u52a0\u7b49\u4e8etarget\u7684\u6570\u5b57\uff0c\u56e0\u6b64\u5c06\u5f53\u524d\u6570\u5b57\u53ca\u5176\u7d22\u5f15\u5b58\u5165\u5b57\u5178\u4e2d\u3002\u5982\u679c\u5728\u5b57\u5178\u4e2d\uff0c\u8bf4\u660e\u5df2\u7ecf\u627e\u5230\u4e86\u4e00\u5bf9\u6570\u5b57\uff0c\u548c\u7b49\u4e8etarget\uff0c\u4e8e\u662f\u8fd4\u56de\u8fd9\u4e24\u4e2a\u6570\u5b57\u7684\u7d22\u5f15\u3002\u8fd9\u4e24\u4e2a\u7d22\u5f15\u5206\u522b\u662fdict[target-nums[i]]\uff08\u5b58\u50a8\u5dee\u503c \u5bf9\u5e94\u7684\u7d22\u5f15\uff09\u548c i\uff08\u5f53\u524d\u6570\u5b57\u7684\u7d22\u5f15\uff09</p> <p>\u5206\u6790\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u5176\u4e2dn\u4e3a\u6570\u7ec4\u7684\u957f\u5ea6\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u5143\u7d20\u53ea\u88ab\u8bbf\u95ee\u4e00\u6b21\uff0c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3aO(n)\uff0c\u6700\u574f\u7684\u60c5\u51b5\u662f\u53ef\u80fd\u9700\u8981\u5b58\u50a8\u6240\u6709\u5143\u7d20\u7684\u7d22\u5f15</p> <p>\u8fd9\u4e2a\u65b9\u6cd5\u4f7f\u7528\u4e86\u54c8\u5e0c\u8868\uff08\u901a\u8fc7\u5b57\u5178\uff09\u5feb\u901f\u67e5\u627e\u76ee\u6807\u6570\u5b57\uff0c\u662f\u4e00\u79cd\u5178\u578b\u7684\u54c8\u5e0c\u8868\u5e94\u7528\u573a\u666f\uff0c\u63d0\u9ad8\u67e5\u627e\u6548\u7387</p>"},{"location":"bagu/leetcode/2/","title":"2","text":"<p>\u4e24\u6570\u76f8\u52a0</p>"},{"location":"bagu/machinelearning/kmeans/","title":"\u624b\u6495kmeans","text":"<pre><code>import numpy as np\ndef kmeans(data, k, thresh=1, max_iterations=100):\n  centers = data[\n     np.random.choice(data.shape[0], k, replace=False)\n     ]\n  for _ in range(max_iterations):\n    distances = np.linalg.norm(\n       data[:, None] - centers, \n       axis=2\n       ) # n,k,d\n    labels = np.argmin(distances, axis=1)\n    new_centers = np.array(\n       [data[labels == i].mean(axis=0) for i in range(k)]\n       )\n    if np.all(centers == new_centers):break\n    center_change = np.linalg.norm(new_centers - centers)\n    if center_change &lt; thresh:break\n    centers = new_centers\n  return labels, centers\ndata = np.random.rand(100, 2)  # 100\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u6709\u4e24\u4e2a\u7279\u5f81\nk = 3  # \u805a\u7c7b\u6570\u4e3a3\nlabels, centers = kmeans(data, k)\nprint(\"\u7c07\u6807\u7b7e:\", labels)\nprint(\"\u805a\u7c7b\u4e2d\u5fc3\u70b9:\", centers)\n</code></pre>"},{"location":"learning/","title":"Index","text":"<ul> <li> vit</li> <li> swintransformer</li> <li> Diffusion</li> </ul>"},{"location":"learning/swintransformer/","title":"SwinTransformer \u5b66\u4e60\u7b14\u8bb0","text":"<ul> <li>SwinTransformer \u5b66\u4e60\u7b14\u8bb0<ul> <li>SwinTransformer\u6548\u679c\u6709\u591a\u597d</li> <li>\u5185\u5bb9\u76ee\u5f55</li> <li>\u5bf9\u6bd4SwinTransformer&amp;ViT</li> <li>\u7f51\u7edc\u6574\u4f53\u6846\u67b6</li> <li>\u4ec0\u4e48\u662fpatch partition?</li> <li>\u4e3a\u4ec0\u4e48SwinTransformer block\u90fd\u662f\u5076\u6570\u6b21\uff1f</li> <li>Patch merging\uff1f</li> <li>W-MSA\uff1f</li> <li>MSA\u548cW-MSA\u8ba1\u7b97\u91cf\u7684\u8ba8\u8bba</li> <li>shifted window multihead self attention\u6a21\u5757<ul> <li>\u4e3a\u4ec0\u4e48\u9700\u8981 shift window multihead self attention\uff1f</li> <li>\u600e\u4e48\u7406\u89e3shift \uff1f</li> <li>\u600e\u4e48\u7406\u89e3shift\u4ee5\u540e\u7684\u4fe1\u606f\u4ea4\u4e92\uff1f</li> <li>shift\u4e4b\u540e\uff0c\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u4f1a\u51fa\u73b0\u7684\u65b0\u95ee\u9898\uff1f</li> <li>\u89e3\u51b3\u7a97\u53e3\u53d8\u591a\u4e14\u4e0d\u4e00\u6837\u5927\u8ba1\u7b97 \u81ea\u6ce8\u610f\u529b\u7684\u95ee\u9898</li> <li>shift window\u548cmask</li> <li>\u533a\u57df3\u548c\u533a\u57df5 \u7684mask</li> </ul> </li> <li>\u7ee7\u7eed\u8ba8\u8bba SW-MSA</li> <li>\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb\uff1frelative position bias<ul> <li>\u4ec0\u4e48\u662f\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\uff1f</li> <li>\u4e8c\u5143\u5750\u6807\u600e\u4e48\u8f6c\u5316\u6210\u4e00\u5143\u5750\u6807</li> <li>\u4ece\u76f8\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u5230\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb</li> </ul> </li> <li>\u6a21\u578b\u8be6\u7ec6\u914d\u7f6e\u53c2\u6570</li> </ul> </li> </ul> <p>\u301012.1 Swin-Transformer\u7f51\u7edc\u7ed3\u6784\u8be6\u89e3-\u54d4\u54e9\u54d4\u54e9\u3011</p> <p>\u7b2c\u4e00\u6b21\u5b66\u7684\u65f6\u5019\uff0c\u6240\u6709\u7684swin\u5168\u90fd\u5199\u6210\u4e86swim\uff0c\u5446\uff0c\u53c8\u53cc\u53d2\u53d5\u4e22\u4eba\u663e\u773c\u4e86\uff0c\u7b11\uff09</p>"},{"location":"learning/swintransformer/#swintransformer_1","title":"SwinTransformer\u6548\u679c\u6709\u591a\u597d","text":"<p>2021\u5e743\u6708\u53d1\u8868</p> <p>COCO\u6570\u636e\u96c6 \u76ee\u6807\u68c0\u6d4b\u4e0b\u7684\u6a21\u578b\u6392\u540d \u90fd\u6709SwinTransformer\u7684\u5f71\u5b50</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#_1","title":"\u5185\u5bb9\u76ee\u5f55","text":"<p>\u5b8c\u5168\u4e0d\u8bb0\u5f97\uff0c\u6211\u5c45\u7136\u5b66\u8fc7\u8fd9\u4e9b\uff0c\u4e5f\u7b97\u662f \u590d\u4e60\u4e86-.-</p> <p>\u7b2c\u56db\u70b9\u3001\u7b2c\u4e94\u70b9\u4e0d\u597d\u7406\u89e3</p>"},{"location":"learning/swintransformer/#swintransformervit","title":"\u5bf9\u6bd4SwinTransformer&amp;ViT","text":"<p>\u4e24\u70b9\u4e0d\u540c\uff1a</p> <p>\u7b2c\u4e00\u70b9\uff1a</p> <p>\uff08a\uff09swintransformer\u7684feature map\u5177\u6709\u5c42\u6b21\u6027\uff0c\u7c7b\u4f3cCNN\uff0c\u968f\u7740\u5c42\u7684\u52a0\u6df1\uff0cfeature map\u7684\u9ad8\u548c\u5bbd\u8d8a\u6765\u8d8a\u5c0f\uff0c\u5206\u522b\u662f4\u500d\u4e0b\u91c7\u6837\u30018\u500d\u4e0b\u91c7\u6837\u548c16\u500d\u4e0b\u91c7\u6837\uff0c\u56e0\u4e3a\u5c42\u6b21\u6027\u7684\u7279\u5f81\u56fe\uff0c\u6240\u4ee5\u5bf9\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u5206\u5272\u4efb\u52a1\u6709\u66f4\u5927\u7684\u4f18\u52bf</p> <p>(b)ViT  \u4e00\u76f4\u662f16\u500d\u7684\u4e0b\u91c7\u6837\u7387</p> <p>\u7b2c\u4e8c\u70b9\uff1a</p> <p>SwinTransformer\u4f7f\u7528\u4e00\u4e2a\u4e00\u4e2a\u7a97\u53e3\u7684\u5f62\u5f0f\u628afeature map\u5206\u5f00\u4e86\uff0c\u7a97\u53e3\u4e0e\u7a97\u53e3\u4e4b\u95f4 \u4e0d\u91cd\u53e0</p> <p>ViT\u4e2d\uff0c\u7a97\u53e3\u662f\u4e00\u4e2a\u6574\u4f53\uff0c\u6ca1\u6709\u5206\u5272</p> <p></p> <p>\u770b\u6700\u540e\u4e00\u5217</p> <p></p>"},{"location":"learning/swintransformer/#_2","title":"\u7f51\u7edc\u6574\u4f53\u6846\u67b6","text":""},{"location":"learning/swintransformer/#patch-partition","title":"\u4ec0\u4e48\u662fpatch partition?","text":"<p>\u662f\u53d6\u539f\u59cb\u56fe\u50cf\u7684\u50cf\u7d20\u503c\uff0c\u7136\u540e\u5c55\u5e73\uff0c\u6211\u89c9\u5f97\u8fd9\u91cc\u6709\u70b9\u95ee\u9898\uff0c\\(4\u00d74\u00d73 \\rightarrow 16\u00d73\\) \u5e94\u8be5\u662f2\u00d72\u7684window\uff1f</p> <p></p> <p></p> <p>\u4e3a\u4ec0\u4e48\u8fd9\u91cc\u662f48\u4e2akernel\uff1f</p>"},{"location":"learning/swintransformer/#swintransformer-block","title":"\u4e3a\u4ec0\u4e48SwinTransformer block\u90fd\u662f\u5076\u6570\u6b21\uff1f","text":""},{"location":"learning/swintransformer/#patch-merging","title":"Patch merging\uff1f","text":""},{"location":"learning/swintransformer/#w-msa","title":"W-MSA\uff1f","text":"<p>\u7f3a\u70b9\uff1awindow\u4e4b\u95f4\u6ca1\u6709\u4fe1\u606f\u4ea4\u4e92\uff0c\u90a3\u4e48\u611f\u53d7\u91ce\u5c31\u4f1a\u53d8\u5c0f\uff0c\u4e5f\u5c31\u662f\u6ca1\u6709\u529e\u6cd5\u770b\u5230\u5168\u5c40\u7684\u89c6\u91ce\uff0c\u6700\u7ec8\u9884\u6d4b\u7ed3\u679c\u4f1a\u4e0d\u597d</p>"},{"location":"learning/swintransformer/#msaw-msa","title":"MSA\u548cW-MSA\u8ba1\u7b97\u91cf\u7684\u8ba8\u8bba","text":"<p>\u666e\u901a\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 \u548c \u5e26\u7a97\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236</p> <p></p> <p></p> <p></p> <p>\u4ee5\u4e0b\u6765\u81ea\uff1aup\u4e3b\u7684\u535a\u6587</p> <p></p> <p>\u8865\u5145\u77e9\u9635\u4e58\u6cd5\u5f53\u4e2d   FLOPS\u8ba1\u7b97\u65b9\u5f0f\uff1a</p> <p></p> <p>\u5982\u679c\u77e9\u9635A\u662fa\u00d7b\u7684\uff0c\u77e9\u9635B\u662fb\u00d7c\u7684\uff0c\u90a3\u4e48\u77e9\u9635\u76f8\u4e58\u4ee5\u540e\u7684flops\u8ba1\u7b97\u91cf\u662f a\u00d7b\u00d7c\u7684</p> <p></p> <p>\u5355\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u91cfflops</p> <p></p> <p>\u6240\u4ee5\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6700\u540e\u8fd8\u8981\u4f7f\u7528 \\(W^O\\)\u7684\u8ba1\u7b97\u91cf</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#shifted-window-multihead-self-attention","title":"shifted window multihead self attention\u6a21\u5757","text":""},{"location":"learning/swintransformer/#shift-window-multihead-self-attention","title":"\u4e3a\u4ec0\u4e48\u9700\u8981 shift window multihead self attention\uff1f","text":"<p>\u4e3a\u4e86\u89e3\u51b3window\u4e0ewindow\u4e4b\u95f4\u6ca1\u6709\u4fe1\u606f\u4ea4\u4e92\u7684\u95ee\u9898</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#shift","title":"\u600e\u4e48\u7406\u89e3shift \uff1f","text":"<p>\u4e0b\u9762\u56fe\uff0c\u9ec4\u8272\u6846 \u5411\u4e0b\u3001\u5411\u53f3\u79fb\u52a8</p> <p></p>"},{"location":"learning/swintransformer/#shift_1","title":"\u600e\u4e48\u7406\u89e3shift\u4ee5\u540e\u7684\u4fe1\u606f\u4ea4\u4e92\uff1f","text":""},{"location":"learning/swintransformer/#shift_2","title":"shift\u4e4b\u540e\uff0c\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u4f1a\u51fa\u73b0\u7684\u65b0\u95ee\u9898\uff1f","text":""},{"location":"learning/swintransformer/#_3","title":"\u89e3\u51b3\u7a97\u53e3\u53d8\u591a\u4e14\u4e0d\u4e00\u6837\u5927\u8ba1\u7b97 \u81ea\u6ce8\u610f\u529b\u7684\u95ee\u9898","text":"<p>\u539f\u6587\u4e2d\u7ed9\u7684\u89e3\u91ca\uff0c\u4e0d\u597d\u770b\u61c2\uff0cup\u4e3b\u7ed9\u51fa\u4e86\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u5f0f</p> <p></p> <p></p>"},{"location":"learning/swintransformer/#shift-windowmask","title":"shift window\u548cmask","text":""},{"location":"learning/swintransformer/#35-mask","title":"\u533a\u57df3\u548c\u533a\u57df5 \u7684mask","text":""},{"location":"learning/swintransformer/#sw-msa","title":"\u7ee7\u7eed\u8ba8\u8bba SW-MSA","text":"<p>\u79fb\u52a8\u7684\u89c4\u5f8b\uff0c\u5148\u628a\u4e0a\u9762\u79fb\u52a8\u5230\u4e0b\u9762\uff0c\u518d\u628a\u5de6\u8fb9\u79fb\u52a8\u5230\u53f3\u8fb9</p> <p></p> <p>\u6ce8\u610f\u770b \u8fd9\u91cc\u9ec4\u8272 \u6bcf\u4e2awindow\u90fd\u80fd\u878d\u5408\u56db\u4e2awindow\u7684\u4fe1\u606f</p> <p>\u5bf9\u4e8e\u7d2b\u8272\u7684\u533a\u57df\u5e76\u4e0d\u662f\u8fde\u7eed\u7684\uff0c\u9700\u8981\u4f7f\u7528mask-msa</p> <p>\u533a\u5206\u597d \u54ea\u91cc\u662f\u8fde\u7eed\u7684 \u54ea\u91cc\u4e0d\u662f\u8fde\u7eed\u7684</p> <p></p> <p>\u8bb0\u5f97\u770b\u4ee3\u7801\u600e\u4e48\u5b9e\u73b0\uff0cup\u4e3b\u4e5f\u8bb2\u4e86</p>"},{"location":"learning/swintransformer/#relative-position-bias","title":"\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb\uff1frelative position bias","text":""},{"location":"learning/swintransformer/#_4","title":"\u4ec0\u4e48\u662f\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\uff1f","text":"<p>\u533a\u5206 \u76f8\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u548c\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e</p> <p></p>"},{"location":"learning/swintransformer/#_5","title":"\u4e8c\u5143\u5750\u6807\u600e\u4e48\u8f6c\u5316\u6210\u4e00\u5143\u5750\u6807","text":""},{"location":"learning/swintransformer/#_6","title":"\u4ece\u76f8\u5bf9\u4f4d\u7f6e\u7d22\u5f15\u5230\u76f8\u5bf9\u4f4d\u7f6e\u504f\u79fb","text":"<p>\u6211\u4eec\u8981\u8bad\u7ec3\u7684\u53c2\u6570\u662f relative Position bias table</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"learning/swintransformer/#_7","title":"\u6a21\u578b\u8be6\u7ec6\u914d\u7f6e\u53c2\u6570","text":""},{"location":"learning/vit/","title":"ViT \u5b66\u4e60\u7b14\u8bb0","text":""},{"location":"literature/ObejectCounting/","title":"\u7d22\u5f15\u9875","text":""},{"location":"literature/ObejectCounting/#todo","title":"TODO","text":"<ul> <li> \u6574\u7406\u8bfb\u8fc7\u7684\u6587\u732e</li> </ul>"},{"location":"literature/ObejectCounting/#_2","title":"\u4e92\u8054\u7f51\u8d44\u6e90","text":"<ul> <li>CCF\u671f\u520a\u5206\u533a\u67e5\u8be2</li> </ul> <ul> <li>Object Counting on FSC147 in paper with code</li> </ul> <ul> <li>\u90d1\u4e4b\u6770 \u76ee\u6807\u8ba1\u6570(Object Counting) </li> </ul> <p>Tips</p> <p>\u9605\u8bfb\u903b\u8f91\uff1a</p> <ol> <li>\u6458\u8981\u3001\u5f15\u8a00-\u8d21\u732e\u3001\u7ed3\u8bba\uff08\u6709\u4e9b\u4f1a\u7ed9\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff09  </li> <li>introdution\u76f8\u5f53\u4e8e \u7814\u7a76\u80cc\u666f\u53ca\u610f\u4e49 \u7ed9\u51fa motivation  </li> <li>related work \u4f1a\u7ed9\u51fa\u7814\u7a76\u73b0\u72b6  </li> <li>method \u90e8\u5206 \u5173\u6ce8\u5c0f\u6807\u9898  </li> <li>experiment \u90fd\u505a\u4e86\u4ec0\u4e48\u5b9e\u9a8c \uff1a\u6cdb\u5316\uff08\u6570\u636e\u96c6\u3001\u4efb\u52a1\uff09\u3001\u5bf9\u6bd4\uff08\u65b9\u6cd5\uff09\u3001\u6d88\u878d \uff08\u6a21\u5757\uff09</li> </ol> <p>\u4e00\u70b9\u6709\u8da3\u7684\u53d1\u73b0\uff1a</p> <p>GeCo\u4f5c\u8005\uff1a27 Sep 2024 \u00b7 Jer Pelhan, Alan Luke\u017ei\u010d, Vitjan Zavrtanik, Matej Kristan</p> <p>LOCA\u4f5c\u8005\uff1a ICCV 2023  \u00b7 Nikola Djukic, Alan Lukezic, Vitjan Zavrtanik, Matej Kristan </p> <p>DAVE\u4f5c\u8005\uff1a25 Apr 2024 \u00b7 Jer Pelhan, Alan Luke\u017ei\u010d, Vitjan Zavrtanik, Matej Kristan \uff08\u4ed3\u5e93\u7684\u5171\u540c\u4f5c\u8005\u4e4b\u4e00\uff1aCounTR\uff09\uff08Jer\uff1aDAVE &amp; GeCo\uff09</p> <p>CounTR\u4f5c\u8005\uff1a29 Aug 2022 \u00b7 Chang Liu, Yujie Zhong, Andrew Zisserman, Weidi Xie  SHJT</p> <p>semAug counTR\u4f5c\u8005\uff1a26 Oct 2023 \u00b7 Perla Doubinsky, Nicolas Audebert, Michel Crucianu, Herv\u00e9 Le Borgne \u00b7</p> <p>SemAug-SAFECount\uff1a26 Oct 2023 \u00b7 Perla Doubinsky, Nicolas Audebert, Michel Crucianu, Herv\u00e9 Le Borgne \u00b7</p> <p></p> <p>22 Jan 2022 \u00b7 Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le \uff08Tsinghua\u3001SHJT\uff09</p>"},{"location":"literature/ObejectCounting/#1","title":"1","text":"<ul> <li>multi-scale feature fusion module</li> <li>Transformer \u2192ViT\u2192SwinTransformer</li> <li>GAN\u2192diffusion\uff08text to image\uff09</li> <li>multimodle\uff1aclip\uff08text and image\uff09\uff1buser interaction</li> <li>SPDCN \u4fee\u6539\u4e86 \u635f\u5931\u51fd\u6570 \uff1b\u6839\u636e\u793a\u4f8b\u5c3a\u5bf8\u7684\u4e0d\u540c \u8c03\u6574\u635f\u5931\u51fd\u6570\u7684\u5f62\u5f0f</li> </ul>"},{"location":"literature/ObejectCounting/#2","title":"2","text":"<ul> <li>\u8ba1\u6570\u65b9\u6cd5  rank8 CounTR</li> <li>\u7c7b\u65e0\u5173\u8ba1\u6570 rank8 CounTR</li> <li>\u6570\u636e\u751f\u6210 rank7 SemAug CounTR</li> </ul>"},{"location":"literature/ObejectCounting/#_3","title":"\u6587\u732e\u7efc\u8ff0","text":"<ol> <li> <p>FamNet</p> </li> <li> <p>SAFECount</p> </li> <li> <p>GMN</p> </li> <li> <p>BMNet</p> </li> <li> <p>CounTR</p> </li> <li> <p>CountGD</p> </li> </ol>"},{"location":"literature/ObejectCounting/rank1%20CountGD/","title":"rank1 CountGD","text":""},{"location":"literature/ObejectCounting/rank1%20CountGD/#_1","title":"\u591a\u6a21\u6001\u7279\u5f81\u5f00\u653e\u4e16\u754c\u76ee\u6807\u8ba1\u6570","text":"<p>2024\u5e747\u67085\u65e5 \u53d1\u8868</p> <p>5 Jul 2024 \u00b7 Niki Amini-Naieni, Tengda Han, Andrew Zisserman \u00b7 </p> <p></p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#countgd-multi-modal-open-world-counting","title":"CountGD: Multi-Modal Open-World Counting","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u4e3a\u4e86\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4ec0\u4e48\u6837\u7684\u89e3\u51b3\u65b9\u6cd5\u3002</p> <p>\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u4fe1\u53f7\uff0c\u4e00\u8d77\u8fdb\u884c\u8ba1\u6570\uff0c\u6587\u672c\u63cf\u8ff0\u6bd4\u5982\u4f1a\u8fc7\u6ee4\u989c\u8272\uff0c\u4f4d\u7f6e\u7b49\uff1b</p> <p>\u4e00\u53e5\u8bdd\u603b\u7ed3\u672c\u6587\uff1aHere, we describe COUNTGD, a single-stage model for open-world object counting that accepts either visual exemplars or text or both together as prompts to specify the object to count.</p> <p>\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86COUNTGD\uff0c\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\u7684\u5355\u9636\u6bb5\u6a21\u578b\uff0c\u5b83\u63a5\u53d7\u89c6\u89c9\u6837\u672c\u6216\u6587\u672c\u6216\u4e24\u8005\u5171\u540c\u4f5c\u4e3a\u63d0\u793a\u6765\u6307\u5b9a\u8981\u8ba1\u6570\u7684\u7269\u4f53\u3002</p> <p>Note</p> <p>COUNTGD\u7684\u51e0\u4e2a\u5173\u952e\u8bcd\uff0c\u5f00\u653e\u4e16\u754c\u7684\u7269\u4f53\u8ba1\u6570\uff0c\u5355\u9636\u6bb5\u6a21\u578b\uff0c\u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7</p> <p>\u6587\u672c\u7279\u5f81\u54ea\u91cc\u6765\u5462\uff1f</p> <ul> <li>\u5bf9\u4e8eFSC147\u6570\u636e\u96c6\uff0cFor text descriptions, we use the singular forms of the class names in <code>FSC-147-D [1]</code> with any prefixes such as \u201cthe\" removed. For example, we change \u201cthe donuts in the donut tray\" in FSC-147-D to \u201cdonut\" by removing the prefix \u201cthe,\" extracting the class name \u201cdonuts,\" and then singularizing it to \u201cdonut.\"  \u6258\u76d8\u4e2d\u7684\u751c\u751c\u5708 \\(\\rightarrow\\) \u751c\u751c\u5708</li> <li>CARPK\uff1aWe use the class name \u201ccar\" as the text description.</li> </ul>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#_2","title":"\u6458\u8981","text":"<p>The goal of this paper is to improve the generality and accuracy of open-vocabulary object counting in images. </p> <p>\u4e3a\u4e86\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\uff0c\u8ba1\u6570\u73b0\u5b9e\u8bcd\u8868\u4e2d\u7684\u4e00\u5207\u7269\u4f53</p> <p>To improve the generality, we repurpose an open vocabulary detection foundation model (GroundingDINO) for the counting task, and also extend its capabilities by introducing modules to enable specifying the target object to count by visual exemplars. </p> <p>\u4e3a\u4e86\u63d0\u9ad8\u6cdb\u5316\u6027\uff0c\u91cd\u65b0\u8bbe\u8ba1\u57fa\u4e8e\u5f00\u653e\u8bcd\u8868\u7684\u68c0\u6d4b\u6a21\u578b\uff1aGroundingDINO</p> <p>\u589e\u52a0\u6a21\u5757\uff0c\u901a\u8fc7\u6837\u4f8b\u6846\u6307\u5b9a\u8ba1\u6570\u76ee\u6807</p> <p>In turn, these new capabilities \u2013 being able to specify the target object by multi-modalites (text and exemplars) \u2013 lead to an improvement in counting accuracy. </p> <p>\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u6307\u5b9a\u8ba1\u6570\u76ee\u6807\uff0c\u63d0\u9ad8\u8ba1\u6570\u7684\u51c6\u786e\u6027</p> <p>we introduce the first open-world counting model, COUNTGD, where the prompt can be specified by a text description or visual exemplars or both; </p> <p>\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\uff1aCOUNTGD\uff0c\u53ef\u4ee5\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff1a\u6587\u672c\u3001\u89c6\u89c9\u6837\u4f8b\u3001\u6216\u8005\u4e24\u4e2a\u90fd</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#-","title":"\u5f15\u8a00-\u8d21\u732e","text":"<p>In summary, we make the following three contributions: </p> <p>First, we introduce COUNTGD, the first openworld object counting model that accepts either text or visual exemplars or both simultaneously, in a single-stage architecture; </p> <p>\u5355\u4e00\u7ed3\u6784\uff0c\u540c\u65f6\u63a5\u6536\u6587\u672c\u4fe1\u53f7\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u8ba1\u6570</p> <p>Second, we evaluate the model on multiple standard counting benchmarks, including FSC-147 [39], CARPK [18] and CountBench [36], and show that COUNTGD significantly improves on the state-of-the-art performance by specifying the target object using both exemplars and text. It also meets or improves on the state-of-the-art for text-only approaches when trained and evaluated using text-only; </p> <p>\u672c\u6587\u6240\u7528\u6570\u636e\u96c6\uff1aFSC-147 [39], CARPK [18] and CountBench [36]</p> <p>Third, we investigate how the text can be used to refine the visual information provided by the exemplar, for example by filtering on color or relative position in the image, to specify a sub-set of the objects to count.</p> <p>\u6587\u672c\u662f\u5982\u4f55\u7ec6\u5316\u7531\u6837\u4f8b\u63d0\u4f9b\u7684\u89c6\u89c9\u4fe1\u606f\u7684\uff0c\u6bd4\u5982\uff1a\u901a\u8fc7\u5bf9\u56fe\u50cf\u4e2d\u7684\u989c\u8272\u6216\u76f8\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u8fc7\u6ee4\uff0c\u6765\u6307\u5b9a\u8981\u8ba1\u6570\u7684\u5bf9\u8c61\u7684\u5b50\u96c6</p> <p>In addition we make two minor improvements to the inference stage: one that addresses the problem of double counting due to self-similarity, and the other to handle the problem of a very high count.</p> <p>\u63a8\u7406\u9636\u6bb5\u7684\u4e24\u4e2a\u6539\u8fdb\uff1a</p> <ul> <li>\u7531\u4e8e\u81ea\u76f8\u4f3c\u6027\u7684\u91cd\u590d\u8ba1\u6570\u95ee\u9898</li> <li>\u6781\u5ea6\u5bc6\u96c6\u573a\u666f\u7684\u8ba1\u6570\u95ee\u9898</li> </ul> <p>\u76ee\u6807\u8ba1\u6570\u7684\u4e24\u5927\u95ee\u9898\uff1a</p> <p>\u7269\u4f53\u5806\u53e0\u5bfc\u81f4\u7684\u91cd\u590d\u8ba1\u6570</p> <p>\u5bc6\u96c6\u573a\u666f\u7684\u8ba1\u6570\u95ee\u9898 </p> \u96be\u9053\u4e0d\u662f\u540c\u4e00\u4e2a\u95ee\u9898\uff1f"},{"location":"literature/ObejectCounting/rank1%20CountGD/#conclusion-future-work","title":"Conclusion &amp; Future Work","text":"<p>We have extended the generality of open-world counting by introducing a model that can accept visual exemplars or text descriptions or both as prompts to specify the target object to count. </p> <p>\u7b2c\u4e00\u70b9\u6211\u4eec\u8fdb\u884c\u4e86\u5f00\u653e\u4e16\u754c\u7684\u8bed\u4e49\u7269\u4f53\u8ba1\u6570\u95ee\u9898\uff0c\u63a5\u6536\u6587\u672c\u63cf\u8ff0\u548c\u793a\u4f8b\u6846\u4fe1\u606f\u6216\u8005\u5171\u540c</p> <p>me\uff1a\u5f00\u653e\u4e16\u754c\u7684\u8bed\u4e49\u7269\u4f53\u8ba1\u6570\u3001\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u793a\u4f8b\u6846</p> <p>\u6587\u672c\u63cf\u8ff0\u662f\u5bf9\u793a\u4f8b\u6846\u7269\u4f53\u7684\u9009\u62e9\u5f15\u5165\u989d\u5916\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u77ed\u8bed\u9650\u5236</p> <p>The complementarity of these prompts in turn leads to improved counting performance. </p> <p>\u672a\u6765\u7684\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff1a</p> <p>There are three research directions that naturally follow on from this work: </p> <p>(i) the performance could probably be further improved by training on larger scale datasets, for example using synthetic data as demonstrated recently for counting [24]; </p> <p>\u7b2c\u4e00\u4e2a\u7814\u7a76\u65b9\u5411\uff1a\u6269\u5c55\u8f93\u5165\u6570\u636e\u7684\u4e30\u5bcc\u6027\uff0c\u6bd4\u5982\u5408\u6210\u6570\u636e\uff0c\u545c\u545c\u545c\u545c\uff0c\u8ddf\u6211\u60f3\u7684\u4e00\u6837 </p> <p>(ii) a larger training set would enable a thorough investigation of freezing more of the GroundingDINO model when adding our new visual exemplar modules; and finally, </p> <p>\u66f4\u5927\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5bf9GroundingDINO\u6a21\u578b\u8fdb\u884c\u66f4\u5f7b\u5e95\u7684\u5b9e\u9a8c</p> <p>(iii) the model does not currently predict the errors of its counting. We discuss this point in the Limitations in the Appendix.</p> <p>Note</p> <p>countGD\uff1b\u5f00\u653e\u4e16\u754c\u7684\u8bed\u4e49\u7269\u4f53\u8ba1\u6570\u95ee\u9898\uff1b\u6cdb\u5316\u6027\u51c6\u786e\u6027\uff1btext and exemplar</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#related-work","title":"Related Work","text":"<p>Prior work on object counting has developed along three axes: \u76ee\u6807\u8ba1\u6570\u7684\u4e09\u4e2a\u7814\u7a76\u7ef4\u5ea6</p> <p>(1) the density map versus detection axis, \u57fa\u4e8e\u56de\u5f52\u7684 &amp; \u57fa\u4e8e\u68c0\u6d4b\u7684</p> <p>(2) the class-specific versus open-world (also referred to as \u201cclass-agnostic\") axis, and \u7279\u5b9a\u7c7b\u522b\u8ba1\u6570 &amp; \u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\uff08\u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570\uff09</p> <p>(3) the visual exemplar versus text axis.  \u57fa\u4e8e\u89c6\u89c9\u4fe1\u53f7\u7684\u8ba1\u6570 \u548c \u57fa\u4e8e\u6587\u672c\u7684\u8ba1\u6570</p> <p>The pattern is that detection, open-world, and text-based methods tend to offer more capabilities and be more general than their analogues along each axis. </p> <p>\u57fa\u4e8e\u68c0\u6d4b\u3001\u5f00\u653e\u4e16\u754c\u3001\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u6cdb\u5316\u6027\u66f4\u597d</p> <p>On the other hand, density map, class-specific, and visual exemplar-based methods tend to be more accurate at the counting tasks they apply to. </p> <p>\u57fa\u4e8e\u56de\u5f52\u5bc6\u5ea6\u56fe\u3001\u7279\u5b9a\u7c7b\u522b\u3001\u89c6\u89c9\u6837\u4f8b\u6846\u7684\u51c6\u786e\u6027\u66f4\u597d</p> <p>COUNTGD integrates the third axis \u2013 the visual exemplar versus text axis \u2013 to achieve more general and accurate counting overall. Below, we discuss where prior work falls along each axis and where COUNTGD stands.</p> <p>COUNTGD\u6574\u5408\u4e86\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e2\u7528\u6587\u672c\uff0c\u53c8\u7528\u793a\u4f8b\u6846</p> <p>Note</p> <p>\u6211\u4eec\u8fd9\u4e2acounGD\u6574\u5408\u4e86\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e2\u6709\u89c6\u89c9\u4fe1\u53f7\u53c8\u6709\u6587\u672c\u4fe1\u53f7\uff0c\u518d\u6b21\u5f3a\u8c03\uff0c\u6587\u672c\u4fe1\u53f7\u6cdb\u5316\u6027\u597d\u3001\u89c6\u89c9\u4fe1\u53f7\u51c6\u786e\u6027\u597d\uff0c\u56e0\u6b64\u65e2\u6709\u89c6\u89c9\u4fe1\u53f7\u53c8\u6709\u6587\u672c\u4fe1\u53f7\u7684\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\u90fd\u5f88\u597d\u3002\u63a5\u4e0b\u6765\u8ba8\u8bba\u5148\u524d\u7684\u5de5\u4f5c\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u53d1\u5c55\u4ee5\u53caCOUNTGD</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#density-map-versus-detection-based-object-counting-axis-1","title":"Density Map versus Detection-based Object Counting (Axis 1).","text":"<p>\u7ef4\u5ea6\u4e00\uff1a\u57fa\u4e8e\u5bc6\u5ea6\u548c\u57fa\u4e8e\u68c0\u6d4b</p> <p>Density Map versus Detection-based Object Counting (Axis 1). </p> <p>In the past, counting techniques that regress and sum density maps [2, 3, 6, 25, 26, 33, 42], instead of detecting and enumerating bounding boxes [5, 8, 18, 35], have proven more accurate in cluttered and dense scenes. \u5728\u5148\u524d\u7684\u5de5\u4f5c\u4e2d\uff0c\u5df2\u7ecf\u8bc1\u660e\u4e86\uff0c\u57fa\u4e8e\u5bc6\u5ea6\u7684\u7269\u4f53\u8ba1\u6570\u65b9\u6cd5\u5728\u5bc6\u96c6\u573a\u666f\u4e0b\u7684\u8ba1\u6570\u9002\u7528\u6027</p> <p>For example, density map-based approaches like CounTX [1], LOCA [10], and CounTR [29] achieve lower counting errors than detection-based approaches such as Mask-RCNN [16] and RetinaNet [27] on standard counting benchmarks. </p> <p>\u4e3e\u4f8b\u8bf4\u660e\uff0c\u57fa\u4e8e\u5bc6\u5ea6\u6bd4\u57fa\u4e8e\u68c0\u6d4b\u7684\u53d1\u5c55\u4f18\u52bf\u3002</p> <p>Concurrent to our work, DAVE [37], integrates density map regression with object detection to construct a more accurate and explainable two-stage counting system. Like DAVE, COUNTGD outputs explicit object locations.</p> <p>\u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u76f8\u4e00\u81f4\uff0cDAVE [ 37 ]\u5c06\u5bc6\u5ea6\u56fe\u56de\u5f52\u4e0e\u76ee\u6807\u68c0\u6d4b\u7ed3\u5408\u8d77\u6765\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u66f4\u7cbe\u786e\u548c\u53ef\u89e3\u91ca\u7684\u4e24\u9636\u6bb5\u8ba1\u6570\u7cfb\u7edf\u3002\u4e0eDAVE\u4e00\u6837\uff0cCOUNTGD\u8f93\u51fa\u660e\u786e\u7684\u5bf9\u8c61\u4f4d\u7f6e\u3002</p> <p>Note</p> <p>DAVE\u5bc6\u5ea6\u56fe\u56de\u5f52\u548c\u7269\u4f53\u68c0\u6d4b\uff0c\u8f93\u51fa\u76ee\u6807\u7684\u4f4d\u7f6e \u4e24\u9636\u6bb5\uff0c\u51c6\u786e\u6027&amp;\u6cdb\u5316\u6027</p> <p>However, COUNTGD is a single-stage approach that achieves better counting accuracy than DAVE and other density map-based techniques.</p> <p>\u7136\u800c\uff0cCOUNTGD\u662f\u4e00\u79cd\u5355\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u5176\u8ba1\u6570\u7cbe\u5ea6\u4f18\u4e8eDAVE\u548c\u5176\u4ed6\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u6280\u672f\u3002</p> <p>Note</p> <p>\u55ef\uff0cDAVE\u4e24\u9636\u6bb5\uff0c\u6211\u4e00\u9636\u6bb5\uff0c\u800c\u4e14\u662f text&amp;exemplar</p> <p>Therefore,while density map-based approaches tend to be more accurate than detectors in highly populated scenes, recent detection-based techniques, including COUNTGD, are beginning to achieve better accuracy than density map-based alternatives.</p> <p>\u867d\u7136\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u65b9\u6cd5\u5728\u4eba\u53e3\u7a20\u5bc6\u7684\u573a\u666f\u4e2d\u5f80\u5f80\u6bd4\u68c0\u6d4b\u5668\u66f4\u51c6\u786e\uff0c\u4f46\u6700\u8fd1\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u6280\u672f\uff0c\u5305\u62ecCOUNTGD\uff0c\u5df2\u7ecf\u5f00\u59cb\u53d6\u5f97\u6bd4\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002</p> <p>Note</p> <p>\u867d\u7136\u57fa\u4e8e\u5bc6\u5ea6\u7684\u5f88\u597d\uff0c\u4f46\u6700\u8fd1\u57fa\u4e8e\u68c0\u6d4b\u7684\u53d1\u5c55\u4e0d\u7518\u793a\u5f31 COUNTGD \u5c31\u662f\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570</p> \u4e3a\u4ec0\u4e48\u53ebCountGD\uff1f"},{"location":"literature/ObejectCounting/rank1%20CountGD/#_3","title":"\u7ef4\u5ea6\u4e8c\uff1a\u7279\u5b9a\u7c7b\u522b \u5bf9\u6bd4 \u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570","text":"<p>Class-specific versus Open-world Object Counting (Axis 2). </p> <p>Object counting methods first developed as class-specific techniques [3, 4, 34, 42], solving the counting problem for only one category of object, but recent methods have generalized these approaches to open-world settings, where counting arbitrary objects is possible. \u5c31\u662f\u8bf4\uff0c\u6700\u5f00\u59cb\u53d1\u5c55\u7684\u5bf9\u7279\u5b9a\u7c7b\u522b\u7684\u8ba1\u6570\uff0c\u540e\u6765\u6f14\u53d8\u6210\u5bf9\u4efb\u610f\u7269\u4f53\u7684\u8ba1\u6570\u95ee\u9898</p> <p>Class-specific methods have been developed to count cars [22], humans [4], and cells [13]. In contrast, open-world methods can count instances from all three categories [32]. </p> <p>\u4e3e\u4f8b\u5177\u4f53\u8bf4\u660e</p> <p>Because class-specific techniques are more specialized than open-world approaches, they tend to be more accurate at counting instances from the class they were designed for. </p> <p>\u5c31\u662f\u8bf4\uff0c\u9488\u5bf9\u7279\u5b9a\u7c7b\u522b\u7684\u7269\u4f53\u8ba1\u6570\u51c6\u786e\u6027\u786e\u5b9e\u5f88\u597d</p> <p>Recent advancements in Vision-Language Foundation Models (VLMs) such as CLIP [38] and GroundingDINO [30] trained on web-scale image-text pairs produce semantically rich visual and textual features. </p> <p>\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b( Vision-Language Foundation Models\uff0cVLMs )\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5982CLIP [ 38 ]\u548cGroundingDINO [ 30 ]\uff0c\u5728\u7f51\u7edc\u89c4\u6a21\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ea7\u751f\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u3002</p> <p>Note</p> <p>emm\u5728\u8fd9\u4e48\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6700\u540e\u5c31\u662f\u5f97\u5230\u4e30\u5bcc\u7684\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81</p> <p>These features generalize to a wide range of open-world downstream tasks. Building on top of these pre-trained VLMs, recent open-world methods [1, 7, 10, 21, 29, 40, 45] have begun to surpass class-specific approaches in counting accuracy. COUNTGD, like these recent approaches, is an open-world object counter that achieves competitive performance in comparison to class-specific alternatives.</p> <p>\u8fd9\u4e9b\u7279\u5f81\u6cdb\u5316\u5230\u4e86\u5e7f\u6cdb\u7684\u5f00\u653e\u4e16\u754c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002\u5728\u8fd9\u4e9b\u9884\u8bad\u7ec3\u7684VLMs\u7684\u57fa\u7840\u4e0a\uff0c\u6700\u8fd1\u7684\u5f00\u653e\u4e16\u754c\u65b9\u6cd5[ 1\u30017\u300110\u300121\u300129\u300140\u300145]\u5df2\u7ecf\u5f00\u59cb\u8d85\u8d8a\u7279\u5b9a\u7c7b\u522b\u7684\u65b9\u6cd5\u5728\u8ba1\u6570\u7cbe\u5ea6\u4e0a\u3002\u4e0e\u8fd9\u4e9b\u6700\u8fd1\u7684\u65b9\u6cd5\u4e00\u6837\uff0cCOUNTGD\u662f\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u7684\u5bf9\u8c61\u8ba1\u6570\u5668\uff0c\u4e0e\u7279\u5b9a\u7c7b\u7684\u66ff\u4ee3\u54c1\u76f8\u6bd4\uff0c\u5b83\u5177\u6709\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002</p> <p>Note</p> <p>\u83b7\u5f97\u8bed\u4e49\u66f4\u52a0\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u53ef\u4ee5\u66f4\u597d\u7684\u6cdb\u5316\u5230\u4e0b\u6e38\u4efb\u52a1\u3002   </p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#_4","title":"\u89d2\u5ea6\u4e09\uff1a\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81","text":"<p>Counting with Visual Exemplars versus Counting with Text (Axis 3).</p> <p>Most open-world object counters approach the problem by using visual exemplars to select the objects in the input image [10, 14, 28, 29, 32, 35, 39, 40, 44, 45], but very recent work [1, 7, 19, 21, 43] has attempted to replace the visual exemplars with text, enabling new capabilities at the cost of reduced accuracy. The stateof-the-art text-based approaches, such as GroundingREC [7], CounTX [1], CLIP-Count [19], and VLCounter [21] are built on top of vision-language foundation models pretrained on large quantities of data to relate images to textual inputs and map them to a joint embedding space. </p> <p>\u5927\u591a\u6570\u5f00\u653e\u4e16\u754c\u5bf9\u8c61\u8ba1\u6570\u5668\u901a\u8fc7\u4f7f\u7528\u89c6\u89c9\u6837\u672c\u6765\u9009\u62e9\u8f93\u5165\u56fe\u50cf[ 10\u300114\u300128\u300129\u300132\u300135\u300139\u300140\u300144\u300145]\u4e2d\u7684\u5bf9\u8c61\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u6700\u8fd1\u7684\u5de5\u4f5c[ 1\u30017\u300119\u300121\u300143]\u5c1d\u8bd5\u7528\u6587\u672c\u4ee3\u66ff\u89c6\u89c9\u6837\u672c\uff0c\u4ee5\u964d\u4f4e\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\u6765\u5b9e\u73b0\u65b0\u7684\u529f\u80fd\u3002\u76ee\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u5982GroundingREC [ 7 ]\uff0cCounTX [ 1 ]\uff0cCLIP-Count [ 19 ]\u548cVLCounter [ 21 ]\uff0c\u90fd\u662f\u5efa\u7acb\u5728\u57fa\u4e8e\u5927\u91cf\u6570\u636e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4e4b\u4e0a\uff0c\u5c06\u56fe\u50cf\u4e0e\u6587\u672c\u8f93\u5165\u76f8\u5173\u8054\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u4e00\u4e2a\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u3002</p> <p>Note</p> <p>\u4e4b\u524d\u7684\u8ba1\u6570\u65b9\u6cd5\u90fd\u662f\u901a\u8fc7\u793a\u4f8b\u6846\u9009\u62e9\u8f93\u5165\u56fe\u50cf\u7684\u76ee\u6807\uff1b\u6700\u8fd1\u7684\u5de5\u4f5c\u5f00\u59cb\u4f7f\u7528\u6587\u672c\uff0c\u8fd8\u8bb0\u5f97\u5427\uff0c\u6587\u672c\u4fe1\u53f7\u6cdb\u5316\u6027\u597d\uff0c\u89c6\u89c9\u4fe1\u53f7\u51c6\u786e\u6027\u597d\uff0c\u56e0\u6b64\u5f53\u4f7f\u7528\u6587\u672c\u4fe1\u53f7\u65f6\uff0c\u662f\u727a\u7272\u4e86\u51c6\u786e\u6027\u3002\u7136\u540e\u5c31\u8bf4\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u9884\u6d4b\u65b9\u6cd5</p> <p>This allows these foundation models to understand general concepts learned during extensive pretraining and provides a mechanism for users to specify extrinsic object properties through text. However, text-based approaches perform significantly worse than state-of-the-art visual exemplar-based approaches such as LOCA [10], CounTR [29], and few-shot DAVE [37]. For example, while both GroundingREC and COUNTGD use the pretrained GroundingDINO [30] vision-language foundation model, unlike GroundingREC, COUNTGD allows the user to input both visual exemplars and text instead of just text. This enables COUNTGD to achieve superior counting accuracy in comparison to GroundingREC.</p> <p>\u8fd9\u4f7f\u5f97\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u80fd\u591f\u7406\u89e3\u5728\u5e7f\u6cdb\u7684\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u5230\u7684\u4e00\u822c\u6982\u5ff5\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u6307\u5b9a\u5916\u90e8\u5bf9\u8c61\u5c5e\u6027\u7684\u673a\u5236\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u660e\u663e\u5dee\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u89c6\u89c9\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u5982LOCA [ 10 ]\uff0cCounTR [ 29 ]\u548c\u5c0f\u6837\u672cDAVE [ 37 ]\u3002\u4f8b\u5982\uff0cGroundingREC\u548cCOUNTGD\u90fd\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3\u7684GroundingDINO [ 30 ]\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u4e0eGroundingREC\u4e0d\u540c\u7684\u662f\uff0cCOUNTGD\u5141\u8bb8\u7528\u6237\u540c\u65f6\u8f93\u5165\u89c6\u89c9\u793a\u4f8b\u548c\u6587\u672c\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6587\u672c\u3002\u8fd9\u4f7f\u5f97COUNTGD\u76f8\u6bd4GroundingREC\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u6570\u7cbe\u5ea6\u3002</p> <p>Note</p> <p>\u4f60\u77e5\u9053\u7684\u5427\uff0c\u5728\u89c6\u89c9\u6587\u672c\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u5b66\u5230\u4e00\u822c\u6982\u5ff5\uff0c\u6240\u4ee5\u6cdb\u5316\u6027\u66f4\u597d\u3002\u4f46\u8fd8\u662f\u90a3\u53e5\u8bdd\uff0c\u51c6\u786e\u6027\u4e0d\u591f\u3002\u8fd9\u91cc\u8fd8\u8bf4\u4e86\u4e0eCountGD\u5de5\u4f5c\u5f88\u76f8\u4f3c\u7684\u6a21\u578bGroundingREC\uff0c\u4f46\u662f\u8f93\u5165\u4fe1\u53f7\u4e0d\u4e00\u6837\uff0c\u76f8\u4f3c\u5728\u4e8e\u90fd\u662f\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684GroundingDINO\u3002\u4e0d\u76f8\u4f3c\u5728\u4e8e\u8f93\u5165\u4fe1\u53f7\u4e0d\u4e00\u6837\u7684</p> <p>Notably, DAVE [37] is a visual exemplar-based approach that also enables textual prompts, but differs from COUNTGD in three important ways:COUNTGD \u4e0eDAVE\u7684\u4e09\u4e2a\u663e\u8457\u4e0d\u540c</p> <p>(1) it does not address the case when both text and visual exemplars are available while COUNTGD does,\u89c6\u89c9\u4fe1\u53f7\u548c\u6587\u672c\u4fe1\u53f7\u90fd\u6765</p> <p>(2) its comparison between text features and image features is not learned as it is by COUNTGD with attention, and     COUNTGD\u6709\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u6587\u672c\u7279\u5f81\u548c\u56fe\u50cf\u7279\u5f81</p> <p>(3) it is a two-stage approach, while COUNTGD solves the problem in a single stage, without relying on another visual exemplar-based counting model. DAVE\u4e24\u9636\u6bb5\u68c0\u6d4b\u65b9\u6cd5</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDAVE [ 37 ]\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u6837\u4f8b\u7684\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u6587\u672c\u63d0\u793a\uff0c\u4f46\u4e0eCOUNTGD\u67093\u4e2a\u91cd\u8981\u7684\u533a\u522b\uff1a( 1 )\u5b83\u6ca1\u6709\u89e3\u51b3COUNTGD\u540c\u65f6\u63d0\u4f9b\u6587\u672c\u548c\u89c6\u89c9\u6837\u4f8b\u7684\u60c5\u51b5\uff1b( 2 )\u5b83\u6ca1\u6709\u50cfCOUNTGD\u90a3\u6837\u5728\u6709\u6ce8\u610f\u529b\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u6587\u672c\u7279\u5f81\u548c\u56fe\u50cf\u7279\u5f81\u4e4b\u95f4\u7684\u6bd4\u8f83\uff1b( 3 )\u5b83\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u800cCOUNTGD\u5728\u4e00\u4e2a\u9636\u6bb5\u4e2d\u89e3\u51b3\u95ee\u9898\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u53e6\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u6837\u4f8b\u7684\u8ba1\u6570\u6a21\u578b\u3002</p> <p>Note</p> <p>\u8fd9\u7bc7\u8bba\u6587\u7684\u6539\u8fdb\u8bba\u6587\u662fDAVE\uff0c\u76ee\u6807\u90fd\u662f\u4e00\u6837\u7684\uff0cmotivation\uff1a\u63d0\u9ad8\u51c6\u786e\u7387 &amp; \u53ec\u56de\u7387</p>"},{"location":"literature/ObejectCounting/rank1%20CountGD/#relation-of-counting-to-other-areas","title":"Relation of Counting to other areas. \u4e0e\u5176\u5b83\u9886\u57df\u5de5\u4f5c\u7684\u5173\u7cfb","text":"<p>\u8ddf\u5f00\u5c71\u4e4b\u4f5c\u7684\u76f8\u5173\u5de5\u4f5c\u53d9\u8ff0\u6709\u70b9\u50cf</p> <p>Our work is related to few-shot image classification [41] and image detection [12, 20] methods.   \u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u548c\u68c0\u6d4b</p> <p>These works require a few query images of novel objects, and then compare the test image with these image examples to determine its semantic content (for image classification), or to spatially localize instances (for object detection). </p> <p>\u6211\u4eec\u7684\u5de5\u4f5c\u4e0e\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b[ 41 ]\u548c\u56fe\u50cf\u68c0\u6d4b[ 12\u300120]\u65b9\u6cd5\u76f8\u5173\u3002\u8fd9\u4e9b\u5de5\u4f5c\u9700\u8981\u4e00\u4e9b\u65b0\u9896\u5bf9\u8c61\u7684\u67e5\u8be2\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u6d4b\u8bd5\u56fe\u50cf\u4e0e\u8fd9\u4e9b\u56fe\u50cf\u793a\u4f8b\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u786e\u5b9a\u5176\u8bed\u4e49\u5185\u5bb9(\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b)\uff0c\u6216\u8005\u5bf9\u5b9e\u4f8b(\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b)\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d\u3002</p> <p>Like these methods, COUNTGD enables us to specify the object to count with visual exemplars (i.e., \u201cquery images\") but also allows for textual inputs, and then compares the test image with the multi-modal specifications to get the final count. Furthermore, we focus on the counting problem, a challenging task for object detectors.</p> <p>\u4e0e\u8fd9\u4e9b\u65b9\u6cd5\u4e00\u6837\uff0cCOUNTGD\u5141\u8bb8\u6211\u4eec\u7528\u53ef\u89c6\u5316\u793a\u4f8b(\u5373\"\u67e5\u8be2\u56fe\u50cf\")\u6307\u5b9a\u8981\u8ba1\u6570\u7684\u5bf9\u8c61\uff0c\u4f46\u4e5f\u5141\u8bb8\u6587\u672c\u8f93\u5165\uff0c\u7136\u540e\u5c06\u6d4b\u8bd5\u56fe\u50cf\u4e0e\u591a\u6a21\u6001\u89c4\u8303\u8fdb\u884c\u6bd4\u8f83\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u8ba1\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5173\u6ce8\u8ba1\u6570\u95ee\u9898\uff0c\u8fd9\u662f\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5\u3001\u5b8c\u6210\u8ba1\u6570\u4efb\u52a1\uff0c\u5176\u5b9e\u73b0\u5728\u7528\u57fa\u4e8e\u68c0\u6d4b\u7684\u7b97\u6cd5\u6765\u8fdb\u884c\u8ba1\u6570\u4efb\u52a1\u662f\u6bd4\u8f83\u5c11\u7684 </p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/","title":"rank10 SPDCN","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>\u5f15\u7528\uff1a2022\u5e74\u7684\u6587\u732e</p> <pre><code>@inproceedings{Lin_2022_BMVC,\nauthor    = {Wei Lin and Kunlin Yang and Xinzhu Ma and Junyu Gao and Lingbo Liu and Shinan Liu and Jun Hou and Shuai Yi and Antoni Chan},\ntitle     = {Scale-Prior Deformable Convolution for Exemplar-Guided Class-Agnostic Counting},\nbooktitle = {33rd British Machine Vision Conference 2022, {BMVC} 2022, London, UK, November 21-24, 2022},\npublisher = {{BMVA} Press},\nyear      = {2022},\nurl       = {https://bmvc2022.mpi-inf.mpg.de/0313.pdf}\n}\n</code></pre> <p>\u6807\u9898\uff1aScale-Prior Deformable Convolution for Exemplar-Guided Class-Agnostic Counting  \u57fa\u4e8e\u793a\u4f8b\u7684\uff0c\u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5377\u79ef\uff0c\u7c7b\u65e0\u5173\u8ba1\u6570\uff08\u7a81\u7136\u60f3\u8d77\u6765\uff0c\u5404\u79cd\u5377\u79ef\uff0c\u7a7a\u6d1e\u5377\u79ef\u3001\u8f6c\u7f6e\u5377\u79ef\u3001\u5206\u7ec4\u5377\u79ef\uff09</p> <p>\u4f5c\u8005\uff1aConference 2022  \u00b7 Wei Lin, Kunlin Yang, Xinzhu Ma, Junyu Gao, Lingbo Liu, Shinan Liu, Jun Hou, Shuai Yi, Antoni B. Chan    \u9999\u6e2f\u5927\u5b66</p> <p></p> <p>\u671f\u520a\uff1aBMVC2022\uff1bCCF-C\u7c7b</p> <p>\u672c\u6587 \u4e3a\u4e86\u89e3\u51b3.......\u63d0\u51fa\u4e86...........</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_1","title":"\u6458\u8981","text":"<p>CAC\u4efb\u52a1\u8fdb\u5c55\u4e0d\u9519(\u8fd9\u662f22\u5e74\u7684\u6587\u7ae0\uff0c\u73b0\u5728\u505a\u7684\u90fd\u662fCAC\u7684)</p> <p>Class-agnostic counting has recently emerged as a more practical counting task, which aims to predict the number and distribution of any exemplar objects, instead of counting specific categories like pedestrians or cars. </p> <p>\u73b0\u6709\u7684\u65b9\u6cd5</p> <ul> <li>\u8bbe\u8ba1\u5408\u9002\u7684\u76f8\u4f3c\u6027\u5339\u914d\u89c4\u5219 \u5728\u793a\u4f8b\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4</li> <li>\u5ffd\u7565\u4e86\u63d0\u53d6\u7279\u5f81\u7684\u9c81\u68d2\u6027</li> </ul> <p>However, recent methods are developed by designing suitable similarity matching rules between exemplars and query images, but ignoring the robustness of extracted features. </p> <p>\u4e3a\u4e86 \u63d0\u53d6\u7279\u5f81\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa  \u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5377\u79ef\uff0c\u6574\u5408\u793a\u4f8b\u4fe1\u606f</p> <p>To address this issue, we propose a scale-prior deformable convolution by integrating exemplars\u2019 information, e.g., scale, into the counting network backbone.\uff08\u6548\u679c\uff09As a result, the proposed counting network can extract semantic features of objects similar to the given exemplars and effectively filter irrelevant backgrounds. \u63d0\u51fa\u7684\u8ba1\u6570\u7f51\u7edc\u53ef\u4ee5\u63d0\u53d6 \u7ed9\u5b9a\u793a\u4f8b\u76f8\u4f3c\u76ee\u6807\u7684 \u8bed\u4e49\u7279\u5f81 \u5e76\u4e14 \u8fc7\u6ee4\u6389 \u4e0d\u76f8\u5173\u7684\u80cc\u666f\u4fe1\u606f</p> <p>\u4f20\u7edf\u7684L2\u635f\u5931\u548c\u6cdb\u5316\u635f\u5931 \u5bf9\u4e8eCAC \u8ba1\u6570\u95ee\u9898 \u4e0d\u5408\u9002\uff1b\u56e0\u4e3a\u5bf9\u4e8e\u4e0d\u540c\u76ee\u6807\u7684\u5c3a\u5ea6\u53d8\u5316\u662f\u6bd4\u8f83\u5927\u7684</p> <p>Besides, we find that traditional L2 and generalized loss are not suitable for class-agnostic counting due to the variety of object scales in different samples. </p> <p>\u4e3a\u4e86\u89e3\u51b3 \u4f20\u7edf\u7684L2\u635f\u5931\u5bf9\u4e8e\u793a\u4f8b\u5c3a\u5ea6\u591a\u53d8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898</p> <p>Here we propose a scale-sensitive generalized loss to tackle this problem. </p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570 \u80fd\u505a\u4ec0\u4e48\uff1f</p> <ul> <li>\u6839\u636e\u7ed9\u5b9a\u793a\u4f8b\u7684\u5927\u5c0f \u8c03\u6574\u635f\u5931\u51fd\u6570\u7684\u5f62\u5f0f\u2192\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u7684\u5dee\u5f02\u66f4\u52a0\u660e\u663e</li> </ul> <p>It can adjust the cost function formulation according to the given exemplars, making the difference between prediction and ground truth more prominent. </p> <p>\u7ed3\u679c</p> <p>Extensive experiments show that our model obtains remarkable improvement and achieves state-of-the-art performance on a public class-agnostic counting benchmark. the source code is available at https://github.com/Elin24/SPDCN-CAC.</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_2","title":"\u603b\u7ed3\u6458\u8981","text":"<ol> <li>\u4e3a\u4e86 \u63d0\u53d6\u7279\u5f81\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa  \u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5377\u79ef\uff0c\u6574\u5408\u793a\u4f8b\u4fe1\u606f</li> <li>\u4e3a\u4e86\u89e3\u51b3 \u4f20\u7edf\u7684L2\u635f\u5931\u5bf9\u4e8e\u793a\u4f8b\u5c3a\u5ea6\u591a\u53d8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898</li> </ol>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_3","title":"\u5f15\u5165\u2014\u2014\u8d21\u732e","text":"<p>To summarize, the key contributions of this paper are:</p> <ul> <li>To address class-agnostic counting, we propose a scale-prior deformable network to better extract exemplar-related features, followed by a segmentation-then-counting stage to count objects. </li> </ul> <p>\u4e3a\u4e86\u89e3\u51b3CAC\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef \u66f4\u597d\u7684\u63d0\u53d6\u4e0e\u6837\u4f8b\u6846\u6709\u5173\u7684\u7279\u5f81\uff1b\u8ddf\u7740\u4e00\u4e2a\u5148\u5206\u5272\u540e\u8ba1\u6570\u7684\u9636\u6bb5\u6765\u6570\u76ee\u6807</p> <ul> <li>We propose a scale-sensitive generalized loss to make the model training adaptive to objects of different sizes, boosting the performance and generalization of trained models. </li> </ul> <p>\u63d0\u51fa\u4e86 \u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931\uff0c\u4f7f\u5f97\u6a21\u578b\u66f4\u597d\u7684\u9002\u7528 \u4e0d\u540c\u5c3a\u5bf8\u7684\u7269\u4f53\uff1b\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd</p> <ul> <li>\uff08\u7ed3\u679c\uff09Extensive experiments and visualizations demonstrate these two designs work well, and outstanding performance is obtained when our model is tested on benchmarks.\uff08\u6709\u5b9e\u9a8c\u3001\u6709\u53ef\u89c6\u5316\uff09</li> </ul>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_4","title":"\u603b\u7ed3","text":"<ul> <li>\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef \u63d0\u53d6\u6837\u4f8b\u6846\u7279\u5f81\uff1b</li> <li>\u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931</li> </ul>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_5","title":"\u7ed3\u8bba","text":"<p>\u7b2c\u4e00\u4e2a\u63d0\u51fa</p> <p>In this paper, we explore exemplar-guided class-agnostic counting. </p> <p>\u8fd9\u7bc7\u6587\u7ae0 \u96c0\u6c0f\u4e3b\u8981\u7814\u7a76\u7684\u6837\u4f8b\u6846\u6307\u5bfc\u7684CAC\u8ba1\u6570\u95ee\u9898</p> <p>\u5c3a\u5ea6\u4f18\u5148\u7684\u53ef\u53d8\u5377\u79ef\u662f\u4e3a\u4e86 \u6574\u5408\u6837\u4f8b\u6846\u4fe1\u606f\uff0c\u4f7f\u5f97\u63d0\u53d6\u5230\u7684\u7279\u5f81\u66f4\u5177\u6709\u7a33\u5065\u6027</p> <p>To take advantage of scale information provided by exemplars, scale-prior deformable convolution is proposed to adjust the receptive fields according to the given exemplars. </p> <p>\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528\u5c3a\u5ea6\u4fe1\u606f\uff0c\u5c3a\u5ea6\u4f18\u5148\u7684\u53ef\u53d8\u5377\u79ef \u88ab\u63d0\u51fa \u66f4\u597d\u7684\u9002\u5e94 \u7ed9\u5b9a\u793a\u4f8b\u7684\u611f\u53d7\u91ce</p> <p>\uff08\u7ed3\u679c\uff09Experimental results show that this operation decreases counting errors dramatically and gives a more accurate density distribution. </p> <p>\u7b2c\u4e8c\u4e2a\u63d0\u51fa</p> <p>We also propose scale-sensitive generalized loss to adapt the cost function according to exemplars, so that different training samples with different object scales have their own distance function for optimal transport.</p> <p>\u540c\u6837\u63d0\u51fa \u5c3a\u5ea6\u654f\u611f\u7684\u6cdb\u5316\u635f\u5931 \u9002\u5e94\u635f\u5931\u51fd\u6570\uff0c\u56e0\u4e3a\u793a\u4f8b\u662f\u5c3a\u5ea6\u53d8\u5316\u7684</p> <p>\u4e0d\u540c\u7684\u8bad\u7ec3\u6837\u672c\u6709\u4e0d\u540c\u7684\u5c3a\u5ea6\u53d8\u5316</p> <p>\uff08\u7ed3\u679c\uff09 This new loss further helps our model perform better than previous models on the class-agnostic counting benchmark.</p> <p>Note</p> <p>\u5199\u4f5c\u903b\u8f91\uff1a   (1)\u63d0\u51fa\u65b9\u6cd5   (2)\u8bf4\u660e\u4f60\u7ed3\u679c  </p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_6","title":"\u5f15\u5165","text":"<p>Introduction</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p1","title":"P1 \u4ece\u7279\u5b9a\u7c7b\u522b\u8bf4\u8d77","text":"<p>Info</p> <p>P1 </p> <p>\uff081\uff09\u4ece\u7279\u5b9a\u7c7b\u522b\u7684\u76ee\u6807\u8ba1\u6570\u5f00\u59cb\u8bf4\uff0c\u5e94\u8be5\u662f\u6587\u7ae0\u6bd4\u8f83\u65e9\uff0c\u6240\u4ee5\u5bf9\u4e8e\u4e3a\u4ec0\u4e48\u8981\u6c42CAC\u8ba1\u6570\u7684\u80cc\u666f\u548c\u52a8\u673a\u8bf4\u7684\u5f88\u5177\u4f53</p> <p>\uff082\uff09\u5f15\u5165\u90e8\u5206 \u90fd\u662f\u4ecesepcific\u8bf4\u8d77</p> <p>In recent years, remarkable progress has been achieved in counting tasks. However, most methods only work in a category-specific manner, like counting crowd [34] or vehicles [20], and thus they fail to meet the requirements of some real-world applications.  \u7279\u5b9a\u7c7b\u522b\u7684\u8ba1\u6570\uff1a\u4eba\u7fa4\u8ba1\u6570&amp;\u8f66\u8f86\u8ba1\u6570\uff1b\u4e0d\u80fd\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u7684\u9700\u8981\uff1b</p> <p>CAC\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f</p> <p>\uff081\uff09For example, there exist demands for counting goods in various categories in supermarkets or warehouses [7]; \u8d85\u5e02\u6216\u8005\u4ed3\u5e93\u4e2d \u5546\u54c1\u8ba1\u6570</p> <p>\uff082\uff09 in agriculture, predicting the crop yield of different fruits/vegetables is required [14, 37]; \u519c\u4e1a\u4e2d\uff0c\u4e0d\u540c\u679c\u852c\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b</p> <p>\uff083\uff09and some may want to know the number of different trees [3, 21]. \u4e0d\u540c\u6811\u6570\u91cf\u9884\u6d4b</p> <p>\u4f46\u662f\uff0c\u5f15\u51fa\u4e0b\u4e00\u6bb5   However, with traditional counting methods, a separate counting model is needed for each object class, which limits its practical applications.</p> <p>Info</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p2-cac","title":"P2  CAC\u95ee\u9898\u7684\u8bf4\u660e\uff0c\u4ece\u5b9a\u4e49\u3001\u635f\u5931\u5230\u6a21\u578b\u6982\u8ff0","text":"<pre><code>\u8fd9\u91cc\u7684\u6a21\u578b\u6982\u8ff0\u8ddf\u4ee5\u524d\u7684\u4e0d\u5927\u4e00\u6837\uff0c\u662f\u8bf4\u6a21\u578b\u65b9\u6cd5\u4ee5\u540e \u5c31\u4f1a\u6307\u51fa\u95ee\u9898\n</code></pre> <p>CAC\u95ee\u9898\u7684\u5b9a\u4e49</p> <p>To tackle the above problem, this paper considers class-agnostic counting, in which counting models predict the number and distribution of objects indicated by a few object exemplars in a set of query images. </p> <p>CAC\u7684\u635f\u5931\u662f\u5982\u4f55\u5b9a\u4e49\u7684</p> <p>During training, both images and exemplars are input to the counting model, and then the loss is calculated between the predicted density maps and human-annotated dot maps [23]. </p> <p>\u73b0\u5728\u7684CAC\u633a\u597d\u7684\uff0c\u4f46\u8fd8\u6709\u6539\u8fdb\u7684\u7a7a\u95f4</p> <p>Although existing class-agnostic counting methods have achieved good performance, there is still much room for improvement.</p> <p>CAC\u6587\u732e\u6982\u8ff0</p> <p>\u7b2c\u4e00\u4e2aCAC\u5f15\u7528\uff1aGMN</p> <p>For example, GMN [16] resizes the given exemplars to a fixed size and then calculates the distance between the exemplar\u2019s feature and local regions of the query image to localize the object of interest. One problem in this process is that exemplar features will lose the scale information provided by the exemplar\u2019s size.  \u95ee\u9898\u5728\u4e8e\uff1a\u635f\u5931\u4e86\u793a\u4f8b\u7684\u5c3a\u5bf8\u4fe1\u606f</p> <p>\u7b2c\u4e8c\u4e2aCAC\u5f15\u7528\uff1aBMNet [26] </p> <p>Although BMNet [26] adds a scale embedding to its network to tackle this problem, its function is not intuitive. BMNet\u89e3\u51b3\u901a\u8fc7 \u5c3a\u5ea6\u5d4c\u5165 \u89e3\u51b3GMN\u7684\u95ee\u9898\uff1b\u4f46\u662f\u5e76\u4e0d\u76f4\u89c2</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p3","title":"P3 \u5f15\u51fa\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef","text":"<p>\u4e0a\u9762\u63d0\u51fa\u95ee\u9898\uff0c\u63a5\u4e0b\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u5229\u7528\u5c3a\u5ea6\u4fe1\u606f\uff0c\u63d0\u51fa\u5c3a\u5ea6\u4f18\u5148\u53ef\u53d8\u5377\u79ef\uff0c\u63d0\u53d6\u7279\u5b9a\u5c3a\u5bf8\u7684\u76ee\u6807\u7279\u5f81</p> <p>To take advantage of scale information, we design a Scale-Prior Deformable Convolution Network (SPDCN) to extract features of objects with specific size. </p> <p>\u5177\u4f53\u600e\u4e48\u5b9e\u73b0\u7684</p> <p>SPDCN embeds the scale information into the deformable convolution, so that its receptive field is adjusted automatically and extracts features corresponding to the scale of the given exemplars. </p> <p>SPDCN\u628a\u5c3a\u5ea6\u4fe1\u606f\u5d4c\u5165\u5230\u53ef\u53d8\u5377\u79ef\u4e2d\uff0c\u8fd9\u6837\u611f\u53d7\u91ce\u5c31\u80fd\u81ea\u9002\u5e94\u7684\u8c03\u6574\uff1b\u5e76\u4e14\u63d0\u53d6\u7ed9\u5b9a\u793a\u4f8b\u5c3a\u5ea6\u7279\u5f81</p> <p>This design significantly boosts the counting performance because objects in the same category typically have similar scale in an image, whereas different object categories may have vastly different scales. </p> <p>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u9ad8\u8ba1\u6570\u6027\u80fd\uff0c\u56e0\u4e3a\u76f8\u540c\u7c7b\u522b\u7684\u76ee\u6807\u5728\u540c\u4e00\u5f20\u56fe\u7247\u4e0a\u7684\u5c3a\u5bf8\u662f\u76f8\u540c\u7684\uff1b\u7136\u800c\u4e0d\u540c\u7c7b\u522b\u7684\u76ee\u6807\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\u662f\u5f88\u5927\u7684</p> <p>With the extracted features, SPDCN then computes the similarity between exemplars and query images to segment out regions containing the counted objects. After that, the generated similarity map and features are sent to a decoder to estimate the density map.</p> <p>\u7136\u540e\uff0c\u5229\u7528\u63d0\u53d6\u7684\u7279\u5f81\uff0cSPDCN\u8ba1\u7b97\u793a\u4f8b\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u5206\u5272\u51fa\u5305\u542b\u88ab\u7edf\u8ba1\u5bf9\u8c61\u7684\u533a\u57df\u3002\u4e4b\u540e\uff0c\u5c06\u751f\u6210\u7684\u76f8\u4f3c\u5ea6\u56fe\u548c\u7279\u5f81\u9001\u5165\u89e3\u7801\u5668\u6765\u4f30\u8ba1\u5bc6\u5ea6\u56fe\u3002 </p> <p>SPDCN \u4e3b\u8981\u5904\u7406\u7684\u5c31\u662f\u6837\u4f8b\u6846\u5c3a\u5bf8\u53d8\u5316\u6bd4\u8f83\u5927\u7684\u95ee\u9898\uff0c\u8fd9\u91cc\u7528\u7684\u8bcd\uff1a\u5206\u5272</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#p4-scale-sensitive-generalized-loss","title":"P4  \u6307\u51fa \u6211\u4eec\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570 scale-sensitive generalized loss","text":"<p>We apply the generalized loss [31] to train SPDCN. However, we find that the vanilla generalized loss is unsuitable for class-agnostic counting because its cost function assumes all objects (people) are the same size, whereas in class-agnostic counting, different object categories have different scales. To tackle this problem, we propose a scale-sensitive generalized loss, in which the cost function is adjusted adaptively based on the object scale. Experiments show that the performance is further improved with our adaptive loss function.</p> <p>P5 \u8d21\u732e \u60f3\u770b\u70b9\u5de6\u4fa7\u76ee\u5f55\u8df3\u8f6c</p> <p>Info</p> <p>\u5f15\u5165\u90e8\u5206\u7684\u5199\u4f5c\u903b\u8f91</p> <ol> <li>\u7279\u5b9a\u7c7b\u522b\u7684\u76ee\u6807\u8ba1\u6570   </li> <li>CAC\u8ba1\u6570   </li> <li>\u5f15\u51fa\u53ef\u53d8\u5377\u79ef    </li> <li>\u5f15\u51fa \u5c3a\u5ea6\u654f\u611f\u7684\u635f\u5931    </li> <li>\u8d21\u732e   </li> </ol>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_7","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>Info</p> <p>\u5206\u6210\u7684\u4e09\u90e8\u5206\uff1a</p> <p>Class-Agnostic Counting. \u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570</p> <p>Deformable Convolution. \u53ef\u53d8\u5377\u79ef</p> <p>Generalized Loss. \u6cdb\u5316\u635f\u5931</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#sepcific-cac","title":"\u7b2c\u4e00\u6bb5\uff1a\u4ecesepcific \u2192 CAC","text":"<p>Class-Agnostic Counting. </p> <p>\u7279\u5b9a\u7c7b\u522b\u8ba1\u6570</p> <p>Previous counting tasks mainly aim at counting objects in a specific category. </p> <p>The most popular task is crowd counting [17, 19, 29, 31, 32, 34, 35]. Vehicle [11, 20], cell [8, 9] and animal [24] counting also attract researchers\u2019 attention, and are applied in various aspects like vehicular management [33], medical research [4], wildlife conservation [1], and so on.</p> <p>However, only a few methods have considered class-agnostic object counting, and relevant datasets are rare. </p> <p>CAC\u8ba1\u6570</p> <p>FamNet</p> <p>FamNet [23] defines class-agnostic counting as predicting the number of given objects represented by only a few exemplars in the same image and constructs the first dataset called FSC-147 [23]. Its baseline model is designed based on self-similarities matching [25]. One problem is that the scale of exemplars is modeled by the kernel size, which is normally too large to compute, so FamNet freezes the parameters in the extractor to overcome this problem. </p> <p>GMN</p> <p>Another similar work is the generic matching network (GMN) [16], which encodes the semantic feature of exemplars to an embedding, and then uses a matching network to model the relation between the exemplar embedding and the image\u2019s feature maps. However, GMN does not consider the scale problem because the size of the embedding vector is fixed. </p> <p>BMNet</p> <p>BMNet [26] considers the scale problem and adds scale embedding into its model, but it is not intuitive. </p> <p>Our SPDCN</p> <p>Compared with these previous works, our proposed SPDCN embeds scale information into the deformable convolution so that the extracted feature can match the exemplar more accurately, yielding improved performance.</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_8","title":"\u7b2c\u4e8c\u6bb5 \u53ef\u53d8\u5377\u79ef\u7684\u76f8\u5173\u5de5\u4f5c","text":"<p>Deformable Convolution. </p> <p>\u56e0\u4e3a\u672c\u6587\u7528\u5230\u4e86\u53ef\u53d8\u5377\u79ef\uff0c\u6240\u4ee5\u4ecb\u7ecd\u4e86\u53ef\u53d8\u5377\u79ef\u7684\u76f8\u5173\u5de5\u4f5c</p> <p>Deformable convolution [2, 38] was proposed for modeling geometric transformations dynamically, and has been applied to video super-resolution [30], font generation [36] and other computer vision tasks. \u53ef\u53d8\u5377\u79ef\u7528\u6765\u5e72\u5565\u7684\uff1a\u52a8\u6001\u5efa\u6a21\u51e0\u4f55\u53d8\u6362\uff0c\u5df2\u7ecf\u5e94\u7528\u4e8e\u89c6\u9891\u8d85\u5206\u8fa8\u7387[ 30 ]\u3001\u5b57\u4f53\u751f\u6210[ 36 ]\u7b49\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u6ca1\u4e86\u89e3\u8fc7 \u53ef\u53d8\u5377\u79ef</p> <p>Compared to the previous works, we introduce scale-prior deformable convolution to class-agnostic counting, where the receptive fields of the counting network are adjusted according to the given exemplars.\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u4eec\u5c06\u5c3a\u5ea6\u5148\u9a8c\u53ef\u53d8\u5f62\u5377\u79ef\u5f15\u5165\u5230\u7c7b\u522b\u65e0\u5173\u8ba1\u6570\u4e2d\uff0c\u5176\u4e2d\u8ba1\u6570\u7f51\u7edc\u7684\u611f\u53d7\u91ce\u6839\u636e\u7ed9\u5b9a\u7684\u6837\u672c\u8fdb\u884c\u8c03\u6574\u3002</p>"},{"location":"literature/ObejectCounting/rank10%20SPDCN/#_9","title":"\u7b2c\u4e09\u6bb5 \u5173\u4e8e\u635f\u5931\u7684\u989d\u76f8\u5173\u5de5\u4f5c","text":"<p>Generalized Loss. </p> <p>\uff08\u4e4b\u524d\u4eba\u7684\u5de5\u4f5c\uff09</p> <p>The generalized loss [31] is designed based on the unbalanced optimal transport (UOT) problem. [31] prove that both L2 loss and Bayesian loss [18] are special cases of the generalized loss. </p> <p>\uff08\u4f5c\u8005\u7684\u5de5\u4f5c\uff09</p> <p>In contrast to [31], which uses a fixed cost function assuming all objects are similar sizes, we propose a scale-sensitive generalized loss for class-agnostic counting, where different object categories have different sizes. Experimental results show that class-agnostic counting models perform better with the scale-sensitive generalized loss, compared to the original version.</p> <ul> <li>\u6211\u4eec\u6ca1\u6709\u91c7\u7528\u56fa\u5b9a\u4e0d\u53d8\u7684\u635f\u5931\u51fd\u6570\u3001\u4e3a\u76f8\u4f3c\u5c3a\u5bf8\u7684\u6240\u6709\u76ee\u6807</li> <li>\u6211\u4eec\u63d0\u51fa\u5c3a\u5ea6\u654f\u611f\u7684 \u6cdb\u5316\u635f\u5931\uff0c\u4e0d\u540c\u7c7b\u7684\u76ee\u6807 \u6709 \u4e0d\u540c\u7684\u5c3a\u5bf8\uff0c\u6240\u4ee5\u635f\u5931\u51fd\u6570\u4e5f\u4e0d\u4e00\u6837</li> <li>\u7ed3\u679c</li> </ul>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/","title":"rank11 GCA SUN","text":"<p>2024\u5e74\u7684\u65b0\u6587\u7ae0\uff0c\u53ef\u4ee5\u770b\u770b</p> <p>\u6e90\u7801\u672a\u516c\u5f00\uff0c\u7b80\u5355\u770b\u770b</p> <p></p> <p>arxiv\u65e5\u671f\uff1a2024\u5e749\u670818\u65e5</p> <p>\u4e00\u773c\u6807\u9898\uff1abuff\u53e0\u6ee1\u4e86\uff0cSwinTransformer &amp; Unet</p> <p>\u4f5c\u8005\uff1a</p> <p>18 Sep 2024 \u00b7 Yuzhe Wu, Yipeng Xu, Tianyu Xu, Jialu Zhang, Jianfeng Ren, Xudong Jiang </p> <p>\u5b81\u6ce2\u8bfa\u4e01\u6c49\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u9662</p> <p>\u5b81\u6ce2\u8bfa\u4e01\u6c49\u5927\u5b66\u5353\u8d8a\u7814\u7a76\u521b\u65b0\u4e2d\u5fc3</p> <p>\u65b0\u52a0\u5761\u5357\u6d0b\u7406\u5de5\u5927\u5b66\u7535\u6c14\u4e0e\u7535\u5b50\u5de5\u7a0b\u5b66\u9662</p> <ul> <li> \u6b63\u5f0f\u53d1\u8868\u65e5\u671f\uff1a</li> </ul> <p>GCA \u8ba9\u6211\u60f3\u8d77\u6765 \u7ea7\u8054\u6ce8\u610f\u529b</p> <ul> <li> \u4e3a\u4ec0\u4e482024\u5e74\u7684\u6587\u7ae0\uff0c\u6392\u540d\u8fd8\u8fd9\u4e48\u4f4e\uff1f</li> <li> \u671f\u520a\uff1f</li> </ul> <p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u672a\u516c\u5f00</p> <p>\u6807\u9898\uff1aGCA-SUN: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting </p> <p>\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5\u3001Swin-UNet\u67b6\u6784\u3001Exemplar-Free Counting\uff08\u5c31\u662f0-shot\u95ee\u9898\uff09</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_1","title":"\u6458\u8981","text":"<p>\uff08\u672c\u6587\u4e3b\u9898\uff1aExemplar-Free Counting\uff09Exemplar-Free Counting aims to count objects of interest without intensive annotations of objects or exemplars. </p> <p>\u672c\u6587\u7684\u7b2c\u4e00\u4e2a\u63d0\u51fa\uff1a\u4e3a\u4e86\u5b9e\u73b0 Exemplar-Free Counting\uff0c\u63d0\u51fa Gated Context-Aware Swin-UNet (GCA-SUN) </p> <p>To achieve this, we propose Gated Context-Aware Swin-UNet (GCA-SUN) to directly map an input image to the density map of countable objects. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5Swin - UNet ( GCA-SUN )\uff0c\u5c06\u8f93\u5165\u56fe\u50cf\u76f4\u63a5\u6620\u5c04\u4e3a\u53ef\u6570\u7269\u4f53\u7684\u5bc6\u5ea6\u56fe</p> <p>Swin-UNet \u7684\u529f\u80fd \u4e00\u53e5\u8bdd\u8bf4\u660e\uff1a\u76f4\u63a5\u5c06\u8f93\u5165\u56fe\u50cf\u6620\u5c04\u5230\u5bc6\u5ea6\u56fe</p> <p>\u5c55\u5f00\u8bf4\u8bf4</p> <p>Specifically, a Gated Context-Aware Modulation module is designed in the encoder to suppress irrelevant objects or background through a gate mechanism and exploit the attentive support of objects of interest through a self-similarity matrix.</p> <p>\u5728\u7f16\u7801\u5668\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5236\u6a21\u5757\uff0c\u901a\u8fc7\u95e8\u673a\u5236\u6765\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u6216\u80cc\u666f\uff0c\u5e76\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u6765\u5229\u7528\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u652f\u6301\u3002</p> <p>The gate strategy is also incorporated into the bottleneck network and the decoder to highlight the features most relevant to objects of interest.\u95e8\u7b56\u7565\u4e5f\u88ab\u7eb3\u5165\u5230\u74f6\u9888\u7f51\u7edc\u548c\u89e3\u7801\u5668\u4e2d\uff0c\u4ee5\u7a81\u51fa\u4e0e\u611f\u5174\u8da3\u5bf9\u8c61\u6700\u76f8\u5173\u7684\u7279\u5f81\u3002</p> <p>By explicitly exploiting the attentive support among countable objects and eliminating irrelevant features through the gate mechanisms, the proposed GCA-SUN focuses on and counts objects of interest without relying on predefined categories or exemplars.</p> <p>\u901a\u8fc7\u663e\u5f0f\u5730\u5229\u7528\u53ef\u6570\u5bf9\u8c61\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u652f\u6301\u548c\u901a\u8fc7\u95e8\u673a\u5236\u6d88\u9664\u65e0\u5173\u7279\u5f81\uff0cGCA - SUN\u5728\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\u6216\u793a\u4f8b\u7684\u60c5\u51b5\u4e0b\u5173\u6ce8\u548c\u8ba1\u6570\u611f\u5174\u8da3\u7684\u5bf9\u8c61\u3002</p> <p>\u7ed3\u679c\uff09</p> <p>Experimental results on the FSC-147 and CARPK datasets demonstrate that GCA-SUN outperforms state-of-the-art methods. </p> <p>\u6240\u7528\u6570\u636e\u96c6\uff1aFSC147\u3001CARPK</p> <p>Index Terms\u2014Object counting, Exemplar-free counting, Gate mechanism, Self-similarity matrix</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_2","title":"\u5f15\u5165\u2014\u8d21\u732e","text":"<p>Our contributions can be summarized as follows. </p> <ol> <li>The proposed GCA-SUN achieves exemplar-free counting through a UNet-like architecture that utilizes Swin transformer blocks for feature encoding and decoding, avoiding the sample bias of exemplar-based approaches [11].   \u6240\u63d0\u51fa\u7684GCA - SUN\u901a\u8fc7\u7c7bUNet\u7ed3\u6784\u5b9e\u73b0\u4e86\u65e0\u6837\u672c\u8ba1\u6570\uff0c\u8be5\u7ed3\u6784\u5229\u7528Swin\u53d8\u6362\u5757\u8fdb\u884c\u7279\u5f81\u7f16\u7801\u548c\u89e3\u7801\uff0c\u907f\u514d\u4e86\u6837\u672c\u504f\u5dee\uff08EFC\u8ba1\u6570\uff09</li> <li>The proposed GCAM exploits attentive support of repetitive objects through the self similarity matrix, to focus on countable objects. \u63d0\u51fa\u7684GCAM\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u5229\u7528\u5bf9\u91cd\u590d\u5bf9\u8c61\u7684\u7ec6\u5fc3\u652f\u6301\uff0c\u805a\u7126\u4e8e\u53ef\u6570\u5bf9\u8c61\u3002</li> <li>The gate mechanism is integrated into various modules, e.g., GCAM, GEFS and GAFU, which suppresses the features of irrelevant objects or background while highlighting the most relevant features to countable objects.  \u5c06\u95e8\u673a\u5236\u96c6\u6210\u5230GCAM\u3001GEFS\u548cGAFU\u7b49\u6a21\u5757\u4e2d\uff0c\u5728\u7a81\u51fa\u4e0e\u53ef\u6570\u5bf9\u8c61\u6700\u76f8\u5173\u7684\u7279\u5f81\u7684\u540c\u65f6\uff0c\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u6216\u80cc\u666f\u7684\u7279\u5f81\u3002</li> <li> <p>The proposed GCA-SUN is evaluated on the FSC-147 and CARPK datasets. It outperforms state-of-the-art methods for exemplar-free counting.  \u5728FSC - 147\u548cCARPK\u6570\u636e\u96c6\u4e0a\u5bf9\u63d0\u51fa\u7684GCA - SUN\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u65e0\u6837\u672c\u8ba1\u6570\u65b9\u9762\uff0c\u5b83\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002</p> </li> <li> <p>\u672c\u6587\u63d0\u51fa\u7684\u7f51\u7edc\u7ed3\u6784\uff1a GCA-SUN\u3001\u4f7f\u7528\u4e86SwinTransformer</p> </li> <li>\u6a21\u5757\uff1a GCAM</li> <li>\u95e8\u673a\u5236\uff1aGCAM, GEFS and GAFU</li> <li>\u6570\u636e\u96c6\uff1a FSC-147 and CARPK datasets</li> </ol>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_3","title":"\u7ed3\u8bba","text":"<p>\u4e00\u3001GCA-SUN</p> <p>The proposed GCA-SUN effectively tackles the problems of exemplar-free counting by using a Swin-UNet architecture to directly map the input image to the density map of countable objects. </p> <p>GCA - SUN\u901a\u8fc7\u4f7f\u7528Swin - UNet\u67b6\u6784\u5c06\u8f93\u5165\u56fe\u50cf\u76f4\u63a5\u6620\u5c04\u5230\u53ef\u6570\u7269\u4f53\u7684\u5bc6\u5ea6\u56fe\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u65e0\u6837\u672c\u8ba1\u6570\u95ee\u9898\u3002</p> <p>\u4e8c\u3001GCAM </p> <p>The proposed GCAM exploits the attention information among the tokens of repetitive objects through the self-similarity matrix, and suppresses the features of irrelevant objects through a gate mechanism.</p> <p>\u6240\u63d0\u51fa\u7684GCAM\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u6316\u6398\u91cd\u590d\u5bf9\u8c61\u6807\u8bb0\u95f4\u7684\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u95e8\u673a\u5236\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u7684\u7279\u5f81\u3002</p> <p>\u4e09\u3001The gate mechanism &amp;  GEFS module  &amp; GAFU module</p> <p>The gate mechanism is also incorporated into the GEFS module and the GAFU module, which highlight the features most relevant to countable objects while suppressing irrelevant ones. </p> <p>\u95e8\u673a\u5236\u4e5f\u88ab\u7eb3\u5165\u5230GEFS\u6a21\u5757\u548cGAFU\u6a21\u5757\u4e2d\uff0c\u7a81\u51fa\u4e0e\u53ef\u6570\u5bf9\u8c61\u6700\u76f8\u5173\u7684\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u4e0d\u76f8\u5173\u7684\u7279\u5f81\u3002</p> <p>\u56db\u3001\u7ed3\u679c</p> <p>Our experiments on the FSC-147 and CARPK datasets demonstrate that GCASUN outperforms state-of-the-art methods, achieving superior performance in both intra-domain and cross-domain scenarios.</p> <p>\u5728FSC - 147\u548cCARPK\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGCASUN\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5728\u57df\u5185\u548c\u8de8\u57df\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_4","title":"\u5f15\u5165","text":""},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#exemplar-free","title":"\u7b2c\u4e00\u6bb5 \u76ee\u6807\u8ba1\u6570\u5206\u6210\u4e09\u7c7b\uff0c\u7279\u5b9a\u7c7b\u522b\u8ba1\u6570\u3001\u7c7b\u65e0\u5173\u8ba1\u6570\u3001exemplar-free\u8ba1\u6570","text":"<p>Object counting determines the number of instances of a specific object class in an image [1], e.g., vehicles [2], crowd [3], and cells [4]. It can be broadly categorized as:</p> <p>1) Class-Specific Counting (CSC), counting specific categories like fruits [5] and animals [6];  2) Class-Agnostic Counting (CAC), counting objects based on visual exemplars [1], [7], [8] or text prompts [9], [10];  3) Exemplar-Free Counting (EFC), counting objects without exemplars, presenting a significant challenge in discerning countable objects and determining their repetitions [8], [11], [12].</p> <p>Note</p> <p>CSC\u8ba1\u6570\u3001CAC\u8ba1\u6570\uff1b\u7279\u5b9a\u7c7b\u522b\u8ba1\u6570\u3001\u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570</p> <p>FSC\u8ba1\u6570\u3001ZSC\u8ba1\u6570\uff1b\u5c0f\u6837\u672c\u8ba1\u6570\u30010\u6837\u672c\u8ba1\u6570</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#exemplar-free-counting-efc","title":"\u7b2c\u4e8c\u6bb5   Exemplar-Free Counting (EFC)\u7684\u7814\u7a76\u73b0\u72b6","text":"<p>Exemplar-Free Counting shows promise for automated systems such as wildlife monitoring [13], healthcare [14], and anomaly detection [15]. </p> <p>Hobley and Prisacariu directly regressed the image-level features learned by attention modules into a density map [12]. </p> <p>CounTR [8] and LOCA [16] are originally designed for CAC tasks, but can be adapted to EFC tasks by using trainable components to simulate exemplars. </p> <p>RepRPN-Counter i==dentifies exemplars from region proposals by majority voting [11], and ==DAVE selects valuable objects using a strategy similar to majority voting based on [17].</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#gca-sunencoder-bottleneckdecoder","title":"\u7b2c\u4e09\u6bb5 GCA-SUN=encoder + bottleneck+decoder","text":"<p>\u73b0\u6709EFC\u8ba1\u6570\u5b58\u5728\u4e0d\u8db3 RepRPN-Counter </p> <p>Despite the advancements, existing models [8], [16], [17] often explicitly require exemplars to count similar objects.EFC methods such as RepRPN-Counter do not require exemplars but generate them through region proposal [11]. Either explicit or implicit exemplars may induce sample bias as exemplars can\u2019t cover the sample distribution.  \u7531\u4e8e\u6837\u4f8b\u65e0\u6cd5\u8986\u76d6\u6837\u672c\u5206\u5e03\uff0c\u4e0d\u8bba\u662f\u5916\u663e\u6837\u4f8b\u8fd8\u662f\u5185\u9690\u6837\u4f8b\u90fd\u53ef\u80fd\u5bfc\u81f4\u6837\u672c\u504f\u5dee\u3002</p> <p>\u95e8\u63a7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684 Gated Context-Aware Swin-UNet (GCA-SUN)\uff1b\u76f4\u63a5\u5c06\u8f93\u5165\u56fe\u7247\u6620\u5c04\u6210\u5bc6\u5ea6\u56fe\uff0c\u4e0d\u9700\u8981\u4efb\u4f55\u793a\u4f8b</p> <p>To address the challenge, we propose Gated Context-Aware Swin-UNet (GCA-SUN), which directly maps an input image to the density map of countable objects, without any exemplars. </p> <p>encoder\u5305\u542b\u4e24\u90e8\u5206\uff1a</p> <ul> <li>SwinTransformer \u63d0\u53d6\u7279\u5f81</li> <li>\u95e8\u63a7\u611f\u77e5\u6a21\u5757 </li> </ul> <p>Specifically, the encoder consists of a set of Swin Transformers to extract features, and Gated Context-Aware Modulation (GCAM) blocks to exploit the attentive supports of countable objects. </p> <p>bottleneck network  \u95e8\u63a7\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u589e\u5f3a encoder\u7279\u5f81 Gated Enhanced Feature Selector (GEFS)</p> <p>The bottleneck network includes a Gated Enhanced Feature Selector (GEFS) to emphasize the encoded features that are relevant to countable objects. </p> <p>decoder\uff1a</p> <ul> <li>SwinTransformer \u751f\u6210\u5bc6\u5ea6\u56fe\uff1a\u7ed3\u5408Gated Adaptive Fusion Units (GAFUs) \u95e8\u63a7\u9002\u5e94\u878d\u5408\u5355\u5143\uff0c\u6309\u7167\u4e0e\u76ee\u6807\u7684\u76f8\u5173\u5ea6\u8fdb\u884c\u52a0\u6743</li> <li>\u6700\u540e \u56de\u5f52\u5934 \u88ab\u4f7f\u7528\uff0c\u4ece\u52a0\u6743\u7279\u5f81\u4e2d\u4ea7\u751f\u5bc6\u5ea6\u56fe</li> </ul> <p>The decoder includes a set of Swin transformers for generating the density map, with the help of Gated Adaptive Fusion Units (GAFUs) to selectively weigh features based on their relevance to countable objects. Finally, a regression head is utilized to derive the density map from the aggregated features.</p> <p>\u603b\u7ed3\u8fd9\u6bb5</p> <ol> <li>Gated Context-Aware Swin-UNet (GCA-SUN)</li> <li>encoder= Swin Transformers +  Gated Context-Aware Modulation (GCAM) blocks </li> <li>bottleneck = Gated Enhanced Feature Selector (GEFS)</li> <li>decoder = Swin transformers + Gated Adaptive Fusion Units (GAFUs)</li> <li>regression head</li> </ol> <p>\u7528\u4e86\u5f88\u591a\u95e8\u63a7\u673a\u5236</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#gcam","title":"\u7b2c\u56db\u6bb5 GCAM","text":"<p>One key challenge in EFC is to effectively differentiate countable objects from other objects.  </p> <p>EFC\u7684\u4e00\u4e2a\u6311\u6218\u662f \u5982\u4f55\u6709\u6548\u7684\u4ece\u5176\u4ed6\u76ee\u6807\u4e2d \u533a\u5206\u51fa \u8ba1\u6570\u7269\u4f53\uff1b</p> <p>EFC\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u6709\u6548\u5730\u533a\u5206\u53ef\u6570\u5bf9\u8c61\u4e0e\u5176\u4ed6\u5bf9\u8c61</p> <p>The GCAM blocks tackle the challenge by first evaluating feature qualities by computing the feature score for each token, and then prioritizing those with informative content.</p> <p>GCAM\u6a21\u5757\u9996\u5148\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2atoken\u7684\u7279\u5f81\u5f97\u5206\u6765\u8bc4\u4f30\u7279\u5f81\u8d28\u91cf\uff0c\u7136\u540e\u4f18\u5148\u8003\u8651\u90a3\u4e9b\u5177\u6709\u4fe1\u606f\u542b\u91cf\u7684\u7279\u5f81\u3002</p> <p>GCAM\u6a21\u5757\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898</p> <p>GACM\u6a21\u5757\u7684\u529f\u80fd</p> <p>\u95ee\u9898\uff1aeffectively differentiate countable objects from other objects  Solution\uff1aGCAM</p> <p>In addition, GCAM computes pairwise similarities between tokens through a self-similarity matrix, exploiting the support of repeating objects in the same scene.</p> <p>\u6b64\u5916\uff0cGCAM\u901a\u8fc7\u81ea\u76f8\u4f3c\u77e9\u9635\u8ba1\u7b97token\u4e4b\u95f4\u7684\u6210\u5bf9\u76f8\u4f3c\u5ea6\uff0c\u5229\u7528\u540c\u4e00\u573a\u666f\u4e2d\u91cd\u590d\u5bf9\u8c61\u7684\u652f\u6301\u5ea6\u3002</p> <p>Lastly, a gate mechanism is incorporated to highlight the most relevant features while suppressing irrelevant ones.</p> <p>\u6700\u540e\uff0c\u5f15\u5165\u95e8\u673a\u5236\uff0c\u7a81\u51fa\u6700\u76f8\u5173\u7684\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u4e0d\u76f8\u5173\u7684\u7279\u5f81\u3002\u2018</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_5","title":"\u7b2c\u4e94\u6bb5","text":"<p>Another challenge is that foreground objects often share similar low-level features with background content. </p> <p>\u53e6\u4e00\u4e2a\u6311\u6218\u662f\uff0c\u524d\u666f\u5bf9\u8c61\u5f80\u5f80\u4e0e\u80cc\u666f\u5185\u5bb9\u5171\u4eab\u76f8\u4f3c\u7684\u4f4e\u7ea7\u7279\u5f81\u3002</p> <p>The skip connections directly fuse low-level features in the encoder with high-level semantics in the decoder, potentially impeding counting performance as the background information could disturb the foreground objects. </p> <p>\u8df3\u8dc3\u8fde\u63a5\u76f4\u63a5\u5c06\u7f16\u7801\u5668\u4e2d\u7684\u4f4e\u7ea7\u7279\u5f81\u4e0e\u89e3\u7801\u5668\u4e2d\u7684\u9ad8\u7ea7\u8bed\u4e49\u8fdb\u884c\u878d\u5408\uff0c\u7531\u4e8e\u80cc\u666f\u4fe1\u606f\u4f1a\u5bf9\u524d\u666f\u7269\u4f53\u4ea7\u751f\u5e72\u6270\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u8ba1\u6570\u6027\u80fd\u3002</p> <p>To tackle this issue, gate mechanisms are incorporated into both GEFS and GAFU to suppress irrelevant low-level features while preserving as much information on objects of interest as possible. </p> <p>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728GEFS\u548cGAFU\u4e2d\u90fd\u878d\u5165\u4e86\u95e8\u673a\u5236\uff0c\u4ee5\u6291\u5236\u4e0d\u76f8\u5173\u7684\u4f4e\u7ea7\u7279\u5f81\uff0c\u540c\u65f6\u5c3d\u53ef\u80fd\u591a\u5730\u4fdd\u7559\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u4fe1\u606f\u3002</p> <p>The former selectively enhances the compressed features at the bottleneck, and the latter filters the features in the decoder.</p> <p>\u524d\u8005\u9009\u62e9\u6027\u5730\u589e\u5f3a\u74f6\u9888\u5904\u7684\u538b\u7f29\u7279\u5f81\uff0c\u540e\u8005\u5728\u89e3\u7801\u5668\u4e2d\u8fc7\u6ee4\u7279\u5f81\u3002</p> <p>\u7b2c\u516d\u6bb5\uff1a\u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank11%20GCA_SUN/#_6","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>\u6ca1\u6709\u76f8\u5173\u5de5\u4f5c\uff0c\u672c\u6587\u7684\u76ee\u5f55\u7ed3\u6784\uff1a</p> <p>\u6807\u9898\uff1aGCA-SUN: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting</p> <p>Abstract</p> <p>I. INTRODUCTION</p> <p>II. PROPOSED METHOD</p> <p>A. Overview of Proposed Method</p> <p>B. Swin-T Encoder with GCAM</p> <p>C. Bottleneck with GEFS</p> <p>D. Swin-T Decoder with GAFU</p> <p>III. EXPERIMENTAL RESULTS</p> <p>A. Experimental Settings</p> <p>B. Comparison with State-of-the-Art Methods</p> <p>C. Cross-Domain Evaluation on CARPK Dataset</p> <p>D. Visualization of GCAM </p> <p>E. Ablation Study</p> <p>IV. CONCLUSION</p> <p>24\u00b711\u00b719\uff1a\u7b2c\u4e00\u6b21\u8bfb\uff0c\u56de\u987e</p> <p>CSC\u8ba1\u6570\uff0c\u51e0\u4e2a\u8ba1\u6570\u7684\u7b80\u5199 \u672c\u6587\u95e8\u63a7\u673a\u5236\u591a \u5176\u5b9e\uff0c\u6ca1\u7528U-Net\uff1fSwinTransformer\u6bd4\u8f83\u7c7b\u4f3cU-Net</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/","title":"rank12 SAFECount","text":"<p>today\uff1a241119 TUE</p> <p></p> <p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>axriv\u65e5\u671f\uff1a2022\u5e749\u670811\u65e5</p> <p>\u4f5c\u8005\uff1a\u6e05\u534e\u5927\u5b66\u3001\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66</p> <p>\u4f1a\u8bae\uff1a</p> <p>WACV </p> <p>WACV\uff08IEEE Winter Conference on Applications of Computer Vision\uff09 IEEE\u51ac\u5b63\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4f1a\u8bae  \u4f1a\u8bae\uff08\u91cd\u8981\u4f1a\u8bae\uff09</p> <p>22 Jan 2022 \u00b7 Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le </p> <p>\u4e09\u5e74\u524d\u7684\u6587\u7ae0</p> <p></p> <p>Title\uff1aFew-shot Object Counting with Similarity-Aware Feature Enhancement</p> <p>\u57fa\u4e8e\u76f8\u4f3c\u6027\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u7684\u5c11\u6837\u672c\u8ba1\u6570</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#_1","title":"\u6458\u8981","text":"<p>\uff08\u95ee\u9898\u5b9a\u4e49\uff09This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. </p> <p>\uff08\u6307\u51fa\u95ee\u9898\uff09The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one.</p> <p>\uff08\u89e3\u51b3\u95ee\u9898\uff09 To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. </p> <p>\u63d0\u51fa\u76f8\u4f3c\u6027\u6bd4\u8f83\u6a21\u5757\u548c\u7279\u5f81\u589e\u5f3a\u6a21\u5757 \u89e3\u51b3 \u76ee\u6807\u5806\u53e0\u7684\u73b0\u8c61</p> <p>\uff08\u5177\u4f53\u6765\u8bf4\uff09Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. </p> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u5e45\u652f\u6301\u56fe\u50cf\u548c\u4e00\u5e45\u67e5\u8be2\u56fe\u50cf\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u6bd4\u8f83\u5b83\u4eec\u5728\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u4e0a\u7684\u6295\u5f71\u7279\u5f81\u5f97\u5230\u4e00\u4e2a\u5f97\u5206\u56fe\u3002</p> <p>The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. </p> <p>\u6240\u6709\u652f\u6301\u56fe\u50cf\u7684\u5f97\u5206\u56fe\u88ab\u6536\u96c6\u5728\u4e00\u8d77\uff0c\u5e76\u5728\u6837\u672c\u7ef4\u5ea6\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ece\u800c\u4ea7\u751f\u53ef\u9760\u7684\u76f8\u4f3c\u5ea6\u56fe\u3002</p> <p>We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. </p> <p>\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u70b9\u76f8\u4f3c\u6027\u4f5c\u4e3a\u52a0\u6743\u7cfb\u6570\uff0c\u7528\u652f\u6301\u7279\u5f81\u6765\u589e\u5f3a\u67e5\u8be2\u7279\u5f81\u3002</p> <p>Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. </p> <p>\u8fd9\u6837\u7684\u8bbe\u8ba1\u9f13\u52b1\u6a21\u578b\u5bf9\u67e5\u8be2\u56fe\u50cf\u8fdb\u884c\u68c0\u67e5\uff0c\u66f4\u591a\u5730\u5173\u6ce8\u4e0e\u652f\u6301\u56fe\u50cf\u76f8\u4f3c\u7684\u533a\u57df\uff0c\u4f7f\u5f97\u4e0d\u540c\u5bf9\u8c61\u4e4b\u95f4\u7684\u8fb9\u754c\u66f4\u52a0\u6e05\u6670\u3002</p> <p>\uff08\u7ed3\u679c\uff09Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to 14.32 (35%\u2191). Code has been released here.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#intro-contribution","title":"intro-contribution","text":"<p>In this work, we propose a S imilarity-A ware F eature  E  nhancement block for object Counting (SAFECount). </p> <p>\u76f8\u4f3c\u6027\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u6a21\u5757</p> <p>As discussed above, feature is more informative while similarity better captures the support-query relationship. Our novel block adequately integrates both of the advantages by exploiting similarity as a guidance to enhance the features for regression. Intuitively, the enhanced feature not only carries the rich semantics extracted from the image, but also gets aware of which regions within the query image are similar to the exemplar object. Specifically, we come up with a similarity comparison module (SCM) and a feature enhancement module (FEM), as illustrated in Fig. 2c. On one hand, different from the naive feature comparison in Fig. 2b, our SCM learns a feature projection, then performs a comparison on the projected features to derive a score map. This design helps select from features the information that is most appropriate for object counting. After the comparison, we derive a reliable similarity map by collecting the score maps with respect to all support images (i.e., few-shot) and normalizing them along both the exemplar dimension and the spatial dimensions. On the other hand, the FEM takes the point-wise similarities as the weighting coefficients, and fuses the support features into the query feature. Such a fusion is able to make the enhanced query feature focus more on the regions akin to the exemplar object defined by support images, facilitating more precise counting.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#conclusion","title":"Conclusion","text":""},{"location":"literature/ObejectCounting/rank12%20SAFECount/#intro","title":"intro","text":""},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p1-specific-object-class-csc","title":"P1 specific object class  CSC\u8ba1\u6570","text":"<p>Object counting [3, 4], which aims at investigating how many times a certain object occurs in the query image, has received growing attention due to its practical usage [8, 13, 19, 51]. Most existing studies assume that the object to count at the test stage is covered by the training data [1, 10, 11, 19, 31, 50, 51]. As a result, each learned model can only handle a specific object class, greatly limiting its application.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p2-fsc","title":"P2  FSC\u8ba1\u6570 \u5b9a\u4e49","text":"<p>To alleviate the generalization problem, few-shot object counting (FSC) is recently introduced [24]. Instead of predefining a common object that is shared by all training images, FSC allows users to customize the object of their own interests with a few support images, as shown in Fig. 1. In this way, we can use a single model to unify the counting of various objects, and even adapt the model to novel classes (i.e., unseen in the training phase) without any retraining.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p3-fsc","title":"P3  FSC\u65b9\u6cd5\u6982\u8ff0","text":"<p>A popular solution to FSC is to first represent both the exemplar object (i.e. the support image) and the query image with expressive features, and then pinpoint the candidates via analyzing the feature correlation [20, 24, 46].  FSC\u7684\u4e00\u4e2a\u6d41\u884c\u7684\u89e3\u51b3\u65b9\u6848\u662f\u9996\u5148\u5c06\u793a\u4f8b\u5bf9\u8c61(\u5373\u652f\u6491\u56fe\u50cf)\u548c\u67e5\u8be2\u56fe\u50cf\u90fd\u8868\u793a\u4e3a\u5177\u6709\u8868\u8fbe\u6027\u7684\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u5206\u6790\u7279\u5f81\u76f8\u5173\u6027[ 20\u300124\u300146 ]\u6765\u786e\u5b9a\u5019\u9009\u5bf9\u8c61\u3002</p> <p>Active attempts roughly fall into two folds. One is feature based [20], as shown in Fig. 2a, where the pooled support feature is concatenated onto the query feature, followed by a regress head to recognize whether the two features are close enough. </p> <p>\u5c1d\u8bd5\u5927\u81f4\u5206\u4e3a\u4e24\u79cd\u3002\u4e00\u79cd\u662f\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5[ 20 ]\uff0c\u5982\u56fe2a\u6240\u793a\uff0c\u5176\u4e2d\u6c60\u5316\u7684\u652f\u6301\u7279\u5f81\u88ab\u8fde\u63a5\u5230\u67e5\u8be2\u7279\u5f81\u4e0a\uff0c\u7136\u540e\u662f\u4e00\u4e2a\u56de\u5f52\u5934\u6765\u8bc6\u522b\u4e24\u4e2a\u7279\u5f81\u662f\u5426\u8db3\u591f\u63a5\u8fd1\u3002</p> <p>\u7a7a\u95f4\u4fe1\u606f\u7531\u4e8e\u6c60\u5316\u6ca1\u6709\u4e86</p> <p>However, the spatial information of the support image is omitted by pooling, leaving the feature comparison unreliable. </p> <p></p> <p>The other is similarity-based [24,46], as shown in Fig. 2b, where a similarity map is developed from raw features as the regression object. Nevertheless, the similarity is far less informative than feature, making it hard to identify clear boundaries between objects (see Fig. 5). Accordingly, the counting performance heavily deteriorates when the target objects are densely packed in the query image, like the shoal of fish in Fig. 1.</p> <p>\u53e6\u4e00\u79cd\u662f\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684[ 24\u300146 ]\uff0c\u5982\u56fe2b\u6240\u793a\uff0c\u5176\u4e2d\u76f8\u4f3c\u6027\u56fe\u662f\u7531\u539f\u59cb\u7279\u5f81\u4f5c\u4e3a\u56de\u5f52\u5bf9\u8c61\u5f00\u53d1\u7684\u3002\u7136\u800c\uff0c\u76f8\u4f3c\u5ea6\u7684\u4fe1\u606f\u91cf\u8fdc\u4e0d\u5982\u7279\u5f81\uff0c\u8fd9\u4f7f\u5f97(\u89c1\u56fe5)\u7684\u5bf9\u8c61\u4e4b\u95f4\u5f88\u96be\u8bc6\u522b\u51fa\u6e05\u6670\u7684\u8fb9\u754c\u3002\u56e0\u6b64\uff0c\u5f53\u76ee\u6807\u7269\u4f53\u5728\u67e5\u8be2\u56fe\u50cf\u4e2d\u5bc6\u96c6\u6392\u5217\u65f6\uff0c\u8ba1\u6570\u6027\u80fd\u4e25\u91cd\u6076\u5316\uff0c\u5982\u56fe1\u4e2d\u7684\u9c7c\u7fa4\u3002</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#p4","title":"P4\u8d21\u732e","text":"<p>P5\uff1aFSC147\u6570\u636e\u96c6 &amp; CARPK\u6570\u636e\u96c6</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#related-work","title":"related work","text":"<p>\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206</p> <ul> <li>\u7279\u5b9a\u7269\u4f53\u8ba1\u6570</li> <li>FSC \u5c0f\u6837\u672c\u8ba1\u6570</li> <li>\u5c0f\u6837\u672c\u5b66\u4e60</li> </ul>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#csc","title":"\u7b2c\u4e00\u90e8\u5206\uff1aCSC \u8ba1\u6570","text":"<p>CAC\u8ba1\u6570 &amp; CSC\u8ba1\u6570 \u8ba1\u6570\u7269\u4f53</p> <p>FSC\u8ba1\u6570 &amp; ZSC\u8ba1\u6570  \u8ba1\u6570\u65b9\u6cd5 density-based\uff1bdetection-based</p> <p>\u63cf\u8ff0\u73b0\u72b6 Class-specific object counting counts objects of a specific class, such as people [19, 31, 50, 51], animals [1], cars [10], among which crowd counting has been widely explored. For this purpose, traditional methods [14, 33, 38] count the number of people occurring in an image through person detection. </p> <p>\u51fa\u73b0\u95ee\u9898 However, object detection is not particularly designed for the counting task and hence shows unsatisfying performance when the crowd is thick. </p> <p>\u89e3\u51b3\u95ee\u9898 To address this issue, recent work [37] employs a deep model to predict the density map from the crowd image, where the sum over the density map gives the counting result [15]. Based on this thought, many attempts have been made to handle more complicated cases [2, 18, 23, 27\u201329, 42, 44, 47, 48, 51]. Some recent studies [31, 36] propose effective loss functions that help predict the position of each person precisely. </p> <p>\u51fa\u73b0\u95ee\u9898 However, all of these methods can only count objects regarding a particular class (e.g., person), making them hard to generalize. </p> <p>\u89e3\u51b3\u95ee\u9898 There are also some approaches targeting counting objects of multiple classes [13, 21, 32, 43]. In particular, Stahl et al. [32] propose to divide the query image into regions and regress the counting results with the inclusion-exclusion principle. Laradji et al. [13] formulate counting as a segmentation problem for better localization. Michel et al. [21] detect target objects and regress multi-class density maps simultaneously. Xu et al. [43] mitigate the mutual interference across various classes by proposing categoryattention module. </p> <p>\u8fd8\u662f\u4e0d\u80fd\u6570\u6ca1\u5728\u8bad\u7ec3\u96c6\u89c1\u8fc7\u7684 Nevertheless, they still can not handle the object classes beyond the training data.</p>"},{"location":"literature/ObejectCounting/rank12%20SAFECount/#fsc","title":"\u7b2c\u4e8c\u90e8\u5206 FSC\u8ba1\u6570","text":"<p>Few-shot object counting (FSC) has recently been proposed [20, 24, 46] and presents a much stronger generalization ability. Instead of pre-knowing the type of object to count, FSC allows users to describe the exemplar object of their own interests with one or several support images. This setting makes the model highly flexible in that it does not require the test object to be covered by the training samples. </p> <p>In other words, a well-learned model could easily make inferences on novel classes (i.e., unseen in the training phase) as long as the support images are provided. To help the model dynamically get adapted to an arbitrary class, a great choice is to compare the object and the query image in feature space [20, 24, 46]. </p> <p>GMN [20] pools the support feature, and concatenates the pooling result onto the query feature, then learns a regression head for pointwise feature comparison. However, the comparison built on concatenation is not as reliable as the similarity [46]. </p> <p>Instead, CFOCNet [46] first performs feature comparison with dot production, and then regresses the density map from the similarity map derived before. </p> <p>FamNet [24] further improves the reliability of the similarity map through multi-scale augmentation and test-time adaptation. But similarities are far less informative than features, hence regressing from the similarity map fails to identify clear boundaries between the densely packed objects. In this work, we propose a similarity-aware feature enhancement block, which integrates the advantages of both features and similarities.</p>"},{"location":"literature/ObejectCounting/rank16%20Counting_DETR/","title":"rank16 Counting DETR","text":""},{"location":"literature/ObejectCounting/rank17%20RCC/","title":"rank17 RCC","text":""},{"location":"literature/ObejectCounting/rank18%20Omnicount/","title":"rank18 Omnicount","text":""},{"location":"literature/ObejectCounting/rank19%20FamNet/","title":"rank19 FamNet","text":""},{"location":"literature/ObejectCounting/rank2%20GeCo/","title":"rank2 GeCo","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>reading in 241117</p> <p>arxiv\u65e5\u671f\uff1a2024\u5e749\u670827\u65e5</p> <p>\u6807\u9898\uff1aA Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation</p> <p>\u4e0e\u5206\u5272\u6709\u4ec0\u4e48\u5173\u7cfb\uff1f</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#abstract","title":"Abstract","text":"<p>Low-shot object counters estimate the number of objects in an image using few or no annotated exemplars. \u95ee\u9898\u5b9a\u4e49</p> <p>Objects are localized by matching them to prototypes, which are constructed by unsupervised image-wide object appearance aggregation. </p> <p>Due to potentially diverse object appearances, the existing approaches often lead to over-generalization and false positive detections. </p> <p>**\u63d0\u51fa\u95ee\u9898\uff1a **   \u7531\u4e8e\u6f5c\u5728\u7684\u591a\u6837\u5316\u7684\u76ee\u6807\u5916\u89c2\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5f80\u5f80\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u6cdb\u5316\u548c\u8bef\u68c0\u3002</p> <p>Furthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each object center. </p> <p>\u6b64\u5916\uff0c\u6027\u80fd\u6700\u597d\u7684\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u66ff\u4ee3\u635f\u5931\u6765\u8bad\u7ec3\u76ee\u6807\u5b9a\u4f4d\uff0c\u5373\u5728\u6bcf\u4e2a\u76ee\u6807\u4e2d\u5fc3\u9884\u6d4b\u4e00\u4e2a\u5355\u4f4d\u9ad8\u65af\u3002</p> <p>This loss is sensitive to annotation error, hyperparameters and does not directly optimize the detection task, leading to suboptimal counts. </p> <p>\u8fd9\u79cd\u635f\u5931\u5bf9\u6807\u6ce8\u9519\u8bef\u3001\u8d85\u53c2\u6570\u654f\u611f\uff0c\u5e76\u4e14\u6ca1\u6709\u76f4\u63a5\u4f18\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u5bfc\u81f4\u6b21\u4f18\u8ba1\u6570\u3002</p> <p>We introduce GeCo, a novel low-shot counter that achieves accurate object detection, segmentation, and count estimation in a unified architecture. </p> <p>Note</p> <p>GeCo:\u5c0f\u6837\u672c\u8ba1\u6570\u5668\uff0c\u540c\u65f6\u662f\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u5272\u548c\u8ba1\u6570</p> <p>GeCo robustly generalizes the prototypes across objects appearances through a novel dense object query formulation. </p> <p>GeCo\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u7a20\u5bc6\u5bf9\u8c61\u67e5\u8be2\u516c\u5f0f\uff0c\u9c81\u68d2\u5730\u6982\u62ec\u4e86\u8de8\u8d8a\u5bf9\u8c61\u5916\u89c2\u7684\u539f\u578b\u3002</p> <p>In addition, a novel counting loss is proposed, that directly optimizes the detection task and avoids the issues of the standard surrogate loss. </p> <p>\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u6570\u635f\u5931\uff0c\u76f4\u63a5\u4f18\u5316\u4e86\u68c0\u6d4b\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u6807\u51c6\u66ff\u4ee3\u635f\u5931\u7684\u95ee\u9898\u3002</p> <p>\uff08\u8bf4\u7ed3\u679c\uff09GeCo surpasses the leading few-shot detection-based counters by \u223c25% in the total count MAE, achieves superior detection accuracy and sets a new solid state-of-the-art result across all low-shot counting setups. The code will be available on GitHub.</p> <p>GeCo\u5728\u603b\u8ba1\u6570MAE\u4e0a\u8d85\u8fc7\u4e86\u9886\u5148\u7684\u57fa\u4e8e\u5c0f\u6837\u672c\u68c0\u6d4b\u7684\u8ba1\u6570\u566825 %\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u6240\u6709\u7684\u5c0f\u6837\u672c\u8ba1\u6570\u8bbe\u7f6e\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u65b0\u7684\u56fa\u6001\u7ed3\u679c\u3002</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#introduction-contribution","title":"Introduction contribution","text":"<p>\uff08\u7b2c\u4e94\u6bb5\uff09 \u8d21\u732e</p> <p>We address the aforementioned challenges by proposing a new single-stage low-shot counter GeCo, which is implemented as an add-on network for SAM [12] backbone. </p> <ul> <li> <p>\u9488\u5bf9\u4e0a\u9762\u7684\u95ee\u9898\uff0c\u63d0\u51faGeCo </p> </li> <li> <p>\u4f18\u70b9\uff1a\u5355\u9636\u6bb5</p> </li> <li> <p>SAM\u662f\u5565\uff1f</p> </li> </ul> <p>A single architecture is thus trained for both few-shot and zero-shot setup, it enables counting by detection and provides segmentation masks for each of the detected objects. </p> <ul> <li>\u4f18\u70b9\uff1a\u5355\u9636\u6bb5\u68c0\u6d4b\u8ba1\u6570\u65b9\u6cd5\uff0c few-shot &amp; zero-shot \u90fd\u662f\u5355\u9636\u6bb5\u8ba1\u6570</li> <li>\u4e3a\u6bcf\u4e2a\u88ab\u68c0\u6d4b\u7684\u5bf9\u8c61\u63d0\u4f9b\u5206\u5272\u63a9\u7801</li> <li>\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5</li> </ul> <p>Our first contribution  is a dense object query formulation, which applies a non-parametric model for image-wide prototype generalization (hence GeCo) in the encoder, and decodes the queries into highly dense predictions.</p> <p>\u6211\u4eec\u7684\u7b2c\u4e00\u4e2a\u8d21\u732e\u662f\u4e00\u4e2a\u7a20\u5bc6\u5bf9\u8c61\u67e5\u8be2\u516c\u5f0f\uff0c\u5b83\u5728\u7f16\u7801\u5668\u4e2d\u5e94\u7528\u4e86\u4e00\u4e2a\u975e\u53c2\u6570\u6a21\u578b\u7528\u4e8e\u56fe\u50cf\u8303\u56f4\u7684\u539f\u578b\u6cdb\u5316(\u56e0\u6b64\u662fGeCo )\uff0c\u5e76\u5c06\u67e5\u8be2\u89e3\u7801\u4e3a\u9ad8\u5ea6\u7a20\u5bc6\u7684\u9884\u6d4b\u3002</p> <p>The formulation simultaneously enables reliable detection in densely-populated regions (Figure 1, column 3&amp;4) and prevents prototype over-generalization, leading to an improved detection precision at a high recall. </p> <p>\u8be5\u516c\u5f0f\u540c\u65f6\u5b9e\u73b0\u4e86\u5728\u4eba\u53e3\u5bc6\u96c6\u533a\u57df(\u56fe1\u7b2c3\u30014\u5217)\u7684\u53ef\u9760\u68c0\u6d4b\uff0c\u5e76\u9632\u6b62\u4e86\u539f\u578b\u7684\u8fc7\u5ea6\u6cdb\u5316\uff0c\u4ece\u800c\u5728\u9ad8\u53ec\u56de\u7387\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002</p> <p>Our second contribution  is a new loss function for dense detection training that avoids the ad-hoc surrogate loss with unit Gaussians, it directly optimizes the detection task, and leads to improved detection not biased towards blob-like regions (Figure 1, column 1&amp;2).</p> <p>\u6211\u4eec\u7684\u7b2c\u4e8c\u4e2a\u8d21\u732e\u662f\u4e00\u4e2a\u65b0\u7684\u7528\u4e8e\u5bc6\u96c6\u68c0\u6d4b\u8bad\u7ec3\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u907f\u514d\u4e86\u4f7f\u7528\u5355\u4f4d\u9ad8\u65af\u7684ad - hoc\u4ee3\u7406\u635f\u5931\uff0c\u5b83\u76f4\u63a5\u4f18\u5316\u4e86\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5bfc\u81f4\u6539\u8fdb\u7684\u68c0\u6d4b\u4e0d\u504f\u5411\u4e8e\u56e2\u5757\u72b6\u533a\u57df(\u56fe1\u7b2c1\u30012\u5217)\u3002</p> <p>\u8bf4\u660e\u4e24\u4e2a\u8d21\u732e</p> <ul> <li>\uff1f</li> <li>\u63d0\u51fa\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570</li> </ul> <p>\uff08\u7b2c\u516d\u6bb5\uff09  \u7ed3\u679c GeCo outperforms all detection-based counters on challenging benchmarks by 24% MAE and the density-based long-standing winner [4] by 27% MAE, while delivering superior detection accuracy. </p> <p>\u4f18\u4e8e\u6240\u6709 \u57fa\u4e8e\u68c0\u6d4b \u548c \u57fa\u4e8e\u5bc6\u5ea6\u7684 \u8ba1\u6570\u65b9\u6cd5</p> <p>The method shows substantial robustness to the number of exemplars. In one-shot scenario, GeCo outperforms the best detection method in 5% AP50, </p> <p>\u57281-shot\u573a\u666f\u7684\u68c0\u6d4b\u6027\u80fd</p> <p>45% MAE and by 14% in a zero-shot scenario. </p> <p>0-shot\u7684\u8ba1\u6570\u6027\u80fd</p> <p>GeCo is the first detection-based counter that outperforms density based counters in all measures by using the number of detections as the estimator, and thus sets a milestone in low-shot detection-based counting.</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#5-conclusion","title":"5 Conclusion","text":"<p>\u7b2c\u4e00\u6bb5 </p> <p>We proposed GeCo, a novel single-stage low-shot counter that integrates accurate detection, segmentation, and count prediction within a unified architecture, and covers all low-shot scenarios with a single trained model. </p> <p>GeCo</p> <ul> <li>\u5355\u9636\u6bb5\u8ba1\u6570\u65b9\u6cd5\u3001\u51c6\u786e\u7684\u68c0\u6d4b\u3001\u5206\u5272\u6027\u80fd</li> </ul> <p>GeCo features remarkables dense object query formulation, and prototype generalization across the image, rather than just into a few prototypes.</p> <p>\uff1fGe Co\u7684\u663e\u8457\u7279\u70b9\u662f\u5bf9\u8c61\u67e5\u8be2\u63cf\u8ff0\u5bc6\u96c6\uff0c\u539f\u578b\u6cdb\u5316\u904d\u5e03\u6574\u4e2a\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u51e0\u4e2a\u539f\u578b\u3002</p> <p>It employs a novel loss function specifically designed for detection tasks, avoiding the biases of traditional Gaussian-based losses. </p> <p>\uff1f\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u68c0\u6d4b\u4efb\u52a1\u8bbe\u8ba1\u7684\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u9ad8\u65af\u7684\u635f\u5931\u7684\u504f\u5dee\u3002</p> <p>The loss optimizes detection accuracy directly, leading to more precise detection and counting. </p> <p>\u8be5\u635f\u5931\u76f4\u63a5\u4f18\u5316\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5bfc\u81f4\u66f4\u7cbe\u786e\u7684\u68c0\u6d4b\u548c\u8ba1\u6570\u3002</p> <p>\u6307\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411</p> <p>The main limitation of the presented method is that it cannot process arbitrarily large images, due to memory constraints, since it, as all current methods, operates globally. In future work, we will explore local counting and incremental image-wide count aggregation.</p> <p>\u672c\u6587\u65b9\u6cd5\u7684\u4e3b\u8981\u5c40\u9650\u6027\u5728\u4e8e\uff0c\u7531\u4e8e\u5185\u5b58\u9650\u5236\uff0c\u65e0\u6cd5\u5904\u7406\u4efb\u610f\u5927\u7684\u56fe\u50cf\uff0c\u56e0\u4e3a\u5b83\u4e0e\u73b0\u6709\u7684\u6240\u6709\u65b9\u6cd5\u4e00\u6837\uff0c\u662f\u5168\u5c40\u64cd\u4f5c\u7684\u3002\u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u7d22\u5c40\u90e8\u8ba1\u6570\u548c\u589e\u91cf\u56fe\u50cf\u8303\u56f4\u5185\u7684\u8ba1\u6570\u805a\u5408\u3002</p> <p>\uff08\u7b2c\u4e8c\u6bb5\uff09  \uff08\u8bf4\u7ed3\u679c\u4e86\uff0c\u6ca1\u5565\u53ef\u770b\u7684\uff09 Extensive analysis showcases that GeCo surpasses the best detection-based counters by approximately 25% in total count MAE, achieving state-of-the-art performance in a few-shot counting setup and demonstrates superior detection capabilities. GeCo showcases remarkable robustness to the number of provided exemplars, and sets a new state-of-the-art in one-shot as well as zero-shot counting.</p> <p>\u5927\u91cf\u7684\u5206\u6790\u8868\u660e\uff0cGeCo\u5728\u603b\u8ba1\u6570MAE\u4e0a\u8d85\u8fc7\u4e86\u6700\u597d\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u7ea625 %\uff0c\u5728\u5c11\u91cf\u7684\u8ba1\u6570\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u793a\u51fa\u5353\u8d8a\u7684\u68c0\u6d4b\u80fd\u529b\u3002GeCo\u5bf9\u6240\u63d0\u4f9b\u7684\u6837\u672c\u6570\u91cf\u8868\u73b0\u51fa\u663e\u8457\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5355\u6837\u672c\u548c\u96f6\u6837\u672c\u8ba1\u6570\u4e2d\u8bbe\u7f6e\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002</p>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#1-introduction","title":"1 Introduction","text":"<p>\uff08\u7b2c\u4e00\u6bb5\uff09  Low-shot object counting considers estimating the number of objects of previously unobserved category in the image, given only a few annotated exemplars (few-shot) or without any supervision (zero-shot) [21]. The current state-of-the-art methods are predominantly based on density estimation [4; 14; 31; 25; 21; 30; 7; 30]. These methods predict a density map over the image and estimate the total count by summing the density.</p> <p>\u6700\u8fd1\u7684\u7814\u7a76\u90fd\u662f\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570</p> <p>\uff08\u7b2c\u4e8c\u6bb5\uff09   While being remarkably robust for global count estimation, density outputs lack explainability such as object location and size, which is crucial for many practical applications [32; 29]. This recently gave rise to detection-based low-shot counters [20; 19; 33], which predict the object bounding boxes and estimate the total count as the number of detections\uff08\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u662f\u5982\u4f55\u5177\u4f53\u5b9e\u73b0\u7684\uff1a\u9884\u6d4b\u8fb9\u754c\u6846\uff0c\u6570\u76d2\u5b50\u6570\uff09. Nevertheless, detection-based counting falls behind the density-based methods in total count estimation, leaving a performance gap.</p> <p>\uff08\u70b9\u660e\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff09\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u51c6\u786e\u6027\u6bd4\u8f83\u9ad8\uff0c\u4f46\u662f\u4e0d\u80fd\u7ed9\u51fa\u76ee\u6807\u7684\u5b9a\u4f4d\u548c\u5c3a\u5bf8</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u80fd\u8be6\u7ec6\u7ed9\u51fa\u76ee\u6807\u7684\u4fe1\u606f\uff0c\u4f46\u662f\u8ba1\u6570\u51c6\u786e\u6027\u4e0d\u9ad8</p> <p>\uff08\u7b2c\u4e09\u6bb5\uff09   In detection-based counters, a dominant approach to identify locations of the objects in the image involves construction of object prototypes from few (e.g., three) annotated exemplar bounding boxes and correlating them with image features [20; 33; 19].</p> <p>\u5728\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u4e2d\uff0c\u8bc6\u522b\u56fe\u50cf\u4e2d\u7269\u4f53\u4f4d\u7f6e\u7684\u4e3b\u8981\u65b9\u6cd5\u662f\u901a\u8fc7\u5c11\u91cf\u7684(\u4f8b\u5982,\u4e09\u4e2a)\u5e26\u6ce8\u91ca\u7684\u6837\u672c\u8fb9\u754c\u6846\u6784\u5efa\u7269\u4f53\u539f\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u56fe\u50cf\u7279\u5f81[ 20 ; 33 ; 3 . 19 ]\u76f8\u5173\u8054\u3002</p> <p>The exemplar construction process is trained to account for potentially large diversity of object appearances in the image, often leading to overgeneralization, which achieves a high recall, but is also prone to false positive detection. </p> <p>\u6837\u4f8b\u6784\u5efa\u8fc7\u7a0b\u88ab\u8bad\u7ec3\u7528\u4e8e\u89e3\u91ca\u56fe\u50cf\u4e2d\u6f5c\u5728\u7684\u5de8\u5927\u7684\u7269\u4f53\u5916\u89c2\u591a\u6837\u6027\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u6cdb\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u53ec\u56de\u7387\uff0c\u4f46\u4e5f\u5bb9\u6613\u4ea7\u751f\u8bef\u68c0\u3002</p> <p>Note</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff1a\u53ec\u56de\u7387\u592a\u9ad8\u4e86</p> <p>Post-hoc detection verification methods have been considered [20; 33] to address the issue, but their multi-stage formulation prevents exploiting the benefits of end-to-end training.</p> <p>\u4e8b\u540e\u68c0\u6d4b\u9a8c\u8bc1\u65b9\u6cd5\u4e00\u76f4\u88ab\u8ba4\u4e3a\u662f[ 20 ; 33]\u6765\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u7684\u591a\u9636\u6bb5\u5236\u5b9a\u963b\u6b62\u4e86\u5229\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u597d\u5904\u3002</p> <p>me\uff1a\u5148\u68c0\u6d4b\u518d\u9a8c\u8bc1\u633a\u597d\u7684\uff0c\u4f46\u662f \u4e0d\u662f\u5355\u4e00\u9636\u6bb5\u7684\uff0c\u4ed6\u8bf4\u7684\u662fDAVE\uff0c\u4e00\u4e2a\u4f5c\u8005\u7684\u5de5\u4f5c</p> <p></p> <p>\uff08\u7b2c\u56db\u6bb5\uff09    Currently, the best detection counters [20; 33] predict object locations based on the local maxima in the correlation map. </p> <p>\u76ee\u524d\uff0c\u6700\u597d\u7684\u68c0\u6d4b\u8ba1\u6570\u5668[ 20 ; 33]\u662f\u6839\u636e\u76f8\u5173\u56fe\u4e2d\u7684\u5c40\u90e8\u6781\u5927\u503c\u6765\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\u3002</p> <p>During training, the map prediction is supervised by a unit Gaussian placed on each object center. </p> <p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u9884\u6d4b\u7531\u6bcf\u4e2a\u76ee\u6807\u4e2d\u5fc3\u7684\u5355\u4f4d\u9ad8\u65af\u5206\u5e03\u76d1\u7763</p> <p>However, the resulting surrogate loss is susceptible to the center annotation noise, requires nontrivial heuristic choice of the Gaussian kernel size and in practice leads to detection preference of compact blob-like structures (see Figure 1, column 1&amp;2). </p> <p>\u7136\u800c\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u66ff\u4ee3\u635f\u5931\u5bb9\u6613\u53d7\u5230\u4e2d\u5fc3\u6807\u6ce8\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u9700\u8981\u975e\u5e73\u51e1\u7684\u542f\u53d1\u5f0f\u9009\u62e9\u9ad8\u65af\u6838\u5927\u5c0f\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u4e2d\u5bfc\u81f4\u7d27\u51d1\u7684\u5757\u72b6\u7ed3\u6784(\u89c1\u56fe1 ,\u52171\u548c2)\u7684\u68c0\u6d4b\u504f\u597d\u3002</p> <p>Recently, DETR [1] inspired counter was proposed to avoid this issue [19], however, it fails in densely populated regions even though it applies a very large number of detection queries in a regular grid (see Figure 1, column 3&amp;4).</p> <p>\u6700\u8fd1\uff0cDETR [ 1 ]\u542f\u53d1\u7684\u8ba1\u6570\u5668\u88ab\u63d0\u51fa\u6765\u907f\u514d\u8fd9\u4e2a\u95ee\u9898[ 19 ]\uff0c\u7136\u800c\uff0c\u5c3d\u7ba1\u5b83\u5728\u89c4\u5219\u7f51\u683c(\u89c1\u56fe1 ,\u7b2c3\u30014\u5217)\u4e2d\u5e94\u7528\u4e86\u5927\u91cf\u7684\u68c0\u6d4b\u67e5\u8be2\uff0c\u4f46\u5b83\u5728\u4eba\u53e3\u5bc6\u96c6\u7684\u5730\u533a\u5931\u8d25\u4e86\u3002</p> <p></p> <p>Figure 1: DAVE [20] predicts object centers (red dots) biased towards blob-like structures, leading to incorrect partial detections of ants (bottom left), while GeCo(ours) addresses this with the new loss (top left). \u56fe1\uff1aDAVE [ 20 ]\u9884\u6d4b\u7684\u76ee\u6807\u4e2d\u5fc3(\u7ea2\u8272\u5706\u70b9)\u504f\u5411\u4e8eblob - like\u7ed3\u6784\uff0c\u5bfc\u81f4\u9519\u8bef\u7684\u90e8\u5206\u8682\u8681\u68c0\u6d4b(\u5de6\u4e0b)\uff0c\u800cGeCo (\u6211\u4eec\u7684)\u7528\u65b0\u7684\u635f\u5931(\u5de6\u4e0a)\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p> <p>CDETR [19] fails in densely populated regions (bottom right), while GeCo addresses this with the new dense query formulation by prototype generalization (top right). Exploiting the SAM backbone, GeCo delivers segmentations as well. Exemplars are denoted in blue.CDETR [ 19 ]\u5728\u4eba\u53e3\u7a20\u5bc6\u533a\u57df(\u53f3\u4e0b\u89d2)\u5931\u6548\uff0c\u800cGe Co\u901a\u8fc7\u539f\u578b\u6cdb\u5316(\u53f3\u4e0a\u89d2)\u7684\u65b0\u7684\u7a20\u5bc6\u67e5\u8be2\u5f62\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u5229\u7528SAM\u9aa8\u5e72\u7f51\uff0cGeCo\u4e5f\u63d0\u4f9b\u4e86\u5206\u6bb5\u3002\u56fe\u4f8b\u7528\u84dd\u8272\u8868\u793a\u3002</p> <p>\u8fd9\u4e00\u6bb5\u3001\u8fd9\u4e2a\u56fe\uff0c\u6211\u90fd\u770b\u4e0d\u61c2\uff1bGeCo\u6709\u4e00\u4e2a\u65b0\u635f\u5931\uff1bGeCo\uff0c\u5bf9\u6bd4\u4e86DAVE\uff08\u4ed6\u81ea\u5df1\u7684\u5de5\u4f5c\uff09\u3001CDETR\uff08\uff1f\uff09</p> <p>Summary</p> <p>\u95ee\u9898\u7684\u5f15\u5165\uff08\u80cc\u666f&amp;\u7814\u7a76\u610f\u4e49\uff09\uff1a</p> <ol> <li>Low-shot object counting</li> <li>\u57fa\u4e8e\u68c0\u6d4b &amp; \u57fa\u4e8e\u56de\u5f52\uff08\u8fd1\u6765\u4e3b\u6d41\uff0c\u4f46\u8ba1\u6570\u5c31\u4ec5\u4ec5\u662f\u8ba1\u6570\uff0c\u65e0\u6cd5\u7ed9\u51fa\u5173\u4e8e\u76ee\u6807\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\uff09</li> <li>\u57fa\u4e8e\u68c0\u6d4b</li> <li>\u57fa\u4e8e\u68c0\u6d4b</li> <li>\u672c\u6587\u8d21\u732e</li> <li>\u7ed3\u679c</li> </ol>"},{"location":"literature/ObejectCounting/rank2%20GeCo/#2-related-works","title":"2 Related works","text":"<p>\u7b2c\u4e00\u6bb5 </p> <p>Traditional counting methods focus on predefined categories like vehicles[3], cells [5], people[15], and polyps, [32] requiring extensive annotated training data and lacking generalization to other categories, necessitating retraining or conceptual changes. Low-shot counting methods address this limitation by estimating counts for arbitrary categories with minimal or no annotations, enabling test-time adaptation.\u4f20\u7edf\u7684\u8ba1\u6570\u65b9\u6cd5\u96c6\u4e2d\u4e8e\u9884\u5b9a\u4e49\u7684\u7c7b\u522b\uff0c\u5982\u8f66\u8f86[ 3 ]\uff0c\u7ec6\u80de[ 5 ]\uff0c\u4eba[ 15 ]\u548c\u606f\u8089[ 32 ]\uff0c\u9700\u8981\u5927\u91cf\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u6982\u5ff5\u66f4\u6539\u3002\u4f4e\u6837\u672c\u8ba1\u6570\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6700\u5c11\u6216\u6ca1\u6709\u6ce8\u91ca\u6765\u4f30\u8ba1\u4efb\u610f\u7c7b\u522b\u7684\u8ba1\u6570\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u95f4\u7684\u81ea\u9002\u5e94\u3002</p> <p>me\uff1a\u6307\u51fa\u4ece \u7279\u5b9a\u7269\u4f53 \u53d1\u5c55\u5230 \u901a\u7528\u7269\u4f53\u8ba1\u6570</p> <p>\u7b2c\u4e8c\u6bb5 </p> <p>With the proposal of the   FSC147   dataset [23] low-shot counting methods emerged, which predict global counts by summing over a predicted density maps. The first method [23] proposed an adaptation of a tracking backbone for density map regression. </p> <p>few-shot \u95ee\u9898\u7684\u7b2c\u4e00\u7bc7\u5de5\u4f5c\uff1aFSC147 \uff1b\u4e14\u662f\u57fa\u4e8e \u56de\u5f52\u7684</p> <p>BMNet+ [25] tackled learning representation and similarity metric, while SAFECount [31] introduced a new feature enhancement module, improving appearance generalization. CounTR [14] utilized a vision transformer for image feature extraction and a convolutional network for encoding the exemplar features. LOCA [4] argued that exemplar shape information should be considered along with the appearance, and proposed an iterative object prototype extraction module. This led to a simplified counter architecture that remains a top-performer among density-based counters.</p> <p>BMNet + [ 25 ]\u89e3\u51b3\u4e86\u5b66\u4e60\u8868\u793a\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u800cSAFECount [ 31 ]\u5f15\u5165\u4e86\u65b0\u7684\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u5916\u89c2\u6cdb\u5316\u6027\u3002Coun TR [ 14 ]\u4f7f\u7528\u89c6\u89c9\u8f6c\u6362\u5668\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u4f7f\u7528\u5377\u79ef\u7f51\u7edc\u5bf9\u6837\u672c\u7279\u5f81\u8fdb\u884c\u7f16\u7801\u3002LOCA [ 4 ]\u8ba4\u4e3a\u6837\u4f8b\u7684\u5f62\u72b6\u4fe1\u606f\u5e94\u8be5\u4e0e\u5916\u89c2\u4e00\u8d77\u8003\u8651\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fed\u4ee3\u7684\u5bf9\u8c61\u539f\u578b\u63d0\u53d6\u6a21\u5757\u3002\u8fd9\u5bfc\u81f4\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u8ba1\u6570\u5668\u67b6\u6784\uff0c\u5b83\u4ecd\u7136\u662f\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u5668\u4e2d\u7684\u4f7c\u4f7c\u8005\u3002</p> <p>Note</p> <p>5\u7bc7\u6587\u7ae0  1. \uff082021\u5e74\uff09FSC147\uff1a\u7b2c\u4e00\u7bc7  few-shot\u95ee\u9898\uff0c\u63d0\u51fa\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5 2. BMNet+ [25] 3. SAFECount [31] 4. CounTR [14]   ViT\u56fe\u7247\u7279\u5f81\u63d0\u53d6\uff1bCNN\u6837\u4f8b\u6846\u7279\u5f81 5. LOCA [4] \u5916\u89c2\u7279\u5f81\u3001\u5f62\u72b6\u7279\u5f81\u3001\u539f\u578b\u8fed\u4ee3\u6a21\u5757\uff1b\u57fa\u4e8e\u5bc6\u5ea6\uff08\u56de\u5f52\uff09\u7684\u8ba1\u6570\u65b9\u6cd5</p> <p>\u7b2c\u4e09\u6bb5</p> <p>To improve explainability of the estimated counts and estimate object locations as well, detectionbased methods emerged. </p> <p>\u4e3a\u4e86\u63d0\u9ad8\u4f30\u8ba1\u8ba1\u6570\u548c\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5\u5e94\u8fd0\u800c\u751f\u3002</p> <p>\uff08\u5148\u8bf4 \u6700\u65e9\u7684\uff09   The first few-shot detection-based counter [19] was an extended transformer-based object detector [2] with the ability to detect objects specified by the exemplars. </p> <p>\u6700\u65e9\u7684\u57fa\u4e8e\u5c0f\u6837\u672c\u68c0\u6d4b\u7684\u8ba1\u6570\u5668[ 19 ]\u662f\u4e00\u79cd\u6269\u5c55\u7684\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u76ee\u6807\u68c0\u6d4b\u5668[ 2 ]\uff0c\u5177\u6709\u68c0\u6d4b\u6837\u672c\u6307\u5b9a\u76ee\u6807\u7684\u80fd\u529b\u3002</p> <p>\uff08\u73b0\u5728\u6700\u597d\u7684\uff09   Current state-of-the-art DAVE [20] proposed a two-stage detect-and-verify paradigm for low-shot counting and detection, where in the first stage it generates object proposals with a high recall, but low precision, which is improved by a subsequent verification step. </p> <p>\u76ee\u524d\u6700\u5148\u8fdb\u7684DAVE [ 20 ]\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u68c0\u6d4b-\u9a8c\u8bc1\u8303\u5f0f\u7528\u4e8e\u4f4e\u955c\u5934\u8ba1\u6570\u548c\u68c0\u6d4b\uff0c\u5176\u4e2d\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7684\u7269\u4f53\u63d0\u8bae\u5177\u6709\u8f83\u9ad8\u7684\u53ec\u56de\u7387\uff0c\u4f46\u7cbe\u5ea6\u8f83\u4f4e\uff0c\u5e76\u901a\u8fc7\u540e\u7eed\u7684\u9a8c\u8bc1\u6b65\u9aa4\u8fdb\u884c\u6539\u8fdb\u3002</p> <p>PSECO [33] proposed a three-stage approach called point-segment-and-count, which employs more involved proposal generation with better detection accuracy and also applies a verification step to improve precision. </p> <p>PSECO [ 33 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u70b9-\u6bb5-\u8ba1\u6570\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e86\u66f4\u591a\u53c2\u4e0e\u7684\u5efa\u8bae\u751f\u6210\uff0c\u5177\u6709\u66f4\u597d\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u4e14\u8fd8\u5e94\u7528\u4e86\u9a8c\u8bc1\u6b65\u9aa4\u6765\u63d0\u9ad8\u7cbe\u5ea6\u3002</p> <p>Both DAVE and PSECO are multi-stage methods that train a network for the surrogate task of predicting density maps for object centers, from which the bounding boxes are predicted. </p> <p>Although detection-based counters offer additional applicability, they fall behind the best density-based counters in global count estimation.</p> <p>DAVE\u548cPSECO\u90fd\u662f\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u76ee\u6807\u4e2d\u5fc3\u7684\u5bc6\u5ea6\u56fe\u7684\u66ff\u4ee3\u4efb\u52a1\uff0c\u5e76\u4ece\u4e2d\u9884\u6d4b\u8fb9\u754c\u6846\u3002\u5c3d\u7ba1\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u9002\u7528\u6027\uff0c\u4f46\u5b83\u4eec\u843d\u540e\u4e8e\u5168\u5c40\u8ba1\u6570\u4f30\u8ba1\u4e2d\u6700\u597d\u7684\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u5668\u3002</p> <p>summary</p> <p>DAVE\u548cPSECO \u591a\u9636\u6bb5\u3001\u6bd4\u4e0d\u8fc7\u73b0\u5728\u6700\u597d\u7684\uff0c\u73b0\u5728\u6700\u597d\u7684\u662f \u57fa\u4e8e\u5bc6\u5ea6\u56de\u5f52\u56fe\u7684CountGD \u00b7 \u6587\u672c&amp;\u56fe\u50cf</p> <p>Note</p> <p>\u884c\u6587\u903b\u8f91</p> <p>\u7b2c\u4e00\u6bb5\uff1a\u4ece\u7279\u5b9a \\(\\rightarrow\\)  \u901a\u7528 \u76ee\u6807\u8ba1\u6570</p> <p>\u7b2c\u4e8c\u6bb5\uff1a\u901a\u7528\u76ee\u6807\u8ba1\u6570\uff1a\u57fa\u4e8e\u5bc6\u5ea6\u56de\u5f52\u56fe\u8ba1\u6570\u65b9\u6cd5\u7684\u53d1\u5c55</p> <p>\u7b2c\u4e09\u6bb5\uff1a\u901a\u7528\u76ee\u6807\u8ba1\u6570\uff1a\u57fa\u4e8e\u68c0\u6d4b\u8ba1\u6570\u65b9\u6cd5\u53d1\u5c55\uff0c\u53ef\u89e3\u91ca\u6027\u6bd4\u8f83\u597d\uff0c\u8ba1\u6570\u4e0d\u53ea\u662f\u4e3a\u4e86\u8ba1\u6570\uff0c\u4e5f\u8981\u6709\u76ee\u6807\u7684\u7279\u5b9a\u4fe1\u606f\uff1aLocation&amp;size</p> <p>241117</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/","title":"rank3 DAVE","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p></p> <p>arxiv\u65e5\u671f\uff1a2024\u5e744\u670825\u65e5</p> <p>\u8bba\u6587\u6b63\u5f0f\u53d1\u8868\u9875\u9762</p> <p>today\uff1a241117</p> <p>\u6807\u9898\uff1aDAVE \u2013 A Detect-and-Verify Paradigm for Low-Shot Counting </p> <ul> <li>\u4e24\u9636\u6bb5\u8ba1\u6570\u65b9\u6cd5\uff1a\u5148\u68c0\u6d4b\u518d\u9a8c\u8bc1</li> <li>\u68c0\u6d4b\uff1a\u9ad8\u53ec\u56de\uff0c\u590d\u7528\u4e86LOCA\u7684\u67b6\u6784</li> <li>\u9a8c\u8bc1\uff1a\u8c31\u805a\u7c7b\u9a8c\u8bc1</li> </ul> <p>\u671f\u520a\uff1aCVPR2024</p> <p>\u5f15\u7528\uff1a</p> <pre><code>@inproceedings{pelhan2024dave,\n  title={DAVE-A Detect-and-Verify Paradigm for Low-Shot Counting},\n  author={Pelhan, Jer and Zavrtanik, Vitjan and Kristan, Matej and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={23293--23302},\n  year={2024}\n}\n</code></pre> <p>LOCA  &amp; DAVE</p> <p></p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#abstract","title":"Abstract","text":"<p>Low-shot counters estimate the number of objects corresponding to a selected category, based on only few or no exemplars annotated in the image. The current state-ofthe-art estimates the total counts as the sum over the object location density map, but does not provide individual object locations and sizes\uff08\u63d0\u51fa\u95ee\u9898\uff09, which are crucial for many applications. This is addressed by detection-based counters, which, however fall behind in the total count accuracy \uff08\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\uff0c\u4f46\u662f\u51c6\u786e\u6027\u4e0d\u9ad8\uff09. Furthermore, both approaches tend to overestimate the counts in the presence of other object classes due to many false positives. \uff08\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5171\u6027\u95ee\u9898\uff1a\u5047\u9633\u6027\u8fc7\u9ad8\uff09</p> <p>\uff08\u672c\u6587\uff09We propose DAVE, a low-shot counter based on a detect-and-verify paradigm, that avoids the aforementioned issues by first generating a high-recall detection set and then verifying the detections to identify and remove the outliers. This jointly increases the recall and precision, leading to accurate counts. </p> <ul> <li>first generating a high-recall detection set and then </li> <li>verifying the detections to identify and remove the outliers. </li> </ul> <p>\uff08\u7ed3\u679c\uff09DAVE outperforms the top densitybased counters by \u223c20% in the total count MAE, it outperforms the most recent detection-based counter by \u223c20% in detection quality and sets a new state-of-the-art in zero-shot as well as text-prompt-based counting. The code and models are available on GitHub.</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#introduction-contribution","title":"Introduction-contribution","text":"<ul> <li> <p>We address the aforementioned issues by proposing a low-shot counter DAVE, which combines the benefits of density-based and detection-based formulations, and introduces a novel detect-and-verify paradigm. </p> </li> <li> <p>DAVE tackles the specificity-generalization issues of the existing counters by applying a two-stage pipeline (Figure 1). </p> </li> <li> <p>In the first, detection stage , DAVE leverages density-based estimation to obtain a high-recall set of candidate detections, which however may contain false positives. </p> </li> <li> <p>This is addressed by the second,  verification stage , where outliers are identified and rejected by analyzing the candidate appearances, thus increasing the detection precision. Regions corresponding to the outliers are then removed from the location density map estimated in the first stage, thus improving the densitybased total count estimates as well. </p> </li> <li> <p>In addition, we extend DAVE to text-prompt-based and to a zero-shot scenario, which makes DAVE the first zero-shot as well as textprompt detection-capable counter.</p> </li> </ul> <p>text prompt &amp; zero-shot</p> <p>The primary contribution of the paper is the detect-andverify paradigm for low-shot counting that simultaneously achieves high recall and precision. </p> <p>The proposed architecture is the first to extend to all low-shot counting scenarios. DAVE uniquely merges the benefits of both density and detection-based counting and is the first zero-shot-capable counter with detection output. </p> <p>\uff08\u7ed3\u679c\uff09</p> <ol> <li>DAVE outperforms all stateof-the-art density-based counters on the challenging benchmark [26], including the longstanding winner [6], achieving a relative 20% MAE and 43% RMSE total-count error reductions. </li> <li>It also outperforms all state-of-the-art detectionbased counters on the recent benchmark FSCD147 [22] by \u223c20% in detection metrics, as well as in the total count estimation by 38% MAE. </li> <li>Furthermore, it sets a new state-ofthe-art in text-prompt-based counting. </li> <li>The zero-shot DAVE variant outperforms all zero-shot density-based counters and delivers detection accuracy on-par with the most recent few-shot counters. </li> <li>DAVE thus simultaneously outperforms both density-based and detection-based counters in a range of counting setups.</li> </ol>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#5-conclusion","title":"5. Conclusion","text":"<p>\u7b2c\u4e00\u6bb5</p> <ol> <li>We presented a novel low-shot object counting and detection method DAVE, that narrows the performance gap between density-based and detection-based counters.</li> </ol> <p>DAVE\u662f\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ba1\u6570\u65b9\u6cd5</p> <p>2.DAVE spans the entire low-shot spectrum, also covering text-prompt setups, and is the first method capable of zero-shot detection-based counting. </p> <p>DAVE \u57fa\u4e8e\u6587\u672c &amp; 0-shot</p> <p>3.This is achieved by the novel detect-and-verify paradigm, which increases the recall as well as precision of the detections. \u68c0\u6d4b&amp;\u9a8c\u8bc1\uff0c\u51c6\u786e\u7387 \u53ec\u56de\u7387\u90fd\u5f88\u9ad8</p> <p>\u7b2c\u4e8c\u6bb5</p> <p>Extensive analysis demonstrates that DAVE sets a new state-of-the-art in total count estimation, as well as in detection accuracy on several benchmarks with comparable complexity to related methods, running 110ms/image.</p> <p>In particular, DAVE outperforms the long-standing top low-shot counter [6], as well as the recent detection-based counter [22]. </p> <p>In a zero-shot setup, DAVE outperforms all density-based counters and delivers detections on par with the most recent few-shot counter that requires at least few annotations. </p> <p>DAVE also sets a new state-of-the-art in prompt-based counting.</p> <p>In our future work, we plan to explore interactive counting with the human in the loop and improve detection in extremely dense regions.\u672a\u6765\u7684\u5de5\u4f5c\uff1a\u4eba\u7684\u4ea4\u4e92\u3001\u5bc6\u96c6\u573a\u666f\u7684\u68c0\u6d4b</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#1-introduction","title":"1. Introduction","text":"<p>P1 \u4eceLow-shot counting\u5f00\u59cb\u8bf4</p> <p>Low-shot counting considers estimating the number of target objects in an image, based only on a few annotated exemplars (few-shot) or even without providing the exemplars (zero-shot). Owing to the emergence of focused benchmarks [22, 26], there has been a surge in low-shot counting research recently. The current state-of-the-art low-shot counters are all density-based [6, 26, 28, 38]. This means that they estimate the total count by summing over an estimated object presence density map. Only recently, fewshot detection-based methods emerged [22] that estimate the counts as the number of detected objects.</p> <p>P2 \u6bd4\u8f83 Density-based &amp; detection-based \uff1b\u6307\u51fa\u76ee\u524d\u57fa\u4e8e\u5bc6\u5ea6\uff08Density-based \uff09\u7684\u8ba1\u6570\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002\u5e76\u8bf4\u660e\uff1aexplainability is crucial \u672c\u6587\u503e\u5411\u4e8e\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5</p> <p>Density-based methods substantially outperform the detection-based counters in total count estimation, but they do not provide detailed outputs such as object locations and sizes. The latter are however important in many downstream tasks such as bio-medical analysis [35, 41], where explainability is crucial for human expert verification as well as for subsequent analyses. There is thus a large applicability gap between the density-based and detection-based low-shot counters.</p> <p>P3  \u5171\u540c\u7684\u7f3a\u70b9 \u5047\u9633\u6027\u8fc7\u9ad8</p> <p>Furthermore, both density-based and detection-based counters are prone to failure in scenes with several object types (Figure 1). The reason lies in the specificity  generalization tradeoff. Obtaining a high recall requires generalizing over the potentially diverse appearances of the selected object type instances in the image. However, this also leads to false activations on objects of other categories (false positives), leading to a reduced precision and count overestimation. A possible solution is to train on multiple-class images [22], however, this typically leads to a reduced recall and underestimated counts.</p> <p>P4-P5 \u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#2-related-work","title":"2. Related Work","text":"<p>\u7b2c\u4e00\u6bb5</p> <p>Object counting emerged as detection-based counting of objects belonging to specific classes, such as vehicles [5], cells [8], people [17], and polyps [41]. To address poor performance in densely populated regions, density-based methods [3, 4, 29\u201331] emerged as an alternative.</p> <p>\u76ee\u6807\u8ba1\u6570\u3001\u5c31\u6709\u68c0\u6d4b\u7684\u65b9\u6cd5\u3001\u7279\u5b9a\u7c7b\u522b\uff1b\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u57fa\u4e8e\u5bc6\u5ea6\u56de\u5f52\u56fe\u7684\u8ba1\u6570\u65b9\u6cd5</p> <p>\u7b2c\u4e8c\u6bb5</p> <p>All these methods rely on the availability of large datasets to train category-specific models, which, however are not available in many applications.\u6570\u636e\u96c6</p> <p>\u7b2c\u4e09\u6bb5</p> <p>Class-agnostic approaches addressed this issue by test-time adaptation to various object categories with minimal supervision.</p> <p>\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5\u3001\u6700\u5c11\u7684\u76d1\u7763\u4fe1\u53f7\u3001\u6d4b\u8bd5\u9636\u6bb5\u8c03\u6574</p> <p>Early representatives [19] and [37] proposed predicting the density map by applying a siamese matching network \u5b6a\u751f\u5339\u914d\u7f51\u7edc to compare image and exemplar features. \u6bd4\u8f83\u56fe\u50cf&amp;\u6837\u4f8b\u6846\u7279\u5f81</p> <ul> <li>Recently, the FSC147 dataset [26] was proposed to encourage the development of few-shot counting methods. Famnet [26] proposed a test-time adaptation of the backbone to improve density map estimation. FSC147\u6570\u636e\u96c6\u3001 Famnet  || FSC147\u6570\u636e\u96c6[ 26 ]\u7684\u63d0\u51fa\u9f13\u52b1\u4e86\u5c0f\u6837\u672c\u8ba1\u6570\u65b9\u6cd5\u7684\u53d1\u5c55\u3002Famnet [ 26 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u9aa8\u5e72\u6d4b\u8bd5\u65f6\u95f4\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u5bc6\u5ea6\u56fe\u4f30\u8ba1\u3002</li> <li>BMNet+ [28] improved localization by jointly learning representation and a non-linear similarity metric. A self-attention mechanism was applied to reduce the intra-class appearance variability. BMNet + [ 28 ]\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8868\u793a\u548c\u975e\u7ebf\u6027\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u6539\u8fdb\u5b9a\u4f4d\u3002\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u51cf\u5c11\u7c7b\u5185\u5916\u89c2\u53d8\u5f02\u6027\u3002</li> <li>SAFECount [38] introduced a feature enhancement module, improving generalization capabilities. SAFECount [ 38 ]\u5f15\u5165\u4e86\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002</li> <li>CounTR [16] used a vision transformer [7] for image feature extraction and a convolutional encoder to extract exemplar features. An interaction module based on cross-attention was proposed to fuse both, image and exemplar features. Coun TR [ 16 ]\u4f7f\u7528\u89c6\u89c9\u8f6c\u6362\u5668[ 7 ]\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u4f7f\u7528\u5377\u79ef\u7f16\u7801\u5668\u63d0\u53d6\u6837\u672c\u7279\u5f81\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u4ea4\u4e92\u6a21\u5757\u6765\u878d\u5408\u56fe\u50cf\u7279\u5f81\u548c\u6837\u672c\u7279\u5f81\u3002</li> <li>LOCA [6] proposed an object prototype extraction module, which combined exemplar appearance and shape with an iterative adaptation.LOCA [ 6 ]\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9\u8c61\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5c06\u6837\u672c\u5916\u89c2\u548c\u5f62\u72b6\u4e0e\u8fed\u4ee3\u81ea\u9002\u5e94\u76f8\u7ed3\u5408\u3002</li> </ul> <p>Note</p> <p>Summary   5\u4e2a\u6a21\u578b   \u7c7b\u522b\u4e0d\u654f\u611f\u8ba1\u6570\u65b9\u6cd5\u3001\u6d4b\u8bd5\u9636\u6bb5\u81ea\u9002\u5e94\u3001\u5c11\u91cf\u76d1\u7763\u4fe1\u53f7\uff08\u4f60\u53bb\u770bGeCo\uff1a\u76f8\u5173\u5de5\u4f5c\uff0c\u4e5f\u662f\u8fd95\u4e2a\u6a21\u578b\u7684 \u7814\u7a76\u73b0\u72b6</p> <ul> <li>FamNet  </li> <li>BMNet+  </li> <li>SAFECount   </li> <li>CounTR  </li> <li>LOCA  </li> </ul> <p>\u7b2c\u56db\u6bb5</p> <p>All few-shot counting methods require few annotated exemplars to specify the object class. With the recent development of large language models (e.g. [23]) text-prompt based counting methods emerged.</p> <p>\u8f93\u5165\u4fe1\u53f7\u7684\u53d1\u5c55\uff1a\u4ece\u524d\u662f\u6807\u6ce8\u7684\u6837\u4f8b\u6846\u6307\u5b9a\u7c7b\u522b $\\rightarrow $  \u73b0\u5728\u57fa\u4e8e\u6587\u672c\u7684\u8ba1\u6570\u65b9\u6cd5\uff08\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff09</p> <p>Instead of specifying exemplars by bounding box annotations, these methods use text descriptions of the target object class. </p> <p>\u2764\ufe0f\u8fd9\u6bb5\u7684\u6587\u732e\u6982\u8ff0\uff1a\u5173\u4e8e\u7684\u662f\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u6307\u5b9a\u76ee\u6807\u7c7b\u522b </p> <p>ZeroCLIP [36] proposed text-based construction of prototypes, which are used to select relevant image patches acting as exemplars for counting.</p> <p>ZeroCLIP [ 36 ]\u63d0\u51fa\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u539f\u578b\u6784\u9020\uff0c\u7528\u4e8e\u9009\u62e9\u76f8\u5173\u7684\u56fe\u50cf\u5757\u4f5c\u4e3a\u8ba1\u6570\u7684\u8303\u4f8b\u3002</p> <p>CLIPCount [15] leveraged CLIP [23] for image-text alignment and introduced patch-text contrastive loss for learning the visual representations used for density prediction. </p> <p>CLIPCount [ 15 ]\u5229\u7528CLIP [ 23 ]\u8fdb\u884c\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u5757-\u6587\u672c\u5bf9\u6bd4\u635f\u5931\u6765\u5b66\u4e60\u7528\u4e8e\u5bc6\u5ea6\u9884\u6d4b\u7684\u89c6\u89c9\u8868\u793a\u3002</p> <p>Several works [13, 25] address the extreme case in which no exemplars are provided and the task is to count the majority class objects (i.e., zero-shot counting).</p> <p>\u4e00\u4e9b\u5de5\u4f5c[ 13\u300125]\u89e3\u51b3\u4e86\u6ca1\u6709\u63d0\u4f9b\u6837\u4f8b\u7684\u6781\u7aef\u60c5\u51b5\uff0c\u5176\u4efb\u52a1\u662f\u5bf9\u591a\u6570\u7c7b\u5bf9\u8c61\u8fdb\u884c\u8ba1\u6570(\u5373\u96f6\u6837\u672c\u8ba1\u6570)</p> <p>Note</p> <p>summary  1. ZeroCLIP  2. CLIPCount   </p> <p>Info</p> <p>DAVE  \u2460\u57fa\u4e8e\u68c0\u6d4b   \u2461text-prompt  \u2462zero-shot</p> <p>\u7b2c\u4e94\u6bb5</p> <p>With minimal architectural changes, the recent few-shot methods [6, 16] also demonstrated a remarkable zero-shot counting performance. A common drawback of densitybased counters is that they do not provide object locations.</p> <p>\u5728\u7ed3\u6784\u53d8\u5316\u5f88\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u8fd1\u7684\u5c11\u6837\u672c\u65b9\u6cd5[ 6\u300116 ]\u4e5f\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u96f6\u6837\u672c\u8ba1\u6570\u6027\u80fd\u3002\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8ba1\u6570\u5668\u7684\u4e00\u4e2a\u5171\u540c\u7f3a\u70b9\u662f\u5b83\u4eec\u4e0d\u63d0\u4f9b\u5bf9\u8c61\u4f4d\u7f6e\u3002</p> <p></p> <p></p> <p>[6]  LOCA  [16]CounTR</p> <p>\u5f00\u59cb\u5f15\u51fa\u53ef\u4ee5\u63d0\u4f9b\u5b9a\u4f4d\u7684\u8ba1\u6570 \u65b9\u6cd5</p> <p>\u7b2c\u516d\u6bb5</p> <p>To address the aforementioned limitation of density based counters, the first few shot counting and detection method [22] has been recently proposed by extending a transformer-based object detector [2] with an ability to detect objects specified by exemplars.</p> <p>\u6587\u732e22 \u7b2c\u4e00\u4e2a\u57fa\u4e8e\u68c0\u6d4b\u7684FSC\u65b9\u6cd5 </p> <p></p> <p>However, the detection based counter falls far behind in total count estimation compared with the best density-based counters.</p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u6700\u5927\u7684\u5f0a\u7aef\uff1a\u8ba1\u6570\u51c6\u786e\u6027\u4e0d\u9ad8(241117)</p>"},{"location":"literature/ObejectCounting/rank3%20DAVE/#3-counting-by-detection-and-verification","title":"3. Counting by detection and verification","text":"<p>todo</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/","title":"rank4 CACViT","text":"<p>Info</p> <p>\u672c\u6587\u6982\u62ec  1.\u57fa\u4e8eViT \u540c\u65f6\u8fdb\u884c\u63d0\u53d6\u548c\u5339\u914d\uff0cdecoupled view  \uff08\u89e3\u51b3\u4e86\u4ee5\u524d\uff1a\u5148\u63d0\u53d6\u540e\u5339\u914d\u7684\u6a21\u5f0f\uff09  2.\u7eb5\u6a2a\u6bd4\u611f\u77e5\u7684\u5c3a\u5ea6\u5d4c\u5165 \u548c \u6570\u91cf\u7ea7\u5d4c\u5165  3.\u6570\u636e\u96c6\uff1aFSC147 &amp; CARPK  </p> <p>\u5f15\u7528\uff1a</p> <pre><code>@inproceedings{wang2024vision,\n  title={Vision transformer off-the-shelf: A surprising baseline for few-shot class-agnostic counting},\n  author={Wang, Zhicheng and Xiao, Liwen and Cao, Zhiguo and Lu, Hao},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  year={2024}\n}\n</code></pre> <p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>arxiv\u65e5\u671f\uff1a2024\u5e743\u67084\u65e5</p> <p>today\uff1a 241117</p> <p>\u6b63\u5f0f\u53d1\u8868\u9875\u9762 \u63d0\u4f9b\u89c6\u9891\u8bb2\u89e3</p> <p></p> <p>\u4f5c\u8005\uff1a\u534e\u4e2d\u79d1\u6280\u5927\u5b66</p> <p></p> <p>\u6807\u9898\uff1aVision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting</p> <p>\u57fa\u4e8e\u73b0\u6709\u7684ViT\u67b6\u6784\uff1aFSC\u8ba1\u6570\u65b9\u6cd5</p> <p></p> <p>\u4f5c\u8005</p> <ul> <li>\u6559\u80b2\u90e8\u56fe\u50cf\u5904\u7406\u548c\u667a\u80fd\u63a7\u5236\u91cd\u70b9\u5b9e\u9a8c\u5ba4</li> <li>\u534e\u4e2d\u79d1\u6280\u5927\u5b66 \u4eba\u5de5\u667a\u80fd\u4e0e\u81ea\u52a8\u5316\u5b66\u9662</li> </ul>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#abstract","title":"Abstract","text":"<p>Class-agnostic counting (CAC) aims to count objects of interest from a query image given few exemplars. </p> <p>\u95ee\u9898\u5b9a\u4e49\uff1aFSC few shot counting</p> <p>CAC\uff1aclass-agnostic counting</p> <p>\u5408\u8d77\u6765\u81ea\u5df1\u9020\u4e2a\u5c31\u662f\uff1aFSCAC few-shot class-agnostic counting\u5c0f\u6837\u672c\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5\u7814\u7a76</p> <p>This task is typically addressed by extracting the features of query image and exemplars respectively and then matching their feature similarity, leading to an extract-then-match paradigm.</p> <p>\u56fe\u7247\u7279\u5f81  \u548c \u6837\u4f8b\u6846\u7279\u5f81  \u5206\u522b\u63d0\u53d6 \u7136\u540e\u8fdb\u884c\u5339\u914d</p> <p>\u6307\u51fa\u7814\u7a76\u73b0\u72b6\uff1a\u5148\u63d0\u53d6\u518d\u5339\u914d  extract-then-match paradigm</p> <p>In this work, we show that CAC can be simplified in an extract-and-match manner, particularly using a vision transformer (ViT) where feature extraction and similarity matching are executed simultaneously within the self-attention. </p> <p>ViT\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5339\u914d</p> <p>We reveal the rationale of such simplification from a decoupled view of the self-attention. The resulting model, termed CACViT, simplifies the CAC pipeline into a single pretrained plain ViT. </p> <p>Further, to compensate the loss of the scale and the order-of-magnitude information due to resizing and normalization in plain ViT, we present two effective strategies for scale and magnitude embedding.</p> <p>\u4e3a\u4e86\u5f25\u8865\u7f3a\u5931\u7684\u4fe1\u606f\uff1a\u5c3a\u5ea6\u5d4c\u5165 &amp; \u6570\u91cf\u7ea7\u5d4c\u5165</p> <p>Extensive experiments on the FSC147 and the CARPK datasets show that CACViT significantly outperforms state-of-the-art CAC approaches in both effectiveness (23.60% error reduction) and generalization, which suggests CACViT provides a concise and strong baseline for CAC. Code will be available.</p> <p>\u672c\u6587\u7684\u6570\u636e\u96c6\uff1a the FSC147 and the CARPK datasets</p> <p>Note</p> <p>\u672c\u6587\u7684\u521b\u65b0\u70b9\uff1a1+2</p> <p>1:CACViT:\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u51fa\u548c\u5339\u914d\uff0c\u5229\u7528ViT\u67b6\u6784\uff0cdecoupled view</p> <p>2:\u5c3a\u5ea6\u5d4c\u5165\u3001\u6570\u91cf\u7ea7\u5d4c\u5165</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#introduction-contribution","title":"Introduction contribution","text":"<p>In a nutshell, our contributions are three-fold: </p> <ul> <li>A novel extract-and-match paradigm: we show that simultaneous feature extraction and matching can be made possible in CAC; \u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5339\u914d</li> <li>CACViT: a simple and strong ViT-based baseline, sets the new state-of-the-art on the FSC-147 benchmark; \u57fa\u4e8eViT</li> <li>We introduce two effective strategies to embed scale, aspect ratio, and order of magnitude information tailored to CACViT. \u57fa\u4e8e\u7eb5\u6a2a\u6bd4\u611f\u77e5\u7684\u5c3a\u5ea6\u5d4c\u5165\u3001\u6570\u91cf\u7ea7\u5d4c\u5165</li> </ul>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#conclusions","title":"Conclusions","text":"<p>In this work, we propose a simple yet efficient ViT-based model CACViT for CAC. \u57fa\u4e8eViT\u67b6\u6784\uff0c\u89e3\u51b3CAC\u95ee\u9898\uff0c\u53d6\u540d\uff1aCACViT</p> <p>Specifically, we show that the ViT is naturally suitable for the CAC task from a decoupled view. \u89e3\u8026\u89c6\u89d2\u4e0b\uff0c\u89e3\u8bfbViT</p> <p>And we propose a ViT-based extract-and-match paradigm for CAC. \u57fa\u4e8eViT\u540c\u65f6\u8fdb\u884c\u63d0\u53d6&amp;\u5339\u914d</p> <p>Then we introduce aspect-ratio-aware scale embedding and magnitude embedding to compensate for the information loss. \u7eb5\u6a2a\u6bd4\u611f\u77e5\u7684\u5c3a\u5ea6\u5d4c\u5165\u548c\u6570\u91cf\u7ea7\u5d4c\u5165\uff0c\u5f25\u8865\u4e22\u5931\u7684\u4fe1\u606f</p> <p>Our CACViT achieves stat-of-the-art results on FSC147, and we also verify the generality on CARPK.</p> <p>\u6570\u636e\u96c6\uff1aFSC147&amp;CARPK</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#introduction","title":"Introduction","text":"<p>P1 \u76ee\u6807\u8ba1\u6570\uff0c\u6700\u5f00\u59cb\u9488\u5bf9\u7279\u5b9a\u9886\u57df</p> <p>Object counting aims to estimate the number of objects from a query image. Most prior object counting approaches target a specific domain, e.g., crowd (Zhang et al. 2015; Shu et al. 2022; Zou et al. 2021), plant (Lu et al. 2017; Madec et al. 2019), and car (Onoro-Rubio and L \u0301 opez-Sastre 2016). </p> <p>They often require numerous class-specific training data to learn a good model (Wang et al. 2020). </p> <p>\u8fc7\u6e21\u5230\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5</p> <p>In contrast, Class-Agnostic Counting (CAC), whose goal is to estimate the counting value of arbitrary categories given only few exemplars \u7ed9\u5b9a\u793a\u4f8b\u6846\u8ba1\u6570\u4efb\u610f\u7c7b\u522b, has recently received much attention due to its potential to generalize to unseen scenes and reduced reliance on class-specific training data (Lu, Xie, and Zisserman 2019; Ranjan et al. 2021; Shi et al. 2022; Liu et al. 2022).</p> <p>P2 \u8bf4\u7684\u662f  extract-then-match paradigm</p> <p>CAC is first introduced by Lu et al. (Lu, Xie, and Zisserman 2019), which is by default formulated as a template matching problem, leading to an extract-then-match paradigm. </p> <p>CAC\u4efb\u52a1\u6700\u65e9\u5f15\u5165\uff1a Lu et al. (Lu, Xie, and Zisserman 2019)\uff0c\u5b9a\u4e49\u4e3a\u6a21\u677f\u5339\u914d\u95ee\u9898\uff0c\u5148\u63d0\u53d6\u540e\u5339\u914d\u7684\u6a21\u5f0f</p> <p>Previous models (Ranjan et al. 2021; Shi et al. 2022; Lin et al. 2022) use shared CNN for query images and exemplars feature extraction, as the bottom-up feature extraction approach of the CNN can adapt to images of entirely different sizes. CNN\u81ea\u5e95\u5411\u4e0a\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u5f0f\u53ef\u4ee5\u9002\u5e94\u5927\u5c0f\u5b8c\u5168\u4e0d\u540c\u7684\u56fe\u50cf\u3002</p> <p>\uff08CounTR\uff09  Witnessing the ability of marking the responses on the attention map by cross-attention mechanism, some models such as CounTR (Liu et al. 2022) employs cross-attention to match the features of query image and exemplars.However, in CounTR the query feature and exemplar feature are embedded separately by a ViT and a CNN, and the matching part is achieved by an extra cross-attention stage. This strategy introduces much redundancy and task-specific designs, which is not in line with the trend of task-agnostic foundation models.</p> <p>\u4e00\u4e9b\u6a21\u578b\u5982CounTR ( Liu et al 2022)\u7b49\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u6ce8\u610f\u529b\u56fe\u4e0a\u6807\u8bb0\u54cd\u5e94\u7684\u80fd\u529b\u6765\u5339\u914d\u67e5\u8be2\u56fe\u50cf\u548c\u793a\u4f8b\u7684\u7279\u5f81\u3002\u7136\u800c\uff0c\u5728CounTR\u4e2d\uff0c\u67e5\u8be2\u7279\u5f81\u548c\u6837\u4f8b\u7279\u5f81\u5206\u522b\u7531\u4e00\u4e2aViT\u548c\u4e00\u4e2aCNN\u5d4c\u5165\uff0c\u5339\u914d\u90e8\u5206\u7531\u989d\u5916\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u9636\u6bb5\u5b9e\u73b0\u3002\u8fd9\u79cd\u7b56\u7565\u5f15\u5165\u4e86\u5927\u91cf\u7684\u5197\u4f59\u548c\u4efb\u52a1\u76f8\u5173\u7684\u8bbe\u8ba1\uff0c\u4e0d\u7b26\u5408\u4efb\u52a1\u65e0\u5173\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u8d8b\u52bf\u3002</p> <p>P3 ViT\u7684\u53d1\u5c55</p> <p>Recently, the computer vision community has witnessed great success with plain ViT in large multi-modal architectures (Touvron et al. 2023; Yu et al. 2022). Soon much work emerges for better adaptation of ViT on downstream vision tasks, such as object detection (Li et al. 2022; Lin et al. 2023), pose estimation (Xu et al. 2022, 2023) and image matting (Yao et al. 2023). As a template matching task, CAC is essentially suitable for using ViT with its attention mechanism; however, there is little focus on the adaptation of ViT on CAC task.</p> <p>P4 \u5f15\u51faViT\u548cCAC\u7684\u5173\u7cfb</p> <p>In this work, we share insights that the attention mechanism in plain ViT has the ability to extract the features for both the query image and the exemplars and perform feature matching for them. By grouping the query and exemplar tokens into concatenation and feeding them to a plain ViT, the self-attention process in ViT can be divide into two groups of self-attention, and two groups of cross-attention. The former self-attentions are to extract features for the query image and the exemplars, while the latter cross-attentions contains the matching process between the query image and the exemplars. Therefore, without multiple feature extractors or extra post-matching, it produces a novel extract-andmatch paradigm. Compared with prior arts, the extra attention from the query feature to the exemplars would further provide additional class information to the query image in this paradigm, enabling better perception of objects.Based on this idea, we propose a framework for CAC that mainly contains a single pretrained ViT, which verifies the feasibility of plain ViT for CAC task.</p> <p>P5</p> <p>For better adaptation of ViT to the specific CAC task, we introduce more insights closely related to CAC task in our model design. Specifically, we observe that certain restrictions or functions such as resizing and softmax normalization within this architecture can result in the loss of scale information and the order of magnitude of counting values. First, the exemplars must be resized to fit the ViT input, which introduces size ambiguity during matching. Prior CNN-based models (Shi et al. 2022) attempt to compensate for the scale information with scale embedding for exemplars; however, they neglect the information of aspect ratios, which is crucial for classes with abnormal ratios. This is largely overlooked in the existing literature. Second, the attention map with softmax function can represent the relative distribution of objects in the query image and therefore weakens the awareness of the model to the number of objects. We address this by restoring the magnitude order in the normalized attention map. Both the proposed scale embedding and magnitude embedding are easy to implement. By infusing the scale and the magnitude information into the plain ViT architecture, we acquire a surprisingly simple yet highly effective ViT baseline for CAC. The resulting model, termed CACViT, fully leverages the self-attention mechanism in ViT while also being tuned to mitigate the defects of this architecture in this task.</p> <p>P6</p> <p>Experiments on the public benchmark FSC147 (Ranjan et al. 2021) show that CACVit outperforms the previous best approaches by large margins, with relative error reductions of 19.04% and 23.60% on the validation and test sets, respectively, in terms of mean absolute error. Its cross-dataset generalization is also demonstrated on a car counting dataset CARPK (Hsieh, Lin, and Hsu 2017). We also provide extensive ablation studies to justify our propositions.</p> <p>P7 \u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#related-work","title":"Related Work","text":"<p>P1 </p> <p>The task of CAC is composed of two main components: feature extraction and feature matching. We first review each component in previous counting models, then discuss jointly feature extraction and matching in the other fields.</p> <p>Note</p> <p>Feature Extraction in Class-Agnostic Counting. CAC \u7684\u7279\u5f81\u63d0\u53d6 Feature Matching in Class-Agnostic Counting CAC\u4e2d\u7684\u7279\u5f81\u5339\u914d\u95ee\u9898 Jointly Feature Extraction and Matching. \u540c\u65f6\u8fdb\u884c\u63d0\u53d6 &amp; \u5339\u914d </p> <p>P2 CAC\u95ee\u9898\u4e2d\u7684\u7279\u5f81\u63d0\u53d6</p> <p>Feature Extraction in Class-Agnostic Counting.</p> <p>The investigation of feature extraction in counting first began with class-specific counting (Abousamra et al. 2021; Cao et al. 2018; He et al. 2021; Idrees et al. 2018; Laradji et al. 2018; Cheng et al. 2022). In class-specific counting, most works are designed to address the challenges posed by quantity variance and scale variance. (\u7279\u5b9a\u7c7b\u522b)</p> <p>Abousamra et al. 2021; </p> <p></p> <p>Cao et al. 2018;</p> <p></p> <p>He et al. 2021;</p> <p></p> <p>Idrees et al. 2018; </p> <p></p> <p>Laradji et al. 2018;</p> <p></p> <p>Cheng et al. 2022</p> <p></p> <p>\u90fd\u662f\u4e00\u4e9b \u6211\u6ca1\u4e86\u89e3\u8fc7\u7684\u53c2\u8003\u6587\u732e\uff0c\u7b11)</p> <p>For class-agnostic counting\uff08\u7c7b\u65e0\u5173\uff09, the core of feature extraction include unified matching space apart from challenges as above. To obtain a unified matching space, most previous work (Ranjan et al. 2021; Shi et al. 2022; You et al. 2023) uses the shared CNN-based feature extractors for query images and exemplars. CounTR (Liu et al. 2022), which first introduces the ViT for feature extraction in CAC, uses different feature extractors for the query images (a ViT) and exemplars (a CNN). Hence, a two-stage training scheme is used for unifying the feature space.</p> <p>Ranjan et al. 2021;  FamNet FSC147\u6570\u636e\u96c6\u3001CAC\u4efb\u52a1\u7684\u7b2c\u4e00\u7bc7\u561b\uff1fanyway\u7ecf\u5178\u6587\u732e\u4e86\u5c5e\u4e8e</p> <p></p> <p>Shi et al. 2022;   BMNet</p> <p></p> <p>You et al. 2023</p> <p></p> <p>CounTR (Liu et al. 2022) \u8001\u719f\u4eba\u60f9\u5012\u662f</p> <p></p> <p>P3 CAC\u95ee\u9898\u4e2d\u7684\u7279\u5f81\u5339\u914d</p> <p>Feature Matching in Class-Agnostic Counting.</p> <p>Compared with feature extraction, matching strategies in CAC have garnered more attention. \uff08\u7279\u5f81\u5339\u914d\u5b9e\u9645\u4e0a\u6709\u66f4\u591a\u7684\u5173\u6ce8\u5ea6\uff09</p> <p>The key points of the matching include the following two:  \u5339\u914d\u95ee\u9898\u4e2d\u4e3b\u8981\u5173\u6ce8\u7684\u4e24\u4e2a\u65b9\u9762</p> <p>1) robustness to appearance variance, and  \u5bf9\u4e8e\u5916\u89c2\u7684\u591a\u53d8 \u4f9d\u7136\u4fdd\u6301\u7a33\u5065\u6027 2) ability to characterize quantity levels.  \u5bf9\u4e8e\u6570\u91cf\u53d8\u5316\u7684\u7a33\u5065\u6027</p> <p>In the early attempt, naive inner product (Ranjan et al. 2021; Yang et al. 2021) is used, which is not robust to the appearance variance of objects to be counted. </p> <p>Ranjan et al. 2021  FamNet </p> <p></p> <p>Yang et al. 2021  </p> <p> </p> <p>Shi et al. (Shi et al. 2022) developed a bilinear matching network (BMNet) that expands the fixed inner product to a learnable bilinear similarity metric, which improves the robustness compared with the inner product. </p> <p>The recent ViT-based model CounTR (Liu et al. 2022) uses cross-attention for matching, which seems a natural choice for a transformer-based solution at first glance. However, we show that, in our plain ViT model CACViT, we can perform feature matching at the same time of extracting features by self-attention.</p> <p>CounTR</p> <p>CACViT</p> <p>P4</p> <p>Jointly Feature Extraction and Matching. </p> <p>For template matching and multi-modal tasks, feature extraction and matching are two main components. In tracking and detection tasks, MixFormer network (Chen et al. 2022) and FCT network (Han et al. 2022) were proposed to enhance the correlation between the target object and the image, thereby obtaining enhanced features for localization head.</p> <p>In multimodal tasks, ViLT (Kim, Son, and Kim 2021) strengthens the interaction between text and image during the feature extraction stage, resulting in efficient multi-modal features that benefit the performance of downstream tasks.  \u591a\u6a21\u6001</p> <p>To the best of our knowledge, we are the first to simultaneously consider feature extraction and matching in CAC, and we provide a decoupled analysis of the feasibility of this paradigm in the CAC, thereby streamlining the workflow of CAC task.\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6&amp;\u5339\u914d\u7684|241117</p>"},{"location":"literature/ObejectCounting/rank4%20CACViT/#class-agnostic-counting-vision-transformer","title":"Class-Agnostic Counting Vision Transformer","text":"<p>Todo</p>"},{"location":"literature/ObejectCounting/rank5%20SSD/","title":"rank5 SSD","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>arxiv\u65e5\u671f\uff1a2024\u5e745\u670820\u65e5</p> <p></p> <p>\u4f5c\u8005\uff1a\u5357\u4eac\u7406\u5de5\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u5b66\u9662</p> <p>\u5f15\u7528</p> <pre><code>@inproceedings{ijcai2024p167,\ntitle = {Learning Spatial Similarity Distribution for Few-shot Object Counting},\nauthor = {Xu, Yuanwu and Song, Feifan and Zhang, Haofeng},\nbooktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}},\npublisher = {International Joint Conferences on Artificial Intelligence Organization},\npages = {1507--1515},\nyear = {2024},\ndoi = {10.24963/ijcai.2024/167},\nurl = {https://doi.org/10.24963/ijcai.2024/167},\n}\n</code></pre> <p>\ud83d\udcdd \u5b66\u4e60\u5c11\u6837\u672c\u76ee\u6807\u8ba1\u6570\u7684\u7a7a\u95f4\u76f8\u4f3c\u5ea6\u5206\u5e03.</p> <p></p> <p>\u4e2d\u6587\u89e3\u91ca\uff1a</p> <p></p> <p></p> <p>\u4e0d\u662f\u6211\u60f3\u4ed4\u7ec6\u7814\u7a76\u7684\u65b9\u5411\uff0cpass</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/","title":"rank6 LOCA","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>\u5f15\u7528\uff1a</p> <pre><code>@InProceedings{Dukic_2023_ICCV,\n    author    = {{\\DJ}uki\\'c, Nikola and Luke\\v{z}i\\v{c}, Alan and Zavrtanik, Vitjan and Kristan, Matej},\n    title     = {A Low-Shot Object Counting Network With Iterative Prototype Adaptation},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2023},\n    pages     = {18872-18881}\n}\n</code></pre> <p>arxiv\u65e5\u671f\uff1a2023\u5e749\u670828\u65e5</p> <p>\u6b63\u5f0f\u53d1\u8868\u9875\u9762 ICCV2023</p> <p>today 241117</p> <p></p> <p>\u6807\u9898\uff1aA Low-Shot Object Counting Network With Iterative Prototype Adaptation</p> <p>\u5c0f\u6837\u672c\u8ba1\u6570\u795e\u7ecf\u7f51\u7edc\uff1a\u8fed\u4ee3\u539f\u578b\u9002\u5e94\u6a21\u5757</p> <p>\u2764\ufe0f\uff1a\u4e22\u5931\u5f62\u72b6\u4fe1\u606f\uff08size or aspect\uff09\uff0c\u56e0\u6b64\u672c\u6587\u63d0\u51fa\u4e86OPE\uff08\u76ee\u6807\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff09\u6765\u628a\u5f62\u72b6\u4fe1\u606f\u8fdb\u884c\u8fed\u4ee3\u878d\u5408</p> <p>\u672c\u6587\u505a\u4e86\u4ec0\u4e48\u4e8b\uff1f</p> <ul> <li>\u6458\u8981</li> <li>\u5f15\u5165\u2014\u8d21\u732e</li> <li>\u7ed3\u8bba</li> </ul> <p>\u5177\u4f53\u600e\u4e48\u5b9e\u73b0\uff1amethod</p> <p>\u7ed3\u679c\uff1aExperiment</p> <p>\u7814\u7a76\u95ee\u9898\u7684\u52a8\u673a\uff1aIntroduction</p> <p>\u7814\u7a76\u73b0\u72b6\uff1arelated work</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_1","title":"\u6458\u8981","text":"<p>We consider low-shot counting of arbitrary semantic categories in the image using only few annotated exemplars (few-shot) or no exemplars (no-shot). \u91c7\u7528\u5c0f\u6837\u672c\u6216\u80050\u6837\u672c \u8ba1\u6570\u4efb\u610f\u8bed\u4e49\u7c7b\u522b</p> <p>The standard few-shot pipeline follows extraction of appearance queries from exemplars and matching them with image features to infer the object counts. </p> <p>\u6807\u51c6\u7684\u5c0f\u6837\u672c \u9075\u5faa\u4ece\u6837\u672c\u4e2d\u63d0\u53d6\u5916\u89c2\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u4e0e\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u5339\u914d\uff0c\u4ee5\u63a8\u65ad\u7269\u4f53\u8ba1\u6570\u3002</p> <p>Existing methods extract queries by feature pooling which neglects the shape information (e.g., size and aspect) and leads to a reduced object localization accuracy and count estimates. \u4e22\u5931\u4e86\u5f62\u72b6\u4fe1\u606f\u548c\u5b9a\u4f4d\u4fe1\u606f\u3001\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u6c60\u5316\u63d0\u53d6\u67e5\u8be2\uff0c\u5ffd\u7565\u4e86\u5f62\u72b6\u4fe1\u606f(\u4f8b\u5982,\u5927\u5c0f\u548c\u7eb5\u6a2a\u6bd4)\uff0c\u5bfc\u81f4\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u6570\u4f30\u8ba1\u503c\u964d\u4f4e\u3002</p> <p>We propose a L ow-shot  O bject C ounting network with iterative prototype A daptation (LOCA). </p> <p>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8fed\u4ee3\u539f\u578b\u81ea\u9002\u5e94( LOCA )\u7684\u5c0f\u6837\u672c\u7269\u4f53\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>Our main contribution is the new object prototype extraction module, which iteratively fuses the exemplar shape and appearance information with image features. </p> <p>\u76ee\u6807\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u8fed\u4ee3\u878d\u5408\u793a\u4f8b\u6846\u5f62\u72b6\u548c\u5916\u89c2\u4fe1\u606f</p> <p>\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u65b0\u7684\u76ee\u6807\u539f\u578b\u63d0\u53d6\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5c06\u6837\u672c\u5f62\u72b6\u548c\u5916\u89c2\u4fe1\u606f\u4e0e\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u8fed\u4ee3\u878d\u5408\u3002</p> <p>The module is easily adapted to zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot counting problems. \u8be5\u6a21\u5757\u53ef\u4ee5\u9002\u7528\u4e8e0-shot\u573a\u666f\uff0c\u4f7f\u5f97LOCA\u80fd\u591f\u8986\u76d6\u6574\u4e2a\u4f4e\u6837\u672c\u8ba1\u6570\u95ee\u9898\u3002</p> <p>LOCA outperforms all recent state-of-the-art methods on FSC147 benchmark by 20-30% in RMSE on one-shot and fewshot and achieves state-of-the-art on zero-shot scenarios, while demonstrating better generalization capabilities. The code and models are available.</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#-","title":"\u5f15\u5165-\u8d21\u732e","text":"<p>Introduction</p> <p>\u5012\u6570\u7b2c\u4e8c\u6bb5</p> <p>\uff08\u7b2c\u4e00\u4e2a\u8d21\u732e\uff0c\u63d0\u51faLOCA\uff09We propose a Low-shot Object Counting network with iterative prototype Adaptation (LOCA).</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8fed\u4ee3\u539f\u578b\u81ea\u9002\u5e94( LOCA )\u7684\u5c0f\u6837\u672c\u7269\u4f53\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>\uff08LOCA\u7684\u4e3b\u8981\u8d21\u732e\u662fOPE\u6a21\u5757\uff0cOPE\u6a21\u5757\u7684\u7a81\u51fa\u7279\u70b9\uff09Our main contribution is the new object prototype extraction module, which separately extracts the exemplar shape and appearance queries. The shape queries are gradually adapted into object prototypes by considering the exemplar appearance as well as the appearance of non-annotated objects, obtaining excellent localization properties and leading to highly accurate counts (Figure 1). </p> <ul> <li>OPE\u6a21\u5757\uff0c\u5206\u522b\u63d0\u53d6\u6837\u4f8b\u6846\u5f62\u72b6\u548c\u5916\u89c2\u67e5\u8be2</li> <li>\u5f62\u72b6\u7279\u5f81 \u662f\u9010\u6e10\u7684\u878d\u5408\u5230 \u76ee\u6807\u539f\u578b\u7684\u3002\uff08\u901a\u8fc7\u8003\u8651\u6837\u4f8b\u5916\u89c2\u548c\u975e\u6807\u6ce8\u5bf9\u8c61\u7684\u5916\u89c2\uff09 $\\rightarrow $  \u83b7\u5f97\u826f\u597d\u7684\u5b9a\u4f4d\u5c5e\u6027\uff0c\u5e76\u5f97\u5230\u9ad8\u5ea6\u51c6\u786e\u7684\u8ba1\u6570(\u56fe1 )\u3002</li> </ul> <p>\uff08\u4eae\u70b9\uff1aLOCA\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6837\u4f8b\u6846\u5f62\u72b6\u4fe1\u606f\u6307\u5bfc\u8ba1\u6570)     To the best of our knowledge, LOCA is the first low-shot counting method that explicitly uses exemplars shape information for counting.</p> <p>\u5012\u6570\u7b2c\u4e00\u6bb5 \uff08\u5f15\u5165\u6700\u540e\u4e00\u6bb5 \u7ed3\u679c\u6bb5\uff09\u6307\u51fa\u672c\u6587\u6240\u7528\u6570\u636e\u96c6\uff1a</p> <ul> <li>FSC148\u6570\u636e\u96c6</li> <li>CARPK</li> </ul>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_2","title":"\u7ed3\u8bba","text":"<p>P1  \u7b2c\u4e00\u70b9\uff1a\u63d0\u51fa\u4e86LOCA\uff0c\u5206\u522b\u8003\u8651\u793a\u4f8b\u7684\u5f62\u72b6\u7279\u5f81\u548c\u5916\u89c2\u7279\u5f81\uff0c\u5e76\u5206\u522b\u901a\u8fc7OPE\u6a21\u5757\u8fdb\u884c\u8fed\u4ee3\u878d\u5408</p> <p>\u56e0\u6b64\uff0c\u539f\u578b\u6cdb\u5316\u5230\u56fe\u50cf\u4e2d\u7684\u975e\u6807\u6ce8\u5bf9\u8c61\uff0c\u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684\u5b9a\u4f4d\u6027\u8d28\u548c\u8ba1\u6570\u4f30\u8ba1\u3002</p> <p>We presented a new low-shot counting method LOCA, that addresses the limitations of the current state-of-the-art methods. LOCA considers the exemplar shape and appearance properties separately and iteratively adapts these into object prototypes by a new object prototype extraction (OPE) module considering the image-wide features. The prototypes thus generalize to the non-annotated objects in the image, leading to better localization properties and count estimates.</p> <p>P2 \u8bf4\u660e\u7ed3\u679c</p> <p>P3 \u6307\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411</p> <p>We envision several possible future research directions. </p> <ul> <li>Additional supervision levels such as introducing negative exemplar annotations could be introduced in LOCA for better specification of the selected object class. This could lead to interactive tools for accurate object counting. </li> </ul> <p>\u4e3a\u4e86\u66f4\u597d\u5730\u89c4\u8303\u6240\u9009\u5bf9\u8c61\u7c7b\uff0c\u53ef\u4ee5\u5728LOCA\u4e2d\u5f15\u5165\u989d\u5916\u7684\u76d1\u7763\u7ea7\u522b\uff0c\u5982\u5f15\u5165\u8d1f\u4f8b\u6ce8\u91ca\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7528\u4e8e\u7cbe\u786e\u7269\u4f53\u8ba1\u6570\u7684\u4ea4\u4e92\u5f0f\u5de5\u5177\u3002</p> <ul> <li>Furthermore, a gap between low-shot counters and object detectors could be further narrowed by enabling bounding box or segmentation mask prediction in LOCA to output additional statistics about the counted objects such as average size, etc., which is useful for many practical applications such as biomedical analysis.  </li> </ul> <p>\u6b64\u5916\uff0c\u901a\u8fc7\u5728LOCA\u4e2d\u5b9e\u73b0\u8fb9\u754c\u6846\u6216\u5206\u5272\u63a9\u7801\u9884\u6d4b\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4f4e\u6837\u672c\u8ba1\u6570\u5668\u548c\u76ee\u6807\u68c0\u6d4b\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u8f93\u51fa\u5173\u4e8e\u8ba1\u6570\u5bf9\u8c61\u7684\u989d\u5916\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f8b\u5982\u5e73\u5747\u5927\u5c0f\u7b49\uff0c\u8fd9\u5bf9\u4e8e\u751f\u7269\u533b\u5b66\u5206\u6790\u7b49\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u662f\u6709\u7528\u7684\u3002</p>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_3","title":"\u5f15\u5165","text":"<p>P1 \u7279\u5b9a\u7269\u4f53\u8ba1\u6570\uff1b\u4eba\u7fa4\u3001\u6c7d\u8f66\u3001\u7269\u79cd\uff1b\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e \u2192 \u5c0f\u6837\u672c\u8ba1\u6570\uff08\u7531\u6837\u4f8b\u6846\u6807\u51fa\u76ee\u6807\uff09\u30010\u6837\u672c\u8ba1\u6570\uff08\u8ba1\u6570\u591a\u6570\u7c7b\u522b\uff09</p> <p>Object counting considers estimation of the number of specific objects in the image. Solutions based on object detectors have been extensively explored for categories such as people [1, 33], cars [20, 12] or animal species [2, 32]. However, these methods require huge annotated training datasets and are not applicable to counting new, previously unobserved, classes with potentially only few annotations. The latter problem is explored by low-shot counting, which encompasses few-shot and zero-shot counting. Few-shot counters count all present objects of some class with only few of them annotated by bounding boxes (exemplars), while zero-shot counters consider counting the most frequent class without annotations.</p> <p>P2   \u89d2\u5ea61\uff1a\u5c0f\u6837\u672c\u8ba1\u6570&amp;\u56de\u5f52\u5bc6\u5ea6\u56fe\uff1b\u89d2\u5ea62\uff1a0-shot</p> <ul> <li> <p>Few-shot counters have recently gained momentum with the emergence of a challenging dataset [24] and follow a common pipeline [18, 24, 13, 26, 31]\uff08\u5c0f\u6837\u672c\u8ba1\u6570\uff0c\u968f\u7740\u6570\u636e\u96c6\u7684\u51fa\u73b0\uff09. Image and exemplar features are extracted into object prototypes, which are matched to the image by correlation.\uff08\u56fe\u50cf\u548c\u6837\u4f8b\u7279\u5f81\u88ab\u63d0\u53d6\u5230\u5bf9\u8c61\u539f\u578b\u4e2d\uff0c\u5bf9\u8c61\u539f\u578b\u901a\u8fc7\u76f8\u5173\u6027\u4e0e\u56fe\u50cf\u8fdb\u884c\u5339\u914d\u3002\uff09 Finally, the obtained intermediate image representation is regressed into a 2D object density map, whose values sum to the object count estimate.\uff08\u6700\u540e\uff0c\u5c06\u5f97\u5230\u7684\u4e2d\u95f4\u56fe\u50cf\u8868\u793a\u56de\u5f52\u4e3a2D\u7269\u4f53\u5bc6\u5ea6\u56fe\uff0c\u5c06\u503c\u76f8\u52a0\u5f97\u5230\u76ee\u6807\u8ba1\u6570\u7684\u4f30\u8ba1\u503c\uff09 The methods primarily differ in the intermediate image representation construction method\uff08\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e \u4e2d\u95f4\u56fe\u50cf\u8868\u793a\u7684\u6784\u5efa\u8fc7\u7a0b\uff09\uff08\u4e3e\u4f8b\u5b50\u8bf4\u660e\uff09, which is based either on Siamese similarity [18, 24], cross-attention [16, 13] or feature and similarity fusion [26, 31].</p> </li> <li> <p>While receiving much less attention, zero-shot counters follow a similar principle, but either identify possible exemplars by majority vote from region proposals [22] or implicitly by attention modules [11].\u867d\u7136\u96f6\u6837\u672c\u8ba1\u6570\u5668\u53d7\u5230\u7684\u5173\u6ce8\u8f83\u5c11\uff0c\u4f46\u9075\u5faa\u7c7b\u4f3c\u7684\u539f\u5219\uff0c\u6216\u8005\u901a\u8fc7\u533a\u57df\u63d0\u6848\u7684\u591a\u6570\u6295\u7968\u6765\u8bc6\u522b\u53ef\u80fd\u7684\u6837\u672c[ 22 ]\uff0c\u6216\u8005\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u9690\u5f0f\u5730\u8bc6\u522b\u53ef\u80fd\u7684\u6837\u672c[ 11 ]\u3002</p> </li> </ul> <p>P3   \u5f15\u51fa\u672c\u6587\u8981\u8ba8\u8bba\u7684\u95ee\u9898\uff1a</p> <p>All few-shot counters construct object prototypes by pooling image features extracted from the exemplars into fixed-sized correlation filters. The prototypes thus fail to encode the object shape information (i.e., width, height and aspect), resulting in a reduced accuracy of the density map. Recent works have shown that this information loss can be partially addressed by complex architectures for learning a nonlinear similarity function [26]. Nevertheless, we argue that a much simpler counting architecture can be used instead, by explicitly addressing the exemplar shape and by applying an appropriate object prototype adaptation method.</p> <p>P4\u3001P5\uff1b\u8d21\u732e </p> <p>structure</p> <ol> <li>\u7279\u5b9a\u7269\u4f53\u8ba1\u6570</li> <li>\u5c0f\u6837\u672c\u8ba1\u6570 &amp;  \u56de\u5f52\u5bc6\u5ea6\u56fe</li> <li>\u5f15\u51fa\u95ee\u9898\uff1a\u4e22\u5931\u5f62\u72b6\u4fe1\u606f\uff0c\u5c3d\u7ba1\u73b0\u5728\u4e5f\u6709\u7814\u7a76\u65b9\u6cd5\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u662f\u7ed3\u6784\u6bd4\u8f83\u590d\u6742</li> <li> <ol> <li>\u8d21\u732e\u3001\u7ed3\u679c</li> </ol> </li> </ol>"},{"location":"literature/ObejectCounting/rank6%20LOCA/#_4","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>P1 \u7279\u5b9a\u7269\u4f53\u8ba1\u6570</p> <p>Historically, object counting has been addressed by class-specific detectors for people [1, 33], cars [20, 12] and animals [2], but these methods do not cope well with extremely crowded scenes. In a jellyfish polyp counting scenario, [32] thus proposed to segment the image and interpret the segmentation as a collection of circular objects. Alternatively, [1, 6] framed counting as a regression of object density map, whose summation predicts the number of objects. A major drawback of these methods is that they require large annotated training datasets for each object class, which is often an unrealistic requirement.</p> <p>P2 class-agnostic counters  \u7c7b\u65e0\u5173\u8ba1\u6570\u7684\u53d1\u5c55</p> <p>In response, class-agnostic counters have been explored, that specialize to the object category at test-time using only a few user-provided object exemplars. </p> <p>\u6587\u732e\u6982\u8ff0</p> <ol> <li>An early representative [18] proposed a two-stream Generic Matching Network, that extracts the image and exemplar object features, concatenates them and regresses the representation into the final density map.  \u65e9\u671f\u7684\u4ee3\u8868[ 18 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u7684\u901a\u7528\u5339\u914d\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u63d0\u53d6\u56fe\u50cf\u548c\u6837\u672c\u5bf9\u8c61\u7279\u5f81\uff0c\u5e76\u5c06\u5b83\u4eec\u4e32\u8054\u8d77\u6765\uff0c\u5c06\u8868\u793a\u56de\u5f52\u5230\u6700\u7ec8\u7684\u5bc6\u5ea6\u56fe\u4e2d\u3002</li> <li>CFOCNet [30] noted that a mere concatenation leads to unreliable localization and proposed a Siamese correlation network(\u5b6a\u751f\u5339\u914d\u7f51) inspired by the tracking literature [3] to improve the localization and counts. CFOCNet [ 30 ]\u6307\u51fa\u7b80\u5355\u7684\u7ea7\u8054\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u5b9a\u4f4d\uff0c\u5e76\u53d7\u8ddf\u8e2a\u5347\u7684\u542f\u53d1\u63d0\u51fa\u4e86\u5b6a\u751f\u76f8\u5173\u7f51\u7edc\u6539\u8fdb\u4e86\u5b9a\u4f4d\u548c\u8ba1\u6570\u3002</li> <li>Ranjan et al. [24] proposed a further improvement of correlation robustness by test-time Siamese backbone adaptation. Ranjan\u7b49\u4eba[ 24 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5b6a\u751f\u9aa8\u5e72\u7f51\u81ea\u9002\u5e94\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u76f8\u5173\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002</li> <li>Shi et al. [26] proposed an alternative approach for jointly learning the representation as well as a nonlinear similarity metric for improved localization and applied self-attention to reduce the within-class appearance variability in the test image. Shi\u7b49\u4eba[ 26 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u8868\u793a\u7684\u66ff\u4ee3\u65b9\u6cd5\u4ee5\u53ca\u6539\u8fdb\u5b9a\u4f4d\u7684\u975e\u7ebf\u6027\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5e76\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u6765\u51cf\u5c11\u6d4b\u8bd5\u56fe\u50cf\u4e2d\u7684\u7c7b\u5185\u5916\u89c2\u53d8\u5f02\u6027\u3002</li> <li>You et al. [31] combined the similarity map with the image features before applying location regression to improve count accuracy and proposed a learnable similarity metric to guide the fusion of exemplar and image features.You\u7b49[ 31 ]\u5728\u5e94\u7528\u4f4d\u7f6e\u56de\u5f52\u4e4b\u524d\u5c06\u76f8\u4f3c\u6027\u56fe\u4e0e\u56fe\u50cf\u7279\u5f81\u7ed3\u5408\u4ee5\u63d0\u9ad8\u8ba1\u6570\u7cbe\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u6765\u6307\u5bfc\u793a\u4f8b\u548c\u56fe\u50cf\u7279\u5f81\u7684\u878d\u5408\u3002</li> <li>Liu et al. [16] adopted a vision transformer [7] for image feature extraction and a convolutional encoder to extract the exemplars. Cross-attention is used to fuse image and exemplar features and a convolutional decoder regresses the density map. Liu\u7b49[ 16 ]\u91c7\u7528\u89c6\u89c9\u8f6c\u6362\u5668[ 7 ]\u8fdb\u884c\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u5377\u79ef\u7f16\u7801\u5668\u63d0\u53d6\u6837\u672c\u3002\u4ea4\u53c9\u6ce8\u610f\u529b\u7528\u4e8e\u878d\u5408\u56fe\u50cf\u548c\u6837\u672c\u7279\u5f81\uff0c\u5377\u79ef\u89e3\u7801\u5668\u5bf9\u5bc6\u5ea6\u56fe\u8fdb\u884c\u56de\u5f52\u3002</li> <li>Recently, few-shot counting has been extended to few-shot detection [21] by adopting the transformer-based object detector [29] to predict also the object bounding box in addition to location.\u6700\u8fd1\uff0c\u5c0f\u6837\u672c\u8ba1\u6570\u5df2\u7ecf\u6269\u5c55\u5230\u5c0f\u6837\u672c\u68c0\u6d4b[ 21 ]\uff0c\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u76ee\u6807\u68c0\u6d4b\u5668[ 29 ]\uff0c\u9664\u4e86\u4f4d\u7f6e\u5916\uff0c\u8fd8\u9884\u6d4b\u76ee\u6807\u8fb9\u754c\u6846\u3002</li> </ol> <p>P3  3-shot\u2192 fewer shot</p> <p>\uff08\u8fd9\u6bb5\u6587\u732e\u7efc\u8ff0\u7684\u4e3b\u9898\uff09While most works addressed situations with several (typically three) exemplars available, only few recent works considered reducing this number. </p> <ul> <li>Lin et al. [13] proposed a counting method that requires only a single exemplar. Their method is based on a transformer architecture and formulates correlation between image and exemplar features by several self- and cross-attention blocks. </li> </ul> <p>Lin\u7b49\u4eba[ 13 ]\u63d0\u51fa\u4e86\u4e00\u79cd\u53ea\u9700\u8981\u5355\u4e2a\u6837\u672c\u7684\u8ba1\u6570\u65b9\u6cd5\u3002\u4ed6\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u4e00\u79cd\u8f6c\u6362\u5668\u7ed3\u6784\uff0c\u901a\u8fc7\u51e0\u4e2a\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u6765\u5efa\u7acb\u56fe\u50cf\u548c\u6837\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002</p> <p>\uff08\u6781\u7aef\u60c5\u51b5 0-shot\uff09An extreme case of zero-shot counting [22, 11] has been explored as well. </p> <ul> <li>Ranjan and Hoai [22] proposed RepRPN-Counter, which combines a region proposal network [25] that also predicts a repetition score of each proposal.Ranjan\u548cHoai</li> </ul> <p>[ 22 ]\u63d0\u51fa\u4e86RepRPN - Counter\uff0c\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u533a\u57df\u63d0\u6848\u7f51\u7edc[ 25 ]\uff0c\u8be5\u7f51\u7edc\u4e5f\u9884\u6d4b\u6bcf\u4e2a\u63d0\u6848\u7684\u91cd\u590d\u8bc4\u5206\u3002</p> <ul> <li>Proposals with the highest repetition scores are used as exemplars and sent through FamNet [24] to predict multiple density maps. </li> </ul> <p>\u91cd\u590d\u5f97\u5206\u6700\u9ad8\u7684\u63d0\u6848\u4f5c\u4e3a\u793a\u4f8b\uff0c\u901a\u8fc7Fam Net [ 24 ]\u53d1\u9001\uff0c\u9884\u6d4b\u591a\u4e2a\u5bc6\u5ea6\u56fe\u3002</p> <ul> <li>On the other hand, Hobley and Prisacariu [11] developed a weakly supervised method that implicitly identifies object category most likely to be counted and predicts a density map for that category. </li> </ul> <p>\u53e6\u4e00\u65b9\u9762\uff0cHobley\u548cPrisacariu [ 11 ]\u53d1\u5c55\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5b83\u9690\u5f0f\u5730\u8bc6\u522b\u6700\u6709\u53ef\u80fd\u88ab\u8ba1\u6570\u7684\u5bf9\u8c61\u7c7b\u522b\uff0c\u5e76\u9884\u6d4b\u8be5\u7c7b\u522b\u7684\u5bc6\u5ea6\u56fe\u3002</p> <ul> <li>Vision transformer with a unsupervised training stage [16] has also shown success in zero-shot counting. </li> </ul> <p>\u5177\u6709\u65e0\u76d1\u7763\u8bad\u7ec3\u9636\u6bb5\u7684\u89c6\u89c9\u8f6c\u6362\u5668[ 16 ]\u5728\u96f6\u6837\u672c\u8ba1\u6570\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u6210\u529f\u3002</p> <p>241118</p> <p>todo\uff1a\u540e\u9762\u7684\u65b9\u6cd5</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/","title":"rank7 SemAug CountTR","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p></p> <p>\u6807\u9898\uff1aSemantic Generative Augmentations for Few-Shot Counting  \u5c0f\u6837\u672c\u8ba1\u6570 \u8bed\u4e49\u751f\u6210\u589e\u5f3a</p> <p>\u672c\u6587\u2764\ufe0f\uff1a</p> <ul> <li>\u4e3a\u4e86\u4f7f\u751f\u6210\u7684\u56fe\u50cf \u7684\u76ee\u6807\u6570\u91cf\u548c\u539f\u56fe\u50cf\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f7f\u7528 stable diffusion\u5408\u6210\u56fe\u50cf\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u53cc\u6761\u4ef6\uff1aprompt &amp; \u5bc6\u5ea6\u56fe</li> <li>\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u56fe\u50cf\u603b\u662f\u8ddf\u8bad\u7ec3\u56fe\u50cf\u76f8\u540c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u589e\u5f3a\u56fe\u50cf\u591a\u6837\u6027\u7684\u7b56\u7565\uff1a\u968f\u673a\u6253\u4e71\u5b57\u5e55\u63cf\u8ff0\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u521b\u5efa\u6ca1\u89c1\u8fc7\u4f46\u662f\u5408\u7406\u7684 \u5bf9\u8c61\u7c7b\u578b\u548c\u7a7a\u95f4\u5206\u5e03</li> </ul> <p>\u5408\u6210\u56fe\u50cf &amp; \u591a\u6837\u5316\u7b56\u7565</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#abstract","title":"Abstract","text":"<p>\uff08\u6587\u751f\u56fe\u6269\u6563\u6a21\u578b\uff09</p> <p>With the availability of powerful text-to-image diffusion models, recent works have explored the use of synthetic data to improve image classification performances. </p> <p>\u968f\u7740\u5f3a\u5927\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u53ef\u7528\u6027\uff0c\u6700\u8fd1\u7684\u5de5\u4f5c\u63a2\u7d22\u4e86\u4f7f\u7528\u5408\u6210\u6570\u636e\u6765\u63d0\u9ad8\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002</p> <p>These works show that it can effectively augment or even replace real data.</p> <p>\u8fd9\u4e9b\u5de5\u4f5c\u8868\u660e\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u589e\u5f3a\u751a\u81f3\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u3002 </p> <p>In this work, we investigate how synthetic data can benefit few-shot class-agnostic counting. </p> <p>\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5408\u6210\u6570\u636e\u5982\u4f55\u6709\u5229\u4e8e\u5c0f\u6837\u672c\u7c7b\u522b\u65e0\u5173\u8ba1\u6570\u3002</p> <p>\u5408\u6210\u56fe\u50cf\u7684\u7b2c\u4e00\u4e2a\u8981\u6c42\uff1a\u5408\u6210\u7684\u56fe\u7247 \u76ee\u6807\u6570\u91cf\u662f\u76f8\u7b49\u7684</p> <p>This requires to generate images that correspond to a given input number of objects. </p> <p>\u8fd9\u5c31\u9700\u8981\u751f\u6210\u4e0e\u7ed9\u5b9a\u7684\u8f93\u5165\u7269\u4f53\u6570\u91cf\u76f8\u5bf9\u5e94\u7684\u56fe\u50cf\u3002</p> <p>However, text-to-image models struggle to grasp the notion of count.</p> <p>\u7136\u800c\uff0c\u6587\u672c\u5230\u56fe\u50cf\u7684\u6a21\u578b\u5f88\u96be\u628a\u63e1\u8ba1\u6570\u7684\u6982\u5ff5\u3002</p> <p>\u5408\u6210\u56fe\u50cf\u7684\u76d1\u7763\u4fe1\u53f7\uff1aprompt\u548c\u5bc6\u5ea6\u56fe\uff1b\u5177\u4f53\u7528\u7684\u6a21\u578b\uff1aStable Diffusion </p> <p>We propose to rely on a double conditioning of Stable Diffusion with both a prompt and a density map in order to augment a training dataset for few-shot counting. </p> <p>\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u7a33\u5b9a\u6269\u6563( Stable Diffusion )\u7684\u63d0\u793a\u56fe\u548c\u5bc6\u5ea6\u56fe\u7684\u53cc\u91cd\u6761\u4ef6\u6765\u589e\u52a0\u5c11\u6837\u672c\u8ba1\u6570\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002</p> <p>Due to the small dataset size, the fine-tuned model tends to generate images close to the training images. </p> <p>\u7531\u4e8e\u6570\u636e\u96c6\u89c4\u6a21\u8f83\u5c0f\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u63a5\u8fd1\u8bad\u7ec3\u56fe\u50cf\u7684\u56fe\u50cf\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3 \u56fe\u50cf\u603b\u662f\u63a5\u8fd1\u8bad\u7ec3\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u63d0\u51fa \u968f\u673a\u6253\u4e71\u56fe\u50cf\u4e4b\u95f4\u7684\u5b57\u5e55</p> <p>We propose to enhance the diversity of synthesized images by exchanging captions between images thus creating unseen configurations of object types and spatial layout. </p> <p>\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u5728\u56fe\u50cf\u4e4b\u95f4\u4ea4\u6362\u5b57\u5e55\u6765\u589e\u5f3a\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u521b\u5efa\u770b\u4e0d\u89c1\u7684\u5bf9\u8c61\u7c7b\u578b\u548c\u7a7a\u95f4\u5e03\u5c40\u914d\u7f6e\u3002</p> <p>\uff08\u7ed3\u679c\uff09Our experiments show that our diversified generation strategy significantly improves the counting accuracy of two recent and performing few-shot counting models on FSC147 and CARPK.</p> <p>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u591a\u6837\u5316\u751f\u6210\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86FSC147\u548cCARPK\u4e0a\u6700\u8fd1\u6267\u884c\u7684\u4e24\u4e2a\u5c11\u6837\u672c\u8ba1\u6570\u6a21\u578b\u7684\u8ba1\u6570\u51c6\u786e\u7387\u3002</p> <p>\u6570\u636e\u96c6\uff1a</p> <ul> <li>FSC147</li> <li>CARPK</li> </ul>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#-","title":"\u5f15\u5165-\u8d21\u732e","text":"<p>To tackle few-shot counting, we propose to synthesize unseen data with Stable Diffusion conditioned by both a textual prompt and a density map.</p> <p>\u4e3a\u4e86\u89e3\u51b3\u5c0f\u6837\u672c\u8ba1\u6570\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u7a33\u5b9a\u6269\u6563\u6765\u5408\u6210\u770b\u4e0d\u89c1\u7684\u6570\u636e\uff0c\u5176\u6761\u4ef6\u662f\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe\u3002</p> stable diffusion\uff1f <p>Note</p> <p>\u2460 \u4f7f\u7528stable diffusion\u5408\u6210\u6570\u636e  \u2461 \u76d1\u7763\u4fe1\u53f7\uff1a\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe       </p> <p>We thus build an augmented FSC dataset that is used to train a deep counting network. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u589e\u5e7f\u7684FSC\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>The double conditioning, implemented with ControlNet [42], allows us to generate novel synthetic images with a precise control, preserving the ground truth for the counting task. </p> <p>\u7528controlnet\u7f51\u7edc[ 42 ]\u5b9e\u73b0\u7684\u53cc\u91cd\u6761\u4ef6\u5316\uff0c\u53ef\u4ee5\u4f7f\u6211\u4eec\u5728\u7cbe\u786e\u63a7\u5236\u4e0b\u751f\u6210\u65b0\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4ece\u800c\u4e3a\u8ba1\u6570\u4efb\u52a1\u4fdd\u7559\u57fa\u672c\u7684\u771f\u503c\u3002</p> <p>It deals well with large numbers of objects, while current methods fail in such cases [19, 27]. </p> <p>\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u5927\u91cf\u7684\u5bf9\u8c61\uff0c\u800c\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5931\u6548[ 19\u300127]\u3002</p> <p>To increase the diversity of the augmented training set, we swap image descriptions between the n available training samples, leading to $\\frac{n(n\u22121)}{2} $ novel couples, each being the source of several possible synthetic images.</p> <p>\u4e3a\u4e86\u589e\u52a0\u6269\u5145\u8bad\u7ec3\u96c6\u7684\u591a\u6837\u6027\uff0c\u6211\u4eec\u5728n\u4e2a\u53ef\u7528\u7684\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u4ea4\u6362\u56fe\u50cf\u63cf\u8ff0\uff0c\u5f97\u5230n ( n-1 ) 2\u4e2a\u65b0\u7684\u5bf9\u5b50\uff0c\u6bcf\u4e2a\u5bf9\u5b50\u90fd\u662f\u82e5\u5e72\u53ef\u80fd\u7684\u5408\u6210\u56fe\u50cf\u7684\u6765\u6e90\u3002</p> <p>However, we show that some combinations do not make sense and lead to poor quality samples. </p> <p>\u7136\u800c\uff0c\u6211\u4eec\u8868\u660e\u4e00\u4e9b\u7ec4\u5408\u6ca1\u6709\u610f\u4e49\uff0c\u5e76\u5bfc\u81f4\u8d28\u91cf\u8f83\u5dee\u7684\u6837\u672c\u3002</p> <p>Therefore, we only select plausible pairs, resulting in improved augmentation quality. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ea\u9009\u62e9\u4e86\u4f3c\u662f\u800c\u975e\u7684\u914d\u5bf9\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u589e\u5f3a\u8d28\u91cf\u3002</p> <p>We evaluate our approach on two class-agnostic counting networks, namely SAFECount [41] and CounTR [6]. We show that it significantly improves the performances on the benchmark dataset FSC147 [28] and allow for a better generalization on the CARPK dataset [14].</p> <p>\u6211\u4eec\u5728SAFECount [ 41 ]\u548cCoun TR [ 6 ]\u4e24\u4e2a\u7c7b\u4e0d\u53ef\u77e5\u8ba1\u6570\u7f51\u7edc\u4e0a\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5b83\u5728\u57fa\u51c6\u6570\u636e\u96c6FSC147 [ 28 ]\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u4e14\u5728CARPK\u6570\u636e\u96c6[ 14 ]\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</p> <ul> <li> <p>\u5bf9\u6bd4\u6a21\u578b\uff1aSAFECount [41] and CounTR [6]</p> </li> <li> <p>benchmark\uff1aFSC147\u3001CARPK</p> </li> <li>\u672c\u6587\u63d0\u51fa\u7684\u662f \u5bf9\u6570\u636e\u8f93\u5165\u7684\u591a\u6837\u6027\u8fdb\u884c\u6269\u5145</li> </ul>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_1","title":"\u7ed3\u8bba","text":"<p>7 Conclusion</p> <p>\u7531\u6269\u6563\u6a21\u578b\u5408\u6210\u6570\u636e\u63d0\u9ad8FSC\u8ba1\u6570\u6027\u80fd</p> <p>We show that synthetic data generated by diffusion models improve deep models for few-shot counting. </p> <p>\u4ee5\u5bc6\u5ea6\u56fe\u4e3a\u6761\u4ef6\uff0c\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u6587\u751f\u56fe\u6a21\u578b</p> <p>We adapt a pretrained text-to-image model with a density map conditioning and </p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u591a\u6837\u5316\u7b56\u7565\uff1a\u5229\u7528\u5b57\u5e55\u76f8\u4f3c\u6027\uff0c\u751f\u6210\u5408\u7406\u7684\u4f46\u662f \u6df7\u5408\u4e86\u4e0d\u540c\u8bad\u7ec3\u56fe\u50cf\u548c\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f</p> <p>we propose a diversification strategy that exploits caption similarities to generate unseen but plausible data that mixes the semantics and the geometry of different training images. </p> <p>\u5c55\u793a\u4e86\u9009\u62e9  compatible images \uff08\u76f8\u5bb9\u7684\u56fe\u50cf\uff1f\uff09\u5408\u6210\u56fe\u50cf\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd|| \u6211\u8bb0\u5f97\u6709\u4e00\u4e2a\u6a21\u578b\u7684\u62fc\u63a5\u56fe\u50cf\u6765\u7740\uff0c\u54ea\u7bc7\u8bba\u6587\u6765\u7740\uff1f</p> <p>We show that selecting compatible images improves synthetic image quality with beneficial effects on model performance. </p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u591a\u6837\u6027\u6570\u636e\u5408\u6210\u7b56\u7565\u63d0\u9ad8\u4e86\u8ba1\u6570\u6027\u80fd\uff0cFSC147 \u548c CARPK</p> <p>We demonstrate that learning with our diverse synthetic data leads to improved counting accuracy on FSC147 and state of the art generalization on CARPK.</p> <p>\u6211\u4eec\u63d0\u51fa\u7684\u6570\u636e\u5408\u6210\u7b56\u7565\u7ecf\u8fc7\u5fae\u8c03\u53ef\u4ee5\u7528\u4e8e\u5176\u4ed6\u9886\u57df\uff1a\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272</p> <p>This strategy could be adapted to other tasks requiring fine grained compositionality, such as object detection and semantic segmentation. </p> <p>\u6211\u4eec\u7684\u591a\u6837\u5316\u7b56\u7565\uff1a\u901a\u8fc7\u5728\u5bc6\u5ea6\u56fe\u5f15\u5165\u5408\u9002\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3001\u901a\u8fc7\u6587\u672c\u4e4b\u95f4\u76f8\u4e92\u4ea4\u6362\u548c\u5bc6\u5ea6\u56fe\u7684\u63a7\u5236\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u6269\u5c55</p> <p>Our diversification scheme could be further extended by swapping both the captions and the density controls, by introducing a suitable similarity metric that operates on the density maps.</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_2","title":"\u5f15\u5165","text":"<p>1 Introduction</p> <p>P1 </p> <p>\u76ee\u6807\u8ba1\u6570\u7684\u5e94\u7528\u9886\u57df</p> <p>Counting objects is a task with applications in many domains e.g. manufacturing, medicine, monitoring, that involve different types of objects.</p> <p>\u7279\u5b9a\u76ee\u6807\u8ba1\u6570 \u2192FSC CAC\u8ba1\u6570</p> <p>\u4e24\u4e2a\u7a81\u51fa\u7279\u70b9\uff1a\u2460 bounding boxes (cf. Fig. 2),  \u2461 an extract-then-match manner [21].</p> <p>(\u540e\u9762\u6709\u4eba\u521b\u65b0\uff0c\u5c31\u628a\u8fd9\u4e2athen\u6539\u6210 and)</p> <p>While earlier works focused on learning specialized networks [2, 7, 14, 16], Few-Shot object Counting (FSC) [31] was recently introduced to train models that can count any object, including from categories outside the training data. Methods tackling FSC rely on exemplar objects annotated with bounding boxes (cf. Fig. 2), in an extract-then-match manner [21]. </p> <p>\u5982\u4f55\u5bf9\u56fe\u50cf\u7279\u5f81 \u548c \u6837\u4f8b\u6846\u7279\u5f81 \u8fdb\u884c\u5339\u914d\uff1a\u2460 correlation maps [31, 41]   \u2461attention [6, 9]</p> <p>The features of the exemplars and query image are compared using e.g. correlation maps [31, 41] or attention [6, 9]. Matched features are then transformed into a density map indicating at each location in the image the density of the objects of interest. The density map is then summed to obtain the predicted count.</p> <p>P2</p> <p>\u6570\u636e\u96c6\u751f\u6210\uff1a\u4eceGAN \u2192 \u6269\u6563\u6a21\u578b</p> <p>\u63d0\u51fachallenge  FSC147\u6570\u636e\u96c6\u6709\u9650 The reference dataset for FSC, namely FSC147 [31], contains a limited amount of data (3659 train images) thus bounding\u9650\u5236 the performances of counting networks [30]. </p> <p>\u6269\u5145\u6570\u636e\u96c6\u5f88\u9ebb\u70e6 Expanding such a dataset is costly as the annotation process requires pinpointing the center of each object present in a query image, with a potentially high number of occurrences. </p> <p>solutions To overcome the small dataset size, Ranjan et al. [30] augment FSC147 using a GAN to diversify the image styles.solutions**\u2460  Ranjan et al. [30]\u8fd9\u4e2a\u4eba\u7528GAN \u591a\u6837\u5316\u56fe\u50cf\u7684\u683c\u5f0f**</p> <p>Diffusion models have now surpassed GANs owing to their training stability and lower sensitivity to mode collapse. \u73b0\u72b6\uff1a\u6269\u6563\u6a21\u578b\ud83d\udd25\u4e86</p> <p>These models produce more effective and diverse augmentations [12, 37, 39]. Recent works mostly aim at augmenting classification datasets e.g. ImageNet [8], where augmentations are generated by prompting the models with the image labels. \u6269\u6563\u6a21\u578b\ud83d\udd25\u7684\u8bc1\u636e\uff0c\u4e14\u4e3b\u8981\ud83d\udd25\u5728\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u56fe\u50cf\u6807\u7b7e\u6765\u63d0\u793a\u6a21\u578b</p> <p>**motivation \u5230\u4e86\u6211\u4eec\u8981\u8ba8\u8bba\u7684\u95ee\u9898\uff1a\u6ca1\ud83d\udd25\u5230\u8ba1\u6570\u6570\u636e\u96c6**This fails to produce satisfying images for counting datasets as text-to-image models struggle to generate the correct number of objects [26]. \u56e0\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u5f88\u96be\u4ea7\u751f \u5bf9\u8c61\u6570\u91cf\u6b63\u786e\u7684\u6570\u636e\u96c6</p> <p>\u505a\u7684\u4e00\u4e9b\u52aa\u529b Some works tackle improving compositionality in vision-language models [19, 25, 27] but are limited to small numbers of objects. \u4e00\u4e9b\u5de5\u4f5c\u81f4\u529b\u4e8e\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b[ 19\u300125\u300127]\u7684\u7ec4\u5408\u6027\uff0c\u4f46\u4ec5\u9650\u4e8e\u5c11\u91cf\u5bf9\u8c61\u3002</p> <p>Other works add more control to pre-trained text-to-image models [15, 23, 42].\u5176\u4ed6\u5de5\u4f5c\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b[ 15\u300123\u300142]\u4e2d\u6dfb\u52a0\u4e86\u66f4\u591a\u7684\u63a7\u5236\u3002</p> <p>P3 \u672c\u6587\u7684\uff1aStable Diffusion &amp; ControlNet [42]</p> <p>\u6211\u4eec\u7684\u5de5\u4f5c \u2b50\ufe0f**To tackle few-shot counting, **we propose to synthesize unseen data with Stable Diffusion conditioned by both a textual prompt and a density map. </p> <p>\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe \u4f7f\u7528\u6269\u6563\u6a21\u578b \u751f\u6210\u6570\u636e</p> <p>We thus build an augmented FSC dataset that is used to train a deep counting network. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u589e\u5e7f\u7684FSC\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u8ba1\u6570\u7f51\u7edc\u3002</p> <p>The double conditioning, implemented with ControlNet [42], allows us to generate novel synthetic images with a precise control, preserving the ground truth for the counting task. </p> <p>\u7528controlnet\u7f51\u7edc[ 42 ]\u5b9e\u73b0\u7684\u53cc\u91cd\u6761\u4ef6\u5316\uff0c\u53ef\u4ee5\u4f7f\u6211\u4eec\u5728\u7cbe\u786e\u63a7\u5236\u4e0b\u751f\u6210\u65b0\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4ece\u800c\u4e3a\u8ba1\u6570\u4efb\u52a1\u4fdd\u7559\u57fa\u672c\u7684\u771f\u503c\u3002</p> <p>It deals well with large numbers of objects, while current methods fail in such cases [19, 27].</p> <p>\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u5927\u91cf\u7684\u5bf9\u8c61\uff0c\u800c\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5931\u6548[ 19\u300127]\u3002</p> <p>To increase the diversity of the augmented training set, we swap image descriptions between the \\(n\\) available training samples, leading to $\\frac{n(n\u22121)}{2} $ novel couples, each being the source of several possible synthetic images. </p> <p>\u4e3a\u4e86\u589e\u52a0\u6269\u5145\u8bad\u7ec3\u96c6\u7684\u591a\u6837\u6027\uff0c\u6211\u4eec\u5728n\u4e2a\u53ef\u7528\u7684\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u4ea4\u6362\u56fe\u50cf\u63cf\u8ff0\uff0c\u5f97\u5230n ( n-1 ) 2\u4e2a\u65b0\u7684\u5bf9\u5b50\uff0c\u6bcf\u4e2a\u5bf9\u5b50\u90fd\u662f\u82e5\u5e72\u53ef\u80fd\u7684\u5408\u6210\u56fe\u50cf\u7684\u6765\u6e90\u3002</p> <p>However, we show that some combinations do not make sense and lead to poor quality samples. </p> <p>\u7136\u800c\uff0c\u6211\u4eec\u8868\u660e\u4e00\u4e9b\u7ec4\u5408\u6ca1\u6709\u610f\u4e49\uff0c\u5e76\u5bfc\u81f4\u8d28\u91cf\u8f83\u5dee\u7684\u6837\u672c\u3002</p> <p>Therefore, we only select plausible pairs, resulting in improved augmentation quality. </p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u53ea\u9009\u62e9\u4e86\u4f3c\u662f\u800c\u975e\u7684\u914d\u5bf9\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u589e\u5f3a\u8d28\u91cf\u3002</p> <p>We evaluate our approach on two class-agnostic counting networks, namely SAFECount [41] and CounTR [6].</p> <p>\u6211\u4eec\u5728SAFECount [ 41 ]\u548cCoun TR [ 6 ]\u4e24\u4e2a\u7c7b\u4e0d\u53ef\u77e5\u8ba1\u6570\u7f51\u7edc\u4e0a\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002</p> <p>We show that it significantly improves the performances on the benchmark dataset FSC147 [28] and allow for a better generalization on the CARPK dataset [14].</p> <p>\u6211\u4eec\u8bc1\u660e\u4e86\u5b83\u5728\u57fa\u51c6\u6570\u636e\u96c6FSC147 [ 28 ]\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u4e14\u5728CARPK\u6570\u636e\u96c6[ 14 ]\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</p>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_3","title":"\u603b\u7ed3","text":"<p>\u672c\u6587\u7684\u5f15\u5165\u662f\u4e09\u6bb5\uff1a</p> <p>\u7b2c\u4e00\u6bb5\uff1a\u76ee\u6807\u8ba1\u6570\u7684\u5e94\u7528\u9886\u57df \u7ecf\u5386\u4e86\u4ece\u7279\u5b9a\u7269\u4f53 \u5230 \u901a\u7528\u7269\u4f53</p> <p>\u7b2c\u4e8c\u6bb5\uff1a\u4ecb\u7ecd\u6570\u636e\u96c6\u5408\u6210\u65b9\u6cd5\uff1a\u4eceGAN \u5230 Stable Diffusion</p> <p>\u7b2c\u4e09\u6bb5\uff1a\u6307\u51fa\u672c\u6587\u8d21\u732e\uff0c\u518d\u6b21\u5f3a\u8c03</p> <ul> <li>\u751f\u6210\u56fe\u50cf\uff1aStable diffusion</li> <li>\u5408\u6210\u56fe\u50cf\u7684\u76ee\u6807\u6570\u91cf  \u548c \u53c2\u8003\u56fe\u50cf \u662f\u76f8\u540c\u7684</li> <li>prompt \u548c density map\u540c\u65f6\u6307\u5bfc\u56fe\u50cf\u5408\u6210</li> <li>\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u7b56\u7565\uff1a</li> <li>swap image descriptions \uff1b\u968f\u673a\u4ea4\u6362\u56fe\u50cf\u63cf\u8ff0</li> </ul>"},{"location":"literature/ObejectCounting/rank7%20SemAug_CountTR/#_4","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>\u603b\u7ed3\uff1a\u672c\u6587\u7684\u76f8\u5173\u5de5\u4f5c\u4ece\u4e24\u65b9\u9762\u5c55\u5f00\uff1a</p> <p>Learning with Generated Data</p> <p>Few-shot Object Counting</p> <p>\u751f\u6210\u6570\u636e\u7684\u5b66\u4e60 \u548c \u5c0f\u6837\u672c\u8ba1\u6570</p> <p>\u7b2c\u4e00\u6bb5\uff1a</p> <p>\u7b2c\u4e00\u90e8\u5206\uff1aLearning with Generated Data</p> <p>\uff08\u73b0\u72b6\uff1a\uff09</p> <p>Improvements in image synthesis using generative models have sparked great interest in generating fake images to train deep neural networks. </p> <p>\u4f7f\u7528\u751f\u6210\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u5408\u6210\u7684\u6539\u8fdb\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u751f\u6210\u5047\u56fe\u50cf\u4ee5\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6781\u5927\u5174\u8da3\u3002 </p> <p>\uff08\u4eceGAN\u5f00\u59cb\uff09</p> <p>GANs were the first popular models to synthesize data for image classification [1, 5, 17], crowd counting [40] and image segmentation [43]. </p> <p>GANs\u662f\u7b2c\u4e00\u4e2a\u6d41\u884c\u7684\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b[ 1\u30015\u300117]\u3001\u4eba\u7fa4\u8ba1\u6570[ 40 ]\u548c\u56fe\u50cf\u5206\u5272[ 43 ]\u7684\u6570\u636e\u5408\u6210\u6a21\u578b\u3002 </p> <p>\uff08\u5230\u73b0\u5728\u7684\u6269\u6563\u6a21\u578b\uff1aDDPM\u3001 Latent Diffusion \uff09</p> <p>Nowadays, diffusion models such as DDPM [13] or Latent Diffusion [32] seem to outperform GANs, demonstrating more stable training, better coverage of the training distribution and higher image quality. </p> <p>\u5982\u4eca\uff0c\u6269\u6563\u6a21\u578b\u5982DDPM [ 13 ]\u6216Latent Diffusion [ 32 ]\u4f3c\u4e4e\u4f18\u4e8eGANs\uff0c\u663e\u793a\u51fa\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\uff0c\u66f4\u597d\u7684\u8bad\u7ec3\u5206\u5e03\u8986\u76d6\u7387\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u3002 </p> <p>\uff08\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ee5\u6587\u672c\u4e3a\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\uff09</p> <p>The availability of powerful text-conditioned diffusion models \u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b [24, 29, 32, 33] has led to many works exploring how to leverage synthetic data for computer vision \u5229\u7528\u751f\u6210\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1, e.g. image classification in low-data regime [12], zero/few-shot learning [37, 39], ImageNet classification [3, 4, 34] and self-supervised learning [38]. </p> <p>\u5f3a\u5927\u7684\u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b[ 24\u300129\u300132\u300133]\u7684\u51fa\u73b0\uff0c\u5f15\u53d1\u4e86\u8bb8\u591a\u7814\u7a76\u5982\u4f55\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5de5\u4f5c\u3002</p> <p>These works focus on how to reduce domain gap \u51cf\u5c11\u9886\u57df\u9e3f\u6c9f [12], improve the prompts \u6539\u8fdb\u63d0\u793a using e.g. text-to-sentence model [12] or WordNet [34] and increase diversity \u589e\u52a0\u591a\u6837\u6027 by optimizing the guidance scale [3, 34, 37].   \u4f18\u5316\u6307\u5bfc\u5c3a\u5ea6</p> <p>\u8fd9\u4e9b\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4f55\u51cf\u5c11\u9886\u57df\u9e3f\u6c9f[ 12 ]\uff0c\u4f7f\u7528\u6587\u672c\u5230\u53e5\u5b50\u6a21\u578b[ 12 ]\u6216\u8bcd\u7f51[ 34 ]\u6765\u6539\u8fdb\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6307\u5bfc\u5c3a\u5ea6[ 3,34,37]\u6765\u589e\u52a0\u591a\u6837\u6027\u3002</p> <p>This body of literature consistently demonstrates how generated data allow deep networks to learn more robust representations and improve generalization for image classification. </p> <p>\u8fd9\u7ec4\u6587\u732e\u4e00\u81f4\u5730\u5c55\u793a\u4e86\u751f\u6210\u6570\u636e\u5982\u4f55\u8ba9\u6df1\u5ea6\u7f51\u7edc\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u5e76\u63d0\u9ad8\u56fe\u50cf\u5206\u7c7b\u7684\u6cdb\u5316\u6027\u3002 </p> <p>\u6211\u4eec\u5728\u56fe\u50cf\u5408\u6210\u9886\u57df\u7684\u5de5\u4f5c\uff1a</p> <p>\u2764\ufe0f\uff1aTo bring the power of synthetic data to counting,  we propose to condition diffusion models  not only on text prompts but also on counting density maps to generate images with the correct number of objects in the desired spatial configuration. </p> <ul> <li>\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff1a\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u5bc6\u5ea6\u56fe</li> <li>\u5728\u671f\u671b\u7684\u7a7a\u95f4\u4f4d\u7f6e\u4e0a\uff0c\u751f\u6210\u6709\u6b63\u786e\u6570\u91cf\u7684\u56fe\u7247</li> </ul> <p>We focus more specifically on few-shot class-agnostic object counting. Compared to image classification, this task involves small datasets and local spatial understanding, as objects can be small and follow complex layouts.</p> <p>\u6211\u4eec\u66f4\u4e13\u6ce8\u4e8e\u5c11\u6837\u672c\u7c7b\u65e0\u5173\u7269\u4f53\u8ba1\u6570\u3002\u4e0e\u56fe\u50cf\u5206\u7c7b\u76f8\u6bd4\uff0c\u8fd9\u9879\u4efb\u52a1\u6d89\u53ca\u5c0f\u578b\u6570\u636e\u96c6\u548c\u5c40\u90e8\u7a7a\u95f4\u7406\u89e3\uff0c\u56e0\u4e3a\u5bf9\u8c61\u53ef\u4ee5\u662f\u5c0f\u578b\u7684\uff0c\u5e76\u4e14\u9075\u5faa\u590d\u6742\u7684\u5e03\u5c40\u3002</p> <p>The generated data needs a level of compositionality that current generative models, including diffusion models \u6269\u6563\u6a21\u578b, struggle to achieve. </p> <p>\u751f\u6210\u7684\u6570\u636e\u9700\u8981\u4e00\u79cd \u7ec4\u5408\u6027\u6c34\u5e73 \uff0c\u8fd9\u662f\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ec**\u6269\u6563\u6a21\u578b**\uff0c\u5728\u5b9e\u73b0\u4e0a\u5b58\u5728\u56f0\u96be\u7684\u3002</p> <p>\u751f\u6210\u6570\u636e\u9700\u8981\u7ec4\u5408\uff0c\u8fd9\u662f\u73b0\u5728\u7684\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ec\u6269\u6563\u6a21\u578b\uff0c\u96be\u4ee5\u5b9e\u73b0\u7684\u3002</p> <p>To bring the power of synthetic data to counting,  we propose to condition diffusion models \u6761\u4ef6\u6269\u6563\u6a21\u578b not only on text prompts but also on counting density maps \u6587\u672c\u63d0\u793a+\u5bc6\u5ea6\u56fe to generate images with the correct number of objects \u751f\u6210\u6709\u6b63\u786e\u5bf9\u8c61\u6570\u91cf\u7684\u56fe\u7247 in the desired spatial configuration. \u5728\u671f\u671b\u7684\u7a7a\u95f4\u4f4d\u7f6e\u4e0a </p> <p>\u4e3a\u4e86\u5c06\u5408\u6210\u6570\u636e\u7684\u80fd\u529b\u7528\u4e8e\u8ba1\u6570\uff0c\u6211\u4eec\u63d0\u51fa\u4e0d\u4ec5\u5728\u6587\u672c\u63d0\u793a\u4e0a\uff0c\u800c\u4e14\u5728\u8ba1\u6570\u5bc6\u5ea6\u56fe\u4e0a\u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4ee5\u751f\u6210\u6240\u9700\u7a7a\u95f4\u914d\u7f6e\u4e2d\u5177\u6709\u6b63\u786e\u6570\u91cf\u5bf9\u8c61\u7684\u56fe\u50cf\u3002</p> <p>We exploit this double control to generate diversified unseen data by prompting the model with novel combinations of the controls.</p> <p>\u6211\u4eec\u5229\u7528\u8fd9\u79cd\u53cc\u91cd\u63a7\u5236\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u672a\u89c1\u6570\u636e\uff0c\u901a\u8fc7\u4f7f\u7528\u65b0\u7684\u63a7\u5236\u7ec4\u5408\u6765\u4fc3\u4f7f\u6a21\u578b\u3002</p> <p>Tip</p> <p>\u6211\u5199\u8fd9\u90e8\u5206\u6587\u732e\u7efc\u8ff0\u7684\u65f6\u5019\uff0c\u4e5f\u4eceGAN\u5f00\u59cb\uff0c\u5199\u5230diffusion\uff0c\u6700\u540e\u5230\u5173\u4e8e\u76ee\u6807\u8ba1\u6570</p> <p>\u7b2c\u4e8c\u6bb5</p> <p>\u7b2c\u4e8c\u90e8\u5206\uff1aFew-shot Object Counting \u5c0f\u6837\u672c\u8ba1\u6570\u7684\u53d1\u5c55</p> <p>CAC\u4efb\u52a1\u7684\u5b9a\u4e49</p> <p>The goal of few-shot class agnostic object counting is to count how many instances of objects of any arbitrary category there are in a given image, by leveraging only a few exemplars of the category of interest. </p> <p>\u6587\u732e1\uff1aFamNet</p> <p>This was initially formulated as matching exemplars and image patch features [21]. FSC147 [31] was later put forward as the main dataset for this task, with an open set train and test split to evaluate generalization to unseen object categories. Its authors introduced FamNet, a deep net trained to infer density maps from feature similarities.</p> <p>\u6587\u732e2\uff1aBMNet</p> <p>In the same lineage, BMNet [36] refines the similarity map by learning the similarity metric jointly with the counting network. </p> <p>\u6587\u732e3\uff1aSAFECount</p> <p>In SAFECount [41], the similarities are used to fuse exemplars features into the query image features. The density map is then predicted from the enhanced features.</p> <p>\u6587\u732e4\u30015\uff1aCounTR [6] and LOCA [9]</p> <p>Other works e.g. CounTR [6] and LOCA [9] focus on improving the feature representations using a Transformer backbone as the visual encoder and injecting information about the exemplars\u2019 shape in the network [9]. </p> <p>\u6587\u732e6\uff1a\u4e0e\u6211\u4eec\u5de5\u4f5c\u6700\u76f8\u5173\u7684\u6587\u732e Vicinal Couting Network from Rajan et al. [30]  \u2192 \u8bf4\u660e \u56fe\u50cf\u589e\u5f3a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8ba1\u6570\u6027\u80fd</p> <p>The closest comparison to our work is the Vicinal Couting Network from Rajan et al. [30]. It augments FSC147 with generated data by training a conditional GAN jointly with the counting network, producing augmentations that preserve the image content while modifying its visual appearance. While outperformed by later models, it introduced the idea that well-chosen augmentations can significantly boost counting accuracy. </p> <p>\u6211\u4eec\u7684\u5de5\u4f5c\uff1a\u591a\u6837\u6027\u5408\u6210\u7b56\u7565\uff1a\u4e0d\u4ec5\u5408\u6210\u5916\u89c2\uff0c\u8fd8\u53ef\u4ee5\u6539\u53d8\u5185\u5bb9\uff1b\u4f7f\u7528\u7684\u6587\u751f\u56fe\u7684\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b</p> <p>In this work, we leverage large pre-trained text-to-image diffusion models to produce diverse augmentations that not only alter the appearance, but are also able to change the content, to synthesize augmentations with a variety of object semantics.</p> <p>24\u00b711\u00b718</p> <p>todo\uff1amethod</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/","title":"rank8 CounTR","text":"<p>\u539f\u6587\u94fe\u63a5</p> <p>\u6e90\u7801\u94fe\u63a5</p> <p>\u671f\u520a\uff1aBMVC</p> <p></p> <ul> <li>\u53d1\u8868\u65f6\u95f4\uff1aBMVC2022 \u3010British Machine Vision Conference\u3011</li> <li>\u4f5c\u8005\uff1a\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\uff1bVGG \u89c6\u89c9\u51e0\u4f55\u7ec4 \u82f1\u56fd\u54c8\u4f5b\u5927\u5b66</li> <li> <p>arxiv\u65e5\u671f\uff1a2023\u5e742\u67082\u65e5\uff08\u5148\u53d1\u8868\u3001\u540e\u6302\u7684arxiv\uff09</p> </li> <li> <p>\u5f15\u7528\uff1a</p> </li> </ul> <pre><code>@inproceedings{liu2022countr,\n  author = {Chang, Liu and Yujie, Zhong and Andrew, Zisserman and Weidi, Xie},\n  title = {CounTR: Transformer-based Generalised Visual Counting},\n  booktitle={British Machine Vision Conference (BMVC)},\n  year = {2022}\n}\n</code></pre> <ul> <li>\u6587\u7ae0\u6807\u9898\uff1aCounTR: Transformer-based Generalised Visual Counting</li> </ul> <p>\u57fa\u4e8eTransformer\u7684\u901a\u7528\u3001\u6cdb\u5316\u89c6\u89c9\u8ba1\u6570</p> <ul> <li>\u672c\u6587\u53d1\u73b0\u4ec0\u4e48\u95ee\u9898\uff0c\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff1f</li> </ul> <p></p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_1","title":"\u2764 \u5168\u6587\u6982\u62ec","text":"<p>\u8bbe\u8ba1\u6a21\u5757\u3011\uff08\u5520\u5520\u53e8\u53e8\u8bf4\u4e86\u4e09\u904d\uff1b\u6458\u8981\u8bf4\u3001\u8d21\u732e\u8bf4\u3001\u7ed3\u8bba\u8bf4\uff09\uff1a</p> <ul> <li> <p>CountTR\uff1b</p> </li> <li> <p>\u4e24\u9636\u6bb5\u8bad\u7ec3\u3010why\uff1f\u3011\uff1b</p> </li> <li> <p>\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u3010input\u7684\u521b\u65b0\u3011\uff1b</p> </li> <li> <p>\u7ed3\u679csota</p> </li> </ul> <p>\u3010\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898? \u3011\uff1a\u8fd9\u7bc7\u6587\u7ae0\u597d\u50cf\u66f4\u5173\u6ce8\u76ee\u6807\u8ba1\u6570\u7684\u6cdb\u5316\u6027\u3002</p> <p>\u6838\u5fc3\u7684\u4e00\u53e5\u8bdd\uff1a</p> <p>\u2b50\ufe0f the generalised visual object counting problem of counting the number of objects from arbitrary semantic categories using arbitrary number of \u201cexemplars\u201d</p> <p>\u2b50\ufe0f Any shot counting</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_2","title":"\u6458\u8981","text":"<p>\u672c\u6587\u6838\u5fc3\uff1a</p> <ul> <li> <p>\u901a\u7528\u89c6\u89c9\u7269\u4f53\u8ba1\u6570</p> </li> <li> <p>\u5bf9\u4e8e\u4efb\u610f\u7c7b\u522b\u3001\u7ed9\u5b9a\u4efb\u610f\u793a\u4f8b</p> </li> </ul> <p>In this paper, we consider the problem of generalised visual object counting, with the goal of developing a computational model for counting the number of objects from arbitrary semantic categories, using arbitrary number of \u201cexemplars\u201d, i.e. zero-shot or few shot counting. </p> <p>\u56db\u4e2a\u8d21\u732e\uff1a</p> <p>To this end, we make the following four contributions:</p> <ol> <li>We introduce a novel transformer-based architecture for generalised visual object counting, termed as Counting TRansformer (CounTR), which explicitly captures the similarity between image patches or with given \u201cexemplars\u201d using the attention mechanism; </li> <li>We adopt a two-stage training regime, that first pre-trains the model with self-supervised learning, and followed by supervised fine-tuning; </li> <li>We propose a simple, scalable pipeline for synthesizing training images with a large number of instances or that from different semantic categories, explicitly forcing the model to make use of the given \u201cexemplars\u201d; </li> <li>We conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC147, and demonstrate state-of-the-art performance on both zero and few-shot settings. Project page: https://verg-avesta.github.io/CounTR_Webpage/.</li> </ol> <p>\u672c\u6587\u7f3a\u9677\uff1a\u540e\u9762\u7684CACViT\u5bf9\u6bd4\u8fd9\u7bc7\u8bba\u6587</p> <p>\u4e24\u9636\u6bb5 extract-then-match  \u2192 \u5355\u4e00\u9636\u6bb5\uff0c\u540c\u65f6 extract-and-match</p> <p>\u56db\u4e2a\u8d21\u732e\uff1a</p> <ol> <li>\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8ba1\u6570\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6355\u6349\u56fe\u50cf\u5757\u548c\u793a\u4f8b\u7684\u76f8\u4f3c\u6027</li> <li>\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a</li> <li>\u7b2c\u4e00\u9636\u6bb5\uff1a\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b</li> <li>\u7b2c\u4e8c\u9636\u6bb5\uff1a\u76d1\u7763\u3001\u5fae\u8c03</li> <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u3001\u53ef\u4f38\u7f29\u7684\u3001\u80fd\u591f\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u7684\uff0c\u5927\u91cf\u5b9e\u4f8b\u3001\u6765\u81ea\u4e0d\u540c\u7684\u8bed\u4e49\u7c7b\u522b\u3001\u4f7f\u5f97\u6a21\u578b\u5145\u5206\u5229\u7528\u7ed9\u5b9a\u793a\u4f8b</li> <li>\u6d88\u878d\u5b9e\u9a8c</li> </ol> <p>\u5bf9\uff01\u5c31\u662f\u8fd9\u7bc7\u8bba\u6587\uff0c\u56db\u5f20\u56fe\u7247\u62fc\u63a5\u8fdb\u884c\u56fe\u50cf\u5408\u6210\uff01\u628a\u8fd9\u7bc7\u8bba\u6587\u548cSemAug CounTR\u533a\u5206\uff1b</p> <ul> <li>\u90fd\u662f\u5408\u6210\u56fe\u50cf\uff0c\u672c\u6587\u7528\u7684\u662f\u88c1\u526a\u3001\u62fc\u63a5\u3001\u6df7\u5408\u3001\u7f29\u653e</li> <li>\u8bed\u4e49\u589e\u5f3a\u7684SemAug CounTR\u7528\u7684\u662f\uff1aStable duffusion</li> </ul> <p></p> <p>\u7ffb\u8bd1\u4e00\u4e0b\u8fd9\u91cc\u7684\u6587\u5b57\uff1a\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u7684\u5904\u7406\u6d41\u7a0b</p> <p>(1) \u4ee3\u8868\u88c1\u526a\u548c\u7f29\u653e</p> <p>(2) \u4ee3\u8868\u62fc\u8d34\u548c\u6df7\u5408</p> <p>\u5728\u63a5\u4e0b\u6765\u7684\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u5c06\u88c1\u526a\u3001\u7f29\u653e\u548c\u62fc\u8d34\u5408\u5e76\u4e3a\u62fc\u8d34\u9636\u6bb5\u3002Type A \u4f7f\u7528\u56db\u5f20\u4e0d\u540c\u7684\u56fe\u50cf\u6765\u63d0\u9ad8\u80cc\u666f\u591a\u6837\u6027\uff0c\u800c Type B \u53ea\u4f7f\u7528\u4e00\u5f20\u56fe\u50cf\u6765\u589e\u52a0\u56fe\u50cf\u4e2d\u5305\u542b\u7684\u7269\u4f53\u6570\u91cf\u3002\u767d\u8272\u9ad8\u5149\u533a\u57df\u662f\u7ecf\u8fc7\u9ad8\u65af\u6ee4\u6ce2\u540e\u7684\u70b9\u6ce8\u91ca\u5bc6\u5ea6\u56fe\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u3002</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_3","title":"\u603b\u7ed3\u6458\u8981","text":"<ul> <li> <p>\u8003\u8651\u7684\u95ee\u9898\uff1a\u89c6\u89c9\u76ee\u6807\u8ba1\u6570\u7684\u6cdb\u5316\u6027</p> </li> <li> <p>\u76ee\u7684\u662f\u4e3a\u4e86\u8bbe\u8ba1\u4e00\u4e2a\u6a21\u578b\uff1a\u7ed9\u5b9a\u4efb\u610f\u6570\u91cf\u7684\u793a\u4f8b\u6846\uff0c\u8ba1\u6570 \u4efb\u610f\u7684\u8bed\u4e49\u7269\u4f53   </p> </li> </ul> <p>\u6211\u4eec\u7684\u6a21\u578b\u65e2\u9002\u7528\u4e8e0-shot\u60c5\u5f62 \u4e5f\u9002\u5408 few-shot\u60c5\u5f62</p> <ul> <li> <p>4\u4e2a\u8d21\u732e\uff1a</p> </li> <li> <p>\u8bbe\u8ba1\u4e00\u4e2a\u65b0\u7684Transformer\u67b6\u6784 \u2192 \u6355\u6349\u56fe\u50cfpatch \u4e0e \u793a\u4f8b\u6846\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027</p> </li> <li> <p>\u91c7\u7528\u4e24\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3</p> <pre><code>\u7b2c\u4e00\u6b65\uff1a\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u6a21\u578b\n\u7b2c\u4e8c\u6b65\uff1a\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\n</code></pre> </li> <li> <p>\u8bbe\u8ba1\u4e86\u4e00\u4e2apipeline\uff0c\u7528\u4e8e\u5408\u6210\u8bad\u7ec3\u56fe\u50cf</p> <ol> <li> <p>\u3010\u4ec0\u4e48\u6837\u7684\u8bad\u7ec3\u56fe\u50cf\uff1f\u3011</p> <pre><code>\u6709\u5927\u91cf\u5b9e\u4f8b\n \u6765\u81ea\u4e0d\u540c\u7684\u8bed\u4e49\u7c7b\u522b\n</code></pre> </li> <li> <p>\u3010\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\uff1f\u3011</p> <ol> <li>\u5f3a\u8feb\u6a21\u578b\u4f7f\u7528 \u7ed9\u5b9a\u7684\u793a\u4f8b\u6846</li> </ol> <p>\u3010\u793a\u4f8b\u6846\u4e3a\u4ec0\u4e48\u52a0\u5f15\u53f7\uff1f\u3011</p> </li> </ol> </li> <li> <p>\u3010\u7ed3\u679c\u3011\u5728FSC147\u6570\u636e\u96c6\u4e0a\u505a\u6d88\u878d\u5b9e\u9a8c\uff0c\u57280-shot  &amp; 1-shot\u4e0a\u90fd\u8fbe\u5230\u4e86SOTA</p> </li> <li> <p>\u4ee3\u7801\u516c\u5f00\u3001\u81ea\u5df1\u505a\u4e86\u4e2a\u7f51\u9875\uff1ahttps://verg-avesta.github.io/CounTR_Webpage/</p> </li> </ul>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#-","title":"\u5f15\u5165-\u8d21\u732e","text":"<p>\uff08\u539f\u6587\uff09To summarise, in this paper, we make four contributions: </p> <p>First, we introduce an architecture for generalised visual object counting based on transformer, termed as CounTR (pronounced as counter). It exploits the attention mechanisms to explicitly capture the similarity between image patches, or with the few-shot instance \u201cexemplars\u201d provided by the end user; </p> <p>Second, we adopt a two-stage training regime (self-supervised pre-training, followed by supervised fine-tuning) and show its effectiveness for the task of visual counting; </p> <p>Third, we propose a simple yet scalable pipeline for synthesizing training images with a large number of instances, and demonstrate that it can significantly improve the performance on images containing a large number of object instances; </p> <p>Fourth, we conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC-147 [24], and demonstrate state-of-the-art performance on both zero-shot and few-shot settings, improving the previous best approach by over 18.3% on the mean absolute error of the test set.</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#-_1","title":"\u603b\u7ed3 \u5f15\u5165-\u8d21\u732e","text":"<p>\u6211\u4eec\u7684\u6a21\u578b\u67094\u4e2a\u8d21\u732e\uff1a</p> <ol> <li> <p>\u3010\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u3011CounTR : \u57fa\u4e8eTransformer\uff1b\u5173\u6ce8\u53ef\u89c1\u7269\u4f53\u7684\u6cdb\u5316 \u8ba1\u6570\u80fd\u529b\uff0c\u8fd9\u8868\u660e\uff1a</p> </li> <li> <p>\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6e05\u695a\u5730\u6355\u6349\u5230image patches\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027</p> </li> <li> <p>\u6216\u8005 few-shot\u4efb\u52a1\u4e2d\uff0c\u7ed9\u5b9a\u793a\u4f8b\u6846\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027</p> </li> <li> <p>\u3010\u4e24\u9636\u6bb5\u8bad\u7ec3\u3011\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5177\u4f53\u6765\u8bf4\u5c31\u662f</p> </li> <li> <p>\u81ea\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u9884\u8bad\u7ec3</p> </li> <li>\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03</li> </ol> <p>\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e2a\u65b9\u6cd5 \u5bf9\u4e8e\u89c6\u89c9\u8ba1\u6570\u4efb\u52a1\u7684\u6709\u6548\u6027</p> <ol> <li> <p>\u3010\u5408\u6210\u56fe\u7247\u3011\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355 \u5e76\u4e14\u5c3a\u5ea6\u53ef\u53d8\u7684 pipeline \u6765\u5408\u6210\u6709\u5927\u91cf\u5b9e\u4f8b\u7684 \u8bad\u7ec3\u56fe\u7247\uff1b\u5e76\u8868\u660e \u5b83\u80fd\u663e\u8457\u63d0\u9ad8 \u6a21\u578b  \u56fe\u50cf\u5728\u6709\u5927\u91cf\u8ba1\u6570\u5bf9\u8c61\u7684\u60c5\u51b5\u4e0b\u7684 \u8ba1\u6570\u6027\u80fd</p> </li> <li> <p>\u3010\u7ed3\u679c\u3011\u6211\u4eec\u5728\u5927\u89c4\u6a21\u8ba1\u6570benchmark\u4e0b\uff0c\u8fdb\u884c\u4e86\u6574\u4e2a\u6d88\u878d\u5b9e\u9a8c\uff0c\u6bd4\u5982FSC147\u6570\u636e\u96c6</p> </li> <li> <p>\u6211\u4eec\u7684\u7ed3\u679c\uff1a0-shot\u30011-shot \u90fd\u662fsota</p> </li> <li>\u5177\u4f53\u6765\u8bf4\uff1aMAE\uff1b test set \uff1b\u4e4b\u524d\u6700\u597d\u7684\u65b9\u6cd5 \u63d0\u5347\u8d8518.3%</li> </ol>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_4","title":"\u7ed3\u8bba","text":"<p>\u539f\u6587\uff1a</p> <p>In this work, we aim at the generalised visual object counting problem of counting the number of objects from arbitrary semantic categories using arbitrary number of \u201cexemplars\u201d. We propose a novel transformer-based architecture for it, termed as CounTR. It is first pretrained with self-supervised learning, and followed by supervised fine-tuning. We also propose a simple, scalable pipeline for synthesizing training images that can explicitly force the model to make use of the given \u201cexemplars\u201d. Our model achieves state-of-the-art performance on both zero-shot and few-shot settings.</p> <p>\u3010\u518d\u6b21\u91cd\u7533\u672c\u6587\u7684\u7814\u7a76target\u3011</p> <p>\u6458\u8981\u548c\u7ed3\u8bba\u90fd\u5728\u5f3a\u8c03</p> <p>\u6211\u4eec\u7684\u6a21\u578b\u81f4\u529b\u4e8e\uff1a\u4e00\u822c\u5316\u7684\u89c6\u89c9\u7269\u4f53\u8ba1\u6570\u95ee\u9898</p> <p>\u5177\u4f53\u6765\u8bf4\u5c31\u662f\uff1a  \u4f7f\u7528\u4efb\u610f\u6570\u91cf\u7684\u6837\u4f8b \u8ba1\u6570 \u4efb\u610f\u8bed\u4e49\u7c7b\u522b\u7684\u5bf9\u8c61\u6570\u91cf</p> <p>\u56e0\u6b64\uff0c</p> <p>\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a \u65b0\u7684 \u57fa\u4e8eTransformer\u7684\u67b6\u6784 \u53eb CountTR\uff0c</p> <p>CountTR =\u300b \u4e24\u9636\u6bb5\u8bad\u7ec3 = \u81ea\u76d1\u7763\u5b66\u4e60 \u9884\u8bad\u7ec3  + \u76d1\u7763\u5b66\u4e60 \u5fae\u8c03</p> <p>pipeline\uff1a\u5408\u6210\u8bad\u7ec3\u56fe\u50cf  =\u300b\u662f\u7684\u6a21\u578b\u80fd\u591f\u5229\u7528\u7ed9\u5b9a\u7684 \u6837\u4f8b</p> <p>0-shot\u3001few-shot \u90fd\u662f sota</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_5","title":"\u968f\u624b\u8bb0","text":"<p>scalable\uff1a\u53ef\u4f38\u7f29\u7684</p> <p>pipeline\uff1a\u6574\u4e2a\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u6d41\u7a0b\u3001\u7aef\u5230\u7aef\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6db5\u76d6\u4e86\u4ece\u539f\u59cb\u6570\u636e\u5230\u6700\u7ec8\u7ed3\u679c\u7684\u5168\u8fc7\u7a0b</p> <p>pipeline\u662f\u6574\u4e2a\u6d41\u7a0b\u7684\u6982\u8ff0</p> <p>backbone\u662f\u6a21\u578b\u4e2d\u7684\u5173\u952e\u7279\u5f81\u63d0\u53d6\u90e8\u5206</p> <p>benchmark\u662f\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u6807\u51c6\u6d4b\u8bd5\u548c\u6307\u6807</p> <p>mosaic training data generation, namely, collage and blending</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_6","title":"\u5f15\u5165","text":"<p>\u7b2c\u4e00\u6bb5\uff1a\u5c0f\u4e8e5\u7684\u80fd\u5feb\u901f\u8ba1\u6570\uff0c\u6570\u91cf\u589e\u591a\u8ba1\u6570\u80fd\u529b\u6025\u5267\u4e0b\u964d</p> <p>Despite all the exceptional abilities, the human visual system is particularly weak in counting objects in the image. In fact, given a visual scene with a collection of objects, one can only make a rapid, accurate, and confident judgment if the number of items is below five, with an ability termed as subitizing [16]. While for scenes with an increasing number of objects, the accuracy and confidence of the judgments tend to decrease dramatically. Until at some point, counting can only be accomplished by calculating estimates or enumerating the instances, which incurs low accuracy or tremendous time cost.</p> <p>\u7b2c\u4e8c\u6bb5\uff1a\u6211\u4eec\u6587\u7ae0\u7684\u76ee\u6807 \u901a\u7528\u89c6\u89c9\u7269\u4f53\u8ba1\u6570\u7cfb\u7edf</p> <p>In this paper, our goal is to develop a generalised visual object counting system, that augments humans\u2019 ability for recognising the number of objects in a visual scene. .......</p> <p>To this end, we propose a novel architecture that transforms the input image (with the few-shot annotations if any) into a density map, and the final count can be obtained by simply summing over the density map.</p> <p>\u7b2c\u4e09\u6bb5\uff1a</p> <p>Specifically, we take inspiration from Lu et al. [19] that self-similarity is a strong prior in visual object counting, and introduce a transformer-based architecture where the self-similarity prior can be explicitly captured by the built-in attention mechanisms, both among the input image patches and with the few-shot annotations (if any).</p> <p>\u89c6\u89c9\u7269\u4f53\u7684\u81ea\u76f8\u4f3c\u6027\uff1bTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6355\u6349\u56fe\u50cf\u5757\u548c\u793a\u4f8b\u7684\u81ea\u76f8\u4f3c\u6027</p> <p>We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.</p> <p>\u4e24\u9636\u6bb5\u8bad\u7ec3\u6a21\u5f0f\uff1b</p> <p>\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u7f16\u7801\u5668\u9996\u5148\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3</p> <p>\u7136\u540e\u5bf9\u624b\u5934\u7684\u4efb\u52a1\u8fdb\u884c\u6709\u76d1\u7763\u7684\u5fae\u8c03\u3002</p> <p>We demonstrate that self-supervised pre-training can effectively learn the visual representation for counting, thus significantly improving the performance. </p> <p>\u6211\u4eec\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u80fd\u6709\u6548\u5b66\u4e60\u8ba1\u6570\u7684\u89c6\u89c9\u8868\u793a\u3001\u5e76\u63d0\u9ad8\u6027\u80fd</p> <p>Additionally, to tackle the long-tailed challenge in existing generalised visual object counting datasets, where the majority of images only contain a small number of objects, we propose a simple, yet scalable pipeline for synthesizing training images with a large number of instances, as a consequence, establishing reliable data sources for model training, to condition the user provided instance exemplars.\uff08\u53e5\u5b50\u662f\u771f\u957f\uff09</p> <p>\u2b50\ufe0f \u4e3a\u4e86\u89e3\u51b3 \u73b0\u6709\u901a\u7528\u89c6\u89c9\u76ee\u6807\u8ba1\u6570\u6570\u636e\u96c6\u7684 \u4e00\u76f4\u5b58\u5728\u7684\u95ee\u9898\uff0c\u90a3\u5c31\u662f \u5927\u591a\u6570\u8bad\u7ec3\u56fe\u50cf\u53ea\u5305\u542b\u4e00\u5c0f\u90e8\u5206\u6570\u91cf\u7684\u7269\u4f53\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u3001\u53ef\u4f38\u7f29\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5408\u6210\u542b\u6709\u5927\u91cf\u5b9e\u4f8b\u7684\u8bad\u7ec3\u56fe\u7247\u2192\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u6570\u636e\u96c6 \u7528\u4e8e\u6a21\u578b\u8bad\u7ec3</p> <p>\u7b2c\u56db\u6bb5\uff1a\u8d21\u732e</p>"},{"location":"literature/ObejectCounting/rank8%20CounTR/#_7","title":"\u76f8\u5173\u5de5\u4f5c","text":"<p>\u76f8\u5173\u5de5\u4f5c\u5c31\u4e24\u6bb5\uff1a</p> <ul> <li>\u89c6\u89c9\u7269\u4f53\u8ba1\u6570</li> <li>\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570</li> </ul> <p>\u7b2c\u4e00\u6bb5\uff1aVisual object counting. </p> <p>\u9996\u5148\u6307\u51fa\uff0c\u76ee\u6807\u8ba1\u6570\u95ee\u9898\u7684\u65b9\u6cd5\u5206\u6210\u4e24\u7c7b\uff1a\u57fa\u4e8e\u68c0\u6d4b\u7684\u548c\u57fa\u4e8e\u56de\u5f52\u7684</p> <p>In the literature, object counting approaches can generally be cast into two categories: detection-based counting [3, 5, 13] or regression-based counting [1, 2, 4, 15, 17, 20, 29]. </p> <p>\u57fa\u4e8e\u68c0\u6d4b\u7684......</p> <p>The former relies on a visual object detector that can localize object instances in an image. This method, however, requires training individual detectors for different objects, and the detection problem remains challenging if only a small number of annotations are given. </p> <p>\u57fa\u4e8e\u56de\u5f52\u7684......\u5c06\u56fe\u50cf\u6620\u5c04\u6210\u6807\u91cf\uff0c\u6216\u8005\u5c06\u56fe\u50cf\u6620\u5c04\u6210\u5bc6\u5ea6\u56fe</p> <p>The latter avoids solving the hard detection problem, instead, methods are designed to learn either a mapping from global image features to a scalar (number of objects), or a mapping from dense image features to a density map, achieving better results on counting overlapping instances. </p> <p>\u4f46\u662f\u6307\u51fa\u95ee\u9898\uff0c\u5148\u524d\u7684\u8ba1\u6570\u65b9\u6cd5\u4e0d\u7ba1\u662f\u57fa\u4e8e\u68c0\u6d4b\u7684\u8fd8\u662f\u57fa\u4e8e\u56de\u5f52\u7684\uff0c\u90fd\u53ea\u80fd\u8ba1\u6570\u7279\u5b9a\u7c7b\u522b\u7684\u7269\u4f53\uff0c\u6240\u4ee5\u5f15\u51fa\u7b2c\u4e8c\u6bb5\uff0c\u7c7b\u65e0\u5173\u8ba1\u6570</p> <p>However, previous methods from both lines (detection, regression) have only been able to count objects of one particular class (e.g. cars, cells).</p> <p>\u7b2c\u4e8c\u6bb5 Class-agnostic object counting. </p> <p>\u7ed9\u51fa\u95ee\u9898\u5b9a\u4e49</p> <p>Recently, class-agnostic few-shot counting [19, 24, 30] has witnessed a rise in research interest in the community. Unlike the class-specific models that could only count objects of specific classes like cars, cells, or people, class-agnostic counting aims to count the objects in an image based on a few given \u201cexemplar\u201d instances,thus is also termed as few-shot counting. </p> <p>\u76ee\u6807\uff1a\u8bad\u7ec3\u9636\u6bb5\uff0c\u6316\u6398\u4e0d\u540c\u7c7b\u522b\u76ee\u6807\u7684\u5171\u6027</p> <p>Generally speaking, class-agnostic few-shot counting models need to mine the commonalities between the counts of different classes of objects during training. </p> <p>\u6587\u732e GMN</p> <p>In [19], the authors propose a generic matching network (GMN), which regresses the density map by computing the similarity between the CNN features from image and exemplar shots; </p> <p>FamNet </p> <p>FamNet [24] utilizes feature correlation for prediction and uses adaptation loss to update the model\u2019s parameters at test time; </p> <p>SAFECount</p> <p>SAFECount [30] uses the support feature to enhance the query feature, making the extracted features more refined and then regresses to obtain density maps; </p> <p>[12]</p> <p>In a very recent work [12], the authors exploit a pre-trained DINO [21] model and a lightweight regression head to count without exemplars. </p> <p></p> <p>\u672c\u6587\uff1a\u57fa\u4e8eTransformer\u7684\uff0cCounTR</p> <p>In this paper, we also use transformer-based architecture, however, train it from scratch, and augment it with the ability to count the objects with any shot.</p>"},{"location":"literature/ObejectCounting/rank9%20SemAug_SAFECount/","title":"rank9 SemAug SAFECount","text":"<p>SemAug \u63d0\u51fa\u7684\u662f\u4e00\u4e2a\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5206\u522b\u5bf9\u4e24\u4e2a\u4e0d\u540c\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u6570\u636e\u589e\u5f3a\uff1aCounTR\u548cSAFECount\uff0c\u5237\u7684\u540c\u4e00\u4e2a\u6570\u636e\u96c6\u90fd\u662fFSC147\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5728\u4e8e\uff0c\u7ecf\u8fc7\u6570\u636e\u589e\u5f3a\u540e\u7684\u6a21\u578b\uff0c\u6548\u679c\u90fd\u6bd4\u539f\u6765\u7684\u597d</p> <p>\u91cd\u70b9\u5728\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u7b56\u7565\uff1a</p> <ul> <li>\u6269\u6563\u6a21\u578b\uff0c\u76d1\u7763\u4fe1\u53f7\uff1aprompt \u548c density map</li> </ul> <p></p>"},{"location":"literature/Reproduction/","title":"Index","text":"<p>null</p>"},{"location":"logs/","title":"\u6742","text":"<p>\u542c\u8bf4\u6bcf\u4e2ap\u4eba\u5fc5\u5907\u7684\u6742\uff0c\u7b11\uff09</p> <p>\ud83d\ude0bvisit my csdn</p> <p>\ud83e\udd63weibo</p>"},{"location":"logs/diary/","title":"\u4e50\u89c2 &amp; \u575a\u5f3a","text":"<ul> <li>241114 \u2b50\ufe0f  \u5077\u5077\u4ef0\u6155\u4e00\u4e0b\u6709\u4ec0\u4e48\u8fc7\u5206\u7684\uff0c\u6709\u70b9\u9177\u7684</li> <li>241115 \u5c0f\u7ea2\u4e66\u4e0a\u53d1\u4e86\u4e2a\u8d34\uff0c\u7fa4\u8d77\u5632\u4e4b\uff1a\u522b\u9a82\u4e86\u522b\u9a82\u4e86\uff0c\u6211\u9519\u4e86</li> <li>241117 \u6211\u6765\u5566</li> <li>241118 \u6765\u54af</li> <li> \u6587\u732e\u9605\u8bfb\u7b14\u8bb0</li> <li> <p> \u597d\u6d88\u606f\uff1a\u6587\u7ae0\u65f6\u95f4\u6233\u6539\u5bf9\u4e86</p> </li> <li> <p>241119 \u5f00\u5de5\uff0c\u6162\u6162\u6765\u4e5f\u633a\u597d\u7684\uff0c\u662f\u7684</p> </li> <li> <p>241125 \u5e72\u6d3b</p> </li> </ul>"},{"location":"sticks/","title":"Index","text":"<pre><code>(base) ... sticks % tree\n.\n\u251c\u2500\u2500 DONE.md\n\u251c\u2500\u2500 TODO\n\u2502   \u251c\u2500\u2500 1.png\n\u2502   \u2514\u2500\u2500 image-20241115095446260.png\n\u251c\u2500\u2500 TODO.md\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs_learn\n\u2502   \u251c\u2500\u2500 image-20241115100605111-1636372-1636377.png\n\u2502   \u251c\u2500\u2500 image-20241115101310759.png\n\u2502   \u2514\u2500\u2500 image-20241115101535524.png\n\u251c\u2500\u2500 mkdocs_learn.md\n\u2514\u2500\u2500 sticks.md\n\n3 directories, 10 files\n</code></pre>"},{"location":"sticks/DONE/","title":"DONE","text":"<ul> <li> \u672c\u5730\u6587\u4ef6\u548c\u5728\u7ebf\u6587\u4ef6\u7684\u5b58\u50a8\u95ee\u9898\uff0c\u4e0a\u4f20\u4e0a\u53bb\u7684\u672c\u5730\u600e\u4e48\u7ba1\u7406\uff0c\u53c8\u4e0d\u80fd\u5b8c\u5168\u5728\u7ebf</li> </ul> <p>\u7b49\u4f60\u5199\u5f97\u591a\u5230\u5360\u7528\u672c\u5730\u592a\u591a\u7a7a\u95f4\u518d\u8bf4\u5427\uff0c\u7b11\uff09</p> <ul> <li> \u56fe\u5e8a &amp; typora&amp; vscode&amp;github</li> </ul> <p>typora \u53ef\u4ee5\u81ea\u52a8\u521b\u5efa\u56fe\u5e8a\u6587\u4ef6\u5939</p> <ul> <li> mkdocs material \u5185\u5bb9\u5dee\u53c2\u8003</li> <li> <p> \u65f6\u95f4\u6233\u663e\u793a\u6709\u95ee\u9898  github actions error</p> </li> <li> <p> \u7248\u672c\u4fee\u6539</p> </li> </ul> <p>\u5b98\u65b9\u94fe\u63a5</p> <p>\u7248\u672c\u63a7\u5236\u793a\u4f8b</p> <p>\u7248\u672c\u63a7\u5236\u6e90\u7801</p> <p>\u597d\u590d\u6742\uff0c\u518d\u8bf4\u5427</p> <ul> <li> \u6587\u6863\u6807\u9898\u52a0\u7f16\u53f7\uff08\u53ef\u4ee5\u4f46\u6ca1\u5fc5\u8981\uff0c\u65b0\u5efaCSS\u6587\u4ef6\uff0c\u7136\u540e\u5728yml\u914d\u7f6e\u6587\u4ef6\u4e2d\u5f15\u7528</li> <li> mkdocs\u7684\u6587\u4ef6\u7ec4\u7ec7\u7ed3\u6784</li> </ul> <p>docs/\u6587\u4ef6\u5939\uff08\u5bfc\u822a\u680f\uff09/\uff08\u8d77\u4e2a\u522b\u540d\uff09/\u6587\u4ef6\u5939/\u6587\u4ef6\u5939/md\u6587\u4ef6</p> <p>docs/\u6587\u4ef6\u5939\uff08\u5bfc\u822a\u680f\uff09/\u6587\u4ef6\u5939\uff08\u5de6\u4fa7\u680f\u4e0b\u62c9\u6761\uff09/md\u6587\u4ef6</p> <p>docs/\u6587\u4ef6\u5939\uff08\u5bfc\u822a\u6a2a\u680f\uff09/md\u6587\u4ef6\uff08\u5de6\u4fa7\u680f\uff09/\u4e00\u7ea7\u6807\u9898\uff08\u6807\u9898\u5904\uff09/\u4e8c\u7ea7\u6807\u9898\uff08\u76ee\u5f55\u4ece\u4e8c\u7ea7\u6807\u9898\u5f00\u59cb\u663e\u793a\uff09</p> <p>\u4e00\u7ea7\u6807\u9898\u76f4\u63a5\u4f1a\u663e\u793a\u5728\u5de6\u4fa7\u680f\uff0c\u6216\u8005\u5728yml\u6587\u4ef6\u4e2d\u8d77\u522b\u540d</p> <ul> <li> \u82f1\u6587\u6587\u672c \u4e24\u7aef\u5bf9\u9f50(\u4ee5\u540e\u518d\u8bf4\u5427\uff0c\u4eba\u5bb6\u90fd\u6ca1\u5f04\uff0c\u6211\u4e5f\u4e0d\u6298\u817e\u4e86)</li> <li> \u8fd9\u4e2a\u4e3b\u9898\u8d85\u597d\u770b\uff0c\u6709\u7a7a\u6298\u817e\u4e00\u4e0b</li> <li> git push origin main\u6bcf\u6b21push\u5c31\u4f1a\u628a\u6240\u6709\u6587\u4ef6\u7684\u65f6\u95f4\u5168\u90e8\u66f4\u6539\u4e86</li> </ul> <p>\u6539\u5bf9\u4e86\uff01\u91cd\u65b0\u628a\u6574\u4e2a \u5de5\u4f5c\u6d41\u6587\u4ef6\u590d\u5236\u4e86\u522b\u4eba\u7684\u4e00\u4efd\u3002</p>"},{"location":"sticks/TODO/","title":"TODO","text":""},{"location":"sticks/TODO/#_1","title":"\u6298\u817e","text":"<ul> <li> \u6587\u4ef6\u7ed3\u6784\u53d8\u4e86\uff0c\u8bb0\u5f97\u4fee\u6539yml\u7684\u8def\u5f84</li> <li> </li> </ul>"},{"location":"sticks/TODO/#_2","title":"\u516b\u80a1","text":"<ul> <li> <p>\u9762\u8bd5\u9898</p> </li> <li> <p>\u529b\u6263</p> </li> </ul>"},{"location":"sticks/TODO/#_3","title":"\u6bd5\u4e1a","text":"<ul> <li> <p>\u5f00\u9898\u62a5\u544a\uff0cppt</p> </li> <li> <p>\u6bd5\u4e1a\u8bba\u6587 latex\u6a21\u677f\uff0c\u516c\u5f0f\uff0c\u53c2\u8003\u6587\u732e</p> </li> </ul>"},{"location":"sticks/markdwon_learn/","title":"markdown sticks","text":""},{"location":"sticks/markdwon_learn/#_1","title":"\u951a\u70b9\u8bbe\u7f6e","text":"<p>\u4ece\u54ea\u513f\u8df3\uff1a</p> <pre><code>[\u8bf4\u660e\u6587\u5b57](#jump)\n</code></pre> <p>\u8df3\u5230\u54ea\u91cc\uff1a</p> <pre><code>&lt;span id = \"jump\"&gt;\u8df3\u8f6c\u5230\u7684\u4f4d\u7f6e&lt;/span&gt;\n</code></pre>"},{"location":"sticks/mkdocs_learn/","title":"mkdocs\u5b66\u4e60","text":"<p>\u4e3b\u9898\u914d\u7f6e\uff1aMaterial for MkDocs</p> <p>\u672c\u5730\u8c03\u8bd5\uff1a</p> <pre><code>(base) .. mkdocs-site % mkdocs -h\nUsage: mkdocs [OPTIONS] COMMAND [ARGS]...\n\n  MkDocs - Project documentation with Markdown.\n\nOptions:\n  -V, --version         Show the version and exit.\n  -q, --quiet           Silence warnings\n  -v, --verbose         Enable verbose output\n  --color / --no-color  Force enable or disable color and wrapping for the output. Default is auto-\n                        detect.\n  -h, --help            Show this message and exit.\n\nCommands:\n  build      Build the MkDocs documentation.\n  get-deps   Show required PyPI packages inferred from plugins in mkdocs.yml.\n  gh-deploy  Deploy your documentation to GitHub Pages.\n  new        Create a new MkDocs project.\n  serve      Run the builtin development server.\n</code></pre> <p>\u53c2\u8003\u6a21\u7248\u6e90\u7801</p> <p>\u53c2\u8003\u6a21\u7248\u5c55\u793a</p> <p>\u5b98\u65b9\u6587\u6863\uff1amkdocs\u914d\u7f6e </p> <p>mkdocs\u5165\u95e8\u6559\u7a0b</p>"},{"location":"sticks/mkdocs_learn/#_1","title":"\u6587\u4ef6\u7ec4\u7ec7\u5f62\u5f0f","text":"<pre><code>(base) ... docs % tree\n.\n\u251c\u2500\u2500 Error  # \u6587\u4ef6\u5939\n\u2502   \u2514\u2500\u2500 \u62a5\u9519.md   # markdown\u6587\u4ef6\n\u251c\u2500\u2500 Leecode\n\u2502   \u2514\u2500\u2500 \u529b\u6263.md\n\u251c\u2500\u2500 home\n\u2502   \u251c\u2500\u2500 page-1.md\n\u2502   \u2514\u2500\u2500 page-2.md\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs\n\u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u251c\u2500\u2500 no-footer.css\n\u2502   \u2502   \u2514\u2500\u2500 unordered-list-symbols.css\n\u2502   \u2514\u2500\u2500 javascripts\n\u2502       \u2514\u2500\u2500 katex.js\n\u2514\u2500\u2500 \u4fbf\u7b7e  # \u6587\u4ef6\u5939\n  \u251c\u2500\u2500 TODO  # \u56fe\u5e8a\n  \u2502   \u251c\u2500\u2500 1.png\n  \u2502   \u2514\u2500\u2500 image-20241115095446260.png\n  \u251c\u2500\u2500 TODO.md #markdown\u6587\u4ef6\n  \u251c\u2500\u2500 mkdocs_learn\n  \u2502   \u2514\u2500\u2500 image-20241115100605111-1636372-1636377.png\n  \u251c\u2500\u2500 mkdocs_learn.md\n  \u2514\u2500\u2500 \u5907\u5fd8.md\n\n10 directories, 14 files\n</code></pre> <p>\u524d\u6bb5\u4e0e\u540e\u7aef\u7684\u5bf9\u5e94</p> <p></p>"},{"location":"sticks/mkdocs_learn/#_2","title":"\u6dfb\u52a0\u9875\u9762\u521b\u5efa\u65f6\u95f4\u3001\u6700\u540e\u4e00\u6b21\u4fee\u6539\u65f6\u95f4","text":"<p>\u5b98\u65b9\u6587\u6863\u94fe\u63a5</p> <p></p>"},{"location":"sticks/mkdocs_learn/#_3","title":"\u5199\u4f5c","text":"<p>\u66f4\u591a\u5199\u4f5c</p> <pre><code>!!! note\n    This is a note.\n</code></pre> <pre><code>!!! tip\n    This is a tip.\n</code></pre> <pre><code>!!! warning\n    This is a warning.\n</code></pre> <pre><code>!!! danger\n    This is a danger.\n</code></pre> <pre><code>!!! success\n    This is a success.\n</code></pre> <pre><code>!!! info\n    This is a info.\n</code></pre> <pre><code>!!! quote\n    This is a quote.\n</code></pre> <pre><code>??? question \"What is the meaning of life, the universe, and everything?\"\n</code></pre> <p>Note</p> <p>This is a note.</p> <p>Tip</p> <p>This is a tip.</p> <p>Warning</p> <p>This is a warning.</p> <p>Danger</p> <p>This is a danger.</p> <p>Success</p> <p>This is a success.</p> <p>Info</p> <p>This is a info.</p> <p>Quote</p> <p>This is a quote.</p> What is the meaning of life, the universe, and everything?"},{"location":"sticks/sticks/","title":"\u5907\u5fd8\u5f55","text":""},{"location":"sticks/sticks/#git","title":"git\u547d\u4ee4","text":"<p>\u65b0\u5efa\u4ed3\u5e93\uff1a</p> <pre><code>echo \"# Rongerr.github.io\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/dearRongerr/Rongerr.github.io.git\ngit push -u origin main\n</code></pre> <p>\u5411\u5df2\u6709\u4ed3\u5e93\u63a8\u9001</p> <pre><code>git remote add origin https://github.com/dearRongerr/Rongerr.github.io.git\ngit branch -M main\ngit push -u origin main\n</code></pre>"},{"location":"sticks/sticks/#mkdocs","title":"mkdocs\u547d\u4ee4","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> <p>Project layout</p> <p>\u200b    mkdocs.yml    # The configuration file. \u200b    docs/ \u200b        index.md  # The documentation homepage. \u200b        ...       # Other markdown pages, images and other files.</p>"},{"location":"sticks/sticks/#_1","title":"\u7ec8\u7aef\u547d\u4ee4","text":"<ul> <li><code>cd ..</code>  \u8fd4\u56de\u4e0a\u7ea7\u6587\u4ef6</li> <li><code>ls</code>  \u663e\u793a\u5f53\u524d\u76ee\u5f55\u6587\u4ef6</li> <li><code>ls -a</code> \u663e\u793a\u5f53\u524d\u76ee\u5f55\u7684\u6240\u6709\u6587\u4ef6\uff0c\u5305\u62ec\u9690\u85cf\u6587\u4ef6</li> </ul> <p>macOS \u7ec8\u7aef\u547d\u4ee4</p> <ul> <li> <p>\u663e\u793a\u6587\u4ef6\u6811</p> </li> <li> <p>\u6253\u5f00mac\u7ec8\u7aef</p> <ul> <li> <p>\u8f93\u5165 brew install tree</p> </li> <li> <p>\u4f7f\u7528\uff1a</p> </li> </ul> <p><code>tree</code> \u663e\u793a\u6587\u4ef6\u6811</p> <p><code>tree -a</code> \u663e\u793a\u6240\u6709\u6587\u4ef6\u6811\uff0c\u5305\u542b\u9690\u85cf\u6587\u4ef6</p> </li> <li> <p>vim\uff1f</p> </li> </ul>"}]}